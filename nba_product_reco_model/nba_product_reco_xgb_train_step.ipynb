{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b0c603b-3f78-40d0-bc02-7dac3ea2a76c",
   "metadata": {},
   "source": [
    "### Import Libraries, declare variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1294f3f-ee1c-4c57-bbb1-635ef08b52d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import google\n",
    "from google.oauth2 import credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2.service_account import Credentials\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# build model\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "SERVICE_TYPE = 'nba_product_reco_model'\n",
    "DATASET_ID = 'nba_product_reco_model'\n",
    "PROJECT_ID = 'divg-groovyhoon-pr-d2eab4' #mapping['PROJECT_ID']\n",
    "RESOURCE_BUCKET = 'divg-groovyhoon-pr-d2eab4-default' #mapping['resources_bucket']\n",
    "FILE_BUCKET = 'divg-groovyhoon-pr-d2eab4-default' #mapping['gcs_csv_bucket']\n",
    "REGION = 'northamerica-northeast1' #mapping['REGION']\n",
    "MODEL_ID = '9999'\n",
    "FOLDER_NAME = 'nba_product_reco_model'.format(MODEL_ID)\n",
    "QUERIES_PATH = 'vertex_pipelines/' + FOLDER_NAME + '/queries/'\n",
    "TRAIN_TABLE_ID = 'nba_training_dataset'\n",
    "VAL_TABLE_ID = 'nba_training_dataset'\n",
    "SCORE_TABLE_ID = 'bq_product_reco_scores'\n",
    "\n",
    "scoringDate = date(2023, 10, 13)  # date.today() - relativedelta(days=2)- relativedelta(months=30)\n",
    "valScoringDate = date(2023, 11, 13)  # scoringDate - relativedelta(days=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30300f-ff71-417d-956f-6d3dcede6c29",
   "metadata": {},
   "source": [
    "### import bq to dataframe function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e098413-7211-4346-b1c9-eafd38ef076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import credentials\n",
    "\n",
    "def import_bq_to_dataframe(project_id, dataset_id, table_id, client): \n",
    "    \n",
    "    \"\"\"\n",
    "    Imports a specific table from BigQuery to a DataFrame. \n",
    "    \n",
    "    Args: \n",
    "        project_id: The name of the project_id where the table is located.\n",
    "        dataset_id: The name of the dataset_id where the table is located.\n",
    "        table_id: The name of the table_id you wish to import to DataFrame.\n",
    "        client: A BigQuery client instance. e.g. client = bigquery.Client(project=project_id).\n",
    "\n",
    "    Returns: \n",
    "        A DataFrame\n",
    "        \n",
    "    Example: \n",
    "        import_bq_to_dataframe('bi-stg-divg-speech-pr-9d940b', 'call_to_retention_dataset', 'bq_ctr_pipeline_dataset')\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    sql = f\"SELECT * FROM `{project_id}.{dataset_id}.{table_id}`\"\n",
    "    \n",
    "    df_return = client.query(sql).to_dataframe()\n",
    "\n",
    "    return df_return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a31e45-47f9-445e-aab4-1e2d8243e1f9",
   "metadata": {},
   "source": [
    "### define get_lift function, import df_train and df_test from gcs bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3837b0-0180-4a8a-978d-b56402062efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "project_id = PROJECT_ID\n",
    "region = REGION\n",
    "resource_bucket = RESOURCE_BUCKET\n",
    "file_bucket = FILE_BUCKET\n",
    "service_type=SERVICE_TYPE\n",
    "project_id=PROJECT_ID\n",
    "dataset_id=DATASET_ID\n",
    "train_table_id = TRAIN_TABLE_ID\n",
    "val_table_id = VAL_TABLE_ID\n",
    "\n",
    "def get_lift(prob, y_test, q):\n",
    "    result = pd.DataFrame(columns=['Prob', 'Churn'])\n",
    "    result['Prob'] = prob\n",
    "    result['Churn'] = y_test\n",
    "    # result['Decile'] = pd.qcut(1-result['Prob'], 10, labels = False)\n",
    "    result['Decile'] = pd.qcut(result['Prob'], q, labels=[i for i in range(q, 0, -1)])\n",
    "    add = pd.DataFrame(result.groupby('Decile')['Churn'].mean()).reset_index()\n",
    "    add.columns = ['Decile', 'avg_real_churn_rate']\n",
    "    result = result.merge(add, on='Decile', how='left')\n",
    "    result.sort_values('Decile', ascending=True, inplace=True)\n",
    "    lg = pd.DataFrame(result.groupby('Decile')['Prob'].mean()).reset_index()\n",
    "    lg.columns = ['Decile', 'avg_model_pred_churn_rate']\n",
    "    lg.sort_values('Decile', ascending=False, inplace=True)\n",
    "    lg['avg_churn_rate_total'] = result['Churn'].mean()\n",
    "    lg = lg.merge(add, on='Decile', how='left')\n",
    "    lg['lift'] = lg['avg_real_churn_rate'] / lg['avg_churn_rate_total']\n",
    "\n",
    "    return lg\n",
    "\n",
    "def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "    token = os.popen('gcloud auth print-access-token').read()\n",
    "    token = re.sub(f'\\n$', '', token)\n",
    "    credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    if use_local_credential:\n",
    "        bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "    return bq_client\n",
    "\n",
    "client = get_gcp_bqclient(project_id)\n",
    "\n",
    "df_train = import_bq_to_dataframe(project_id, dataset_id, train_table_id, client)\n",
    "df_val = import_bq_to_dataframe(project_id, dataset_id, val_table_id, client)\n",
    "\n",
    "print(f'df_train: {df_train.shape}')\n",
    "print(f'df_val: {df_val.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94961cc0-7e92-44eb-9e1e-15d16d2024ed",
   "metadata": {},
   "source": [
    "### add targets to df_train and df_target \n",
    "\n",
    "- df_target_train is from `divg-josh-pr-d1cc3a.tos_crosssell.bq_tos_cross_sell_targets_q3` \n",
    "- df_target_test is from `divg-josh-pr-d1cc3a.tos_crosssell.bq_tos_cross_sell_targets_q4` \n",
    "- some parts of the code and sql queries need to be dynamically adjusted to be included in the deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc272d97-768b-401b-9100-923cc63cf1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "df_train, df_test = train_test_split(df_train, test_size=0.3, random_state=42, stratify=df_train['target_ind'])\n",
    "\n",
    "df_train.to_csv('gs://{}/{}/{}_train_final.csv'.format(FILE_BUCKET, SERVICE_TYPE, SERVICE_TYPE), index=False)\n",
    "df_test.to_csv('gs://{}/{}/{}_test_final.csv'.format(FILE_BUCKET, SERVICE_TYPE, SERVICE_TYPE), index=False)\n",
    "df_val.to_csv('gs://{}/{}/{}_val_final.csv'.format(FILE_BUCKET, SERVICE_TYPE, SERVICE_TYPE), index=False)\n",
    "\n",
    "#set up features (list)\n",
    "cols_1 = df_train.columns.values\n",
    "cols_2 = df_test.columns.values\n",
    "cols_3 = df_val.columns.values\n",
    "\n",
    "cols = set(cols_1).intersection(set(cols_2))\n",
    "cols = set(cols).intersection(set(cols_3))\n",
    "\n",
    "features_to_exclude = ['split_type','model_scenario','ref_dt','cust_id','cust_src_id','ban','ban_src_id','lpds_id','fms_address_id','label','label_dt'] \n",
    "\n",
    "features = [f for f in cols if f not in features_to_exclude]\n",
    "\n",
    "ban_train = df_train[['ban', 'lpds_id']]\n",
    "X_train = df_train[features]\n",
    "y_train = np.squeeze(df_train['model_scenario'].values)\n",
    "target_train = df_train['model_scenario']\n",
    "\n",
    "ban_test = df_test[['ban', 'lpds_id']] \n",
    "X_test = df_test[features]\n",
    "y_test = np.squeeze(df_test['model_scenario'].values)\n",
    "target_test = df_test['model_scenario']\n",
    "\n",
    "ban_val = df_val[['ban', 'lpds_id']] \n",
    "X_val = df_val[features]\n",
    "y_val = np.squeeze(df_val['model_scenario'].values)\n",
    "target_val = df_val['model_scenario']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee477829-f647-4302-b142-67f33d33dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a591fa94-0f6d-47a7-9bb5-53d4a39ca42b",
   "metadata": {},
   "source": [
    "### preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38820930-8f64-4d21-bd82-54aef8e6d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(df, cat_feature_names): \n",
    "    \n",
    "    df_income_dummies = pd.get_dummies(df[cat_feature_names]) \n",
    "    df_income_dummies.columns = df_income_dummies.columns.str.replace('&', 'and')\n",
    "    df_income_dummies.columns = df_income_dummies.columns.str.replace(' ', '_')\n",
    "\n",
    "    df.drop(columns=cat_feature_names, axis=1, inplace=True)\n",
    "\n",
    "    df = df.join(df_income_dummies)\n",
    "    \n",
    "    #column name clean-up\n",
    "    df.columns = df.columns.str.replace(' ', '_')\n",
    "    df.columns = df.columns.str.replace('-', '_')\n",
    "    \n",
    "    return df\n",
    "\n",
    "cat_feature_names = ['revenue_band', 'payment_mthd', 'ebill_ind', 'dvc_non_telus_ind', 'credit_class', 'contract_type', 'bacct_delinq_ind', 'urbn_rur_ind',\n",
    "                     'dnc_sms_ind', 'dnc_em_ind', 'data_usg_trend', 'wls_data_plan_ind', 'wls_data_shr_plan_ind', 'demo_sgname', 'demo_lsname']\n",
    "\n",
    "# cat_feature_names = ['revenue_band', 'payment_mthd', 'ebill_ind', 'dvc_non_telus_ind', 'credit_class', 'contract_type', 'bacct_delinq_ind', 'urbn_rur_ind',\n",
    "#                      'dnc_sms_ind', 'dnc_em_ind', 'data_usg_trend', 'wls_data_plan_ind', 'wls_data_shr_plan_ind']\n",
    "    \n",
    "X_train = to_categorical(X_train, cat_feature_names)\n",
    "X_test = to_categorical(X_test, cat_feature_names)\n",
    "X_val = to_categorical(X_val, cat_feature_names)\n",
    "\n",
    "#set up features (list)\n",
    "cols_1 = X_train.columns.values\n",
    "cols_2 = X_test.columns.values\n",
    "cols_3 = X_val.columns.values\n",
    "\n",
    "cols = set(cols_1).intersection(set(cols_2))\n",
    "cols = set(cols).intersection(set(cols_3))\n",
    "\n",
    "features = [f for f in cols if f not in features_to_exclude]\n",
    "\n",
    "# features = ['contract_type_MTM'\n",
    "# , 'contract_type_BYOD'\n",
    "# , 'contract_type_On_Contract'\n",
    "# , 'cntrct_end_recency'\n",
    "# , 'cntrct_start_recency'\n",
    "# , 'ban_tenure'\n",
    "# , 'contract_mth'\n",
    "# , 'urbn_rur_ind_Rural'\n",
    "# , 'sub_tenure'\n",
    "# , 'urbn_rur_ind_Urban'\n",
    "# , 'dvc_non_telus_ind_Y'\n",
    "# , 'demo_lsname_Large_Diverse_Families'\n",
    "# , 'dvc_non_telus_ind_N'\n",
    "# , 'demo_sgname_Diverse_Urban_Fringe'\n",
    "# , 'demo_sgname_Lower_Middle_Rural'\n",
    "# , 'data_usg_trend_unknown'\n",
    "# , 'demo_sgname_Midscale_Urban_Fringe'\n",
    "# , 'clk_app_offer'\n",
    "# , 'bacct_delinq_ind_Y'\n",
    "# , 'revenue_band_D'\n",
    "# , 'payment_mthd_R'\n",
    "# , 'clk_app_usage'\n",
    "# , 'demo_sgname_Upper_Middle_Suburbia'\n",
    "# , 'demo_sgname_Urban_Diversity'\n",
    "# , 'easy_pymt_avg'\n",
    "# , 'rate_plan_amt'\n",
    "# , 'demo_sgname_Upscale_Suburban_Diversity'\n",
    "# , 'sub_cnt'\n",
    "# , 'clk_app_ovrview'\n",
    "# , 'credit_class_D'\n",
    "# , 'demo_sgname_Town_Mix'\n",
    "# , 'demo_sgname_Young_Urban_Core'\n",
    "# , 'payment_mthd_C'\n",
    "# , 'bacct_delinq_ind_N'\n",
    "# , 'artm_min_qty_avg'\n",
    "# , 'credit_class_V'\n",
    "# , 'hm_call_cnt_avg'\n",
    "# , 'wls_data_shr_plan_ind_Y'\n",
    "# , 'demo_lsname_Older_Families_and_Empty_Nests'\n",
    "# , 'dnc_sms_ind_N'\n",
    "# , 'demo_lsname_School_Age_Families'\n",
    "# , 'credit_class_X'\n",
    "# , 'clk_web_plan'\n",
    "# , 'hm_data_usg_avg'\n",
    "# , 'revenue_band_F'\n",
    "# , 'demo_lsname_Young_Families'\n",
    "# , 'demo_lsname_Middle_Age_Families'\n",
    "# , 'clk_app_subslct'\n",
    "# , 'cr_disc_amt_avg'\n",
    "# , 'tot_data_usg_avg'\n",
    "# , 'age'\n",
    "# , 'demo_avg_income'\n",
    "# , 'data_usg_trend_increasing'\n",
    "# , 'revenue_band_N'\n",
    "# , 'revenue_band_C'\n",
    "# , 'dvc_non_telus_ind_U'\n",
    "# , 'wls_data_plan_ind_N'\n",
    "# , 'ebill_ind_N'\n",
    "# , 'net_inv_amt_avg'\n",
    "# , 'clk_app_bill'\n",
    "# , 'clk_app_changefg'\n",
    "# , 'revenue_band_NA'\n",
    "# , 'demo_sgname_Older_Urban_Francophone'\n",
    "# , 'demo_sgname_Upper_Middle_Rural'\n",
    "# , 'sms_dnc_recency'\n",
    "# , 'demo_sgname_Upper_Middle_Suburban_Francophone'\n",
    "# , 'demo_sgname_Unassigned'\n",
    "# , 'clk_web_bill'\n",
    "# , 'clk_app_drawer'\n",
    "# , 'demo_sgname_Rural_Francophone'\n",
    "# , 'clk_web_phnumber'\n",
    "# , 'ld_min_qty_avg'\n",
    "# , 'em_dnc_recency'\n",
    "# , 'effort_durtn_sec_qty_s'\n",
    "# ]\n",
    "\n",
    "X_train = X_train[features] \n",
    "X_test = X_test[features] \n",
    "X_val = X_val[features]\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fb3c25-0efe-4842-9dd2-53da180d7155",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# del df_train, df_val, df_test\n",
    "del df_train, df_test, df_val\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfd29e3-d1b3-4cbe-983c-b0033ed8554a",
   "metadata": {},
   "source": [
    "### fit training data in xgboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7616d7-5909-4cb8-8e74-821d3e92dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build model and fit in training data\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=250,\n",
    "    max_depth=12,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='binary:logistic',\n",
    "    nthread=4,\n",
    "    scale_pos_weight=1,\n",
    "    seed=27\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "print('xgb training done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5abc939-f3aa-408a-b3a6-0beb1ff89c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0ea7ec-74db-4419-84bb-0d44545fec56",
   "metadata": {},
   "source": [
    "### make predictions on X_test set, assign deciles to the predicted values, and save in df_test_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3c55fb-754b-4fa9-b0ac-cdcf45a3dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions on X_test\n",
    "y_pred = xgb_model.predict_proba(X_test, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "\n",
    "#join ban_test, X_test, y_test and y_pred and print to csv\n",
    "#CHECK THE SIZE OF EACH COMPONENT BEFORE JOINING\n",
    "q=10\n",
    "df_ban_test = ban_test\n",
    "df_test_exp = df_ban_test.join(X_test) \n",
    "df_test_exp['y_test'] = y_test\n",
    "df_test_exp['y_pred_proba'] = y_pred\n",
    "df_test_exp['y_pred'] = (df_test_exp['y_pred_proba'] > 0.5).astype(int)\n",
    "df_test_exp['decile'] = pd.qcut(df_test_exp['y_pred_proba'], q, labels=[i for i in range(q, 0, -1)])\n",
    "\n",
    "lg = get_lift(y_pred, y_test, q)\n",
    "\n",
    "lg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2767b8f5-8bdb-4d5f-9b5b-2b340289cb94",
   "metadata": {
    "tags": []
   },
   "source": [
    "### export df_test_exp and lift scores to gcs bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2015d1-d1a5-410f-95c5-2b5054e2abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_exp.to_csv('gs://{}/{}/{}_df_test_exp.csv'.format(file_bucket, SERVICE_TYPE, SERVICE_TYPE, index=True))\n",
    "print(\"....df_test_exp exported\")\n",
    "\n",
    "lg.to_csv('gs://{}/{}/{}_lift_on_test_data.csv'.format(file_bucket, SERVICE_TYPE, SERVICE_TYPE, index=False))\n",
    "print(\"....lift_to_csv done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a67377-2467-47f9-aa71-2063716eda1f",
   "metadata": {},
   "source": [
    "### make predictions on X_val set, assign deciles to the predicted values, and save in df_val_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7231aba-49a5-4a16-9c57-2061ebe27892",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions on X_val\n",
    "y_pred = xgb_model.predict_proba(X_val, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "\n",
    "#join ban_val, X_val, y_val and y_pred and print to csv\n",
    "#CHECK THE SIZE OF EACH COMPONENT BEFORE JOINING\n",
    "q=10\n",
    "df_ban_val = ban_val\n",
    "df_val_exp = df_ban_val.join(X_val) \n",
    "df_val_exp['y_val'] = y_val\n",
    "df_val_exp['y_pred_proba'] = y_pred\n",
    "df_val_exp['y_pred'] = (df_val_exp['y_pred_proba'] > 0.5).astype(int)\n",
    "df_val_exp['decile'] = pd.qcut(df_val_exp['y_pred_proba'], q, labels=[i for i in range(q, 0, -1)])\n",
    "\n",
    "lg = get_lift(y_pred, y_val, q)\n",
    "\n",
    "lg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca9c17-8af8-4f29-b3a1-d35a1c50cf19",
   "metadata": {},
   "source": [
    "### export df_val_exp and lift scores to gcs bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570c3887-fc3a-44a7-b037-4bdfd61eb568",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_exp.to_csv('gs://{}/{}/{}_df_val_exp.csv'.format(file_bucket, SERVICE_TYPE, SERVICE_TYPE, index=True))\n",
    "print(\"....df_val_exp exported\")\n",
    "\n",
    "lg.to_csv('gs://{}/{}/{}_lift_on_val_data.csv'.format(file_bucket, SERVICE_TYPE, SERVICE_TYPE, index=False))\n",
    "print(\"....lift_to_csv done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8864bda6-bda0-4032-9704-f513da84e132",
   "metadata": {},
   "source": [
    "### get feature importances from xgboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43768ef0-d6cc-40e4-a3a9-dbcdbceedb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from xgboost model\n",
    "importances = xgb_model.feature_importances_\n",
    "\n",
    "# Get the index of importances from greatest importance to least\n",
    "sorted_index = np.argsort(importances)[::-1]\n",
    "x = range(len(importances))\n",
    "\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create tick labels \n",
    "labels = np.array(feature_names)[sorted_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb976c57-68c2-4d13-84c6-7f138ae560bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in sorted_index: \n",
    "    print(f'{feature_names[idx]}, {importances[idx]}', end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e9b69c-0ab4-4595-b2f2-3b0a6387d3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38edf5cf-4f19-41c3-ab79-3b560cdd1f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9209e72-72ba-4c08-a7c6-c8317836fcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(importances)[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f8515b-07cd-4c76-9d79-944145ef3de1",
   "metadata": {},
   "source": [
    "### Load results to BQ - WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514b9b52-17c6-4c7f-8a36-9afdfc17f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORE_TABLE_ID = 'bq_telus_12_months_churn_scores'\n",
    "\n",
    "project_id = PROJECT_ID \n",
    "dataset_id = DATASET_ID\n",
    "score_table_id = SCORE_TABLE_ID\n",
    "\n",
    "# get full score to cave into bucket\n",
    "pred_prob = xgb_model.predict_proba(X_val, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "result = pd.DataFrame(columns=['ban', 'subscriber_no', 'score_date', 'y_true', 'y_pred'])\n",
    "\n",
    "result['ban'] = list(ban_val['ban'])\n",
    "result['ban'] = result['ban'].astype('str')\n",
    "\n",
    "result['subscriber_no'] = list(ban_val['ban'])\n",
    "result['subscriber_no'] = result['subscriber_no'].astype('str')\n",
    "\n",
    "result['score_date'] = \"2023-11-04\"\n",
    "\n",
    "result['y_true'] = list(y_val)\n",
    "result['y_true'] = result['y_true'].fillna(0.0).astype('float64')\n",
    "\n",
    "result['y_pred'] = list(pred_prob)\n",
    "result['y_pred'] = result['y_pred'].fillna(0.0).astype('float64')\n",
    "\n",
    "############# updated up to here ############\n",
    "\n",
    "result.to_csv('gs://{}/{}/ucar/{}_prediction_v2.csv'.format(file_bucket, service_type, service_type), index=False)\n",
    "\n",
    "# define dtype_bq_mapping\n",
    "dtype_bq_mapping = {np.dtype('int64'): 'INTEGER', \n",
    "np.dtype('float64'):  'FLOAT', \n",
    "np.dtype('float32'):  'FLOAT', \n",
    "np.dtype('object'):  'STRING', \n",
    "np.dtype('bool'):  'BOOLEAN', \n",
    "np.dtype('datetime64[ns]'):  'DATE', \n",
    "pd.Int64Dtype(): 'INTEGER'} \n",
    "\n",
    "# export df_final to bigquery \n",
    "schema_list = [] \n",
    "for column in result.columns: \n",
    "    schema_list.append(bigquery.SchemaField(column, dtype_bq_mapping[result.dtypes[column]], mode='NULLABLE')) \n",
    "print(schema_list) \n",
    "\n",
    "dest_table = f'{project_id}.{dataset_id}.{score_table_id}'\n",
    "\n",
    "# Sending to bigquery \n",
    "client = get_gcp_bqclient(project_id)\n",
    "job_config = bigquery.LoadJobConfig(schema=schema_list, write_disposition='WRITE_TRUNCATE') \n",
    "job = client.load_table_from_dataframe(result, dest_table, job_config=job_config) \n",
    "job.result() \n",
    "\n",
    "table = client.get_table(dest_table) # Make an API request \n",
    "print(\"Loaded {} rows and {} columns to {}\".format(table.num_rows, len(table.schema), table_id)) \n",
    "\n",
    "time.sleep(60)\n",
    "\n",
    "# table_ref = f'{project_id}.{dataset_id}.{score_table}'\n",
    "# client = bigquery.Client(project=project_id)\n",
    "# table = client.get_table(table_ref)\n",
    "# schema = table.schema\n",
    "\n",
    "# ll = []\n",
    "# for item in schema:\n",
    "#     col = item.name\n",
    "#     d_type = item.field_type\n",
    "#     if 'float' in str(d_type).lower():\n",
    "#         d_type = 'FLOAT64'\n",
    "#     ll.append((col, d_type))\n",
    "\n",
    "#     if 'integer' in str(d_type).lower():\n",
    "#         result[col] = result[col].fillna(0).astype(int)\n",
    "#     if 'float' in str(d_type).lower():\n",
    "#         result[col] = result[col].fillna(0.0).astype(float)\n",
    "#     if 'string' in str(d_type).lower():\n",
    "#         result[col] = result[col].fillna('').astype(str)\n",
    "\n",
    "# table_ref = '{}.{}.{}'.format(project_id, dataset_id, temp_table)\n",
    "# client = bigquery.Client(project=project_id)\n",
    "# if if_tbl_exists(client, table_ref):\n",
    "#     client.delete_table(table_ref)\n",
    "\n",
    "# client.create_table(table_ref)\n",
    "# config = bigquery.LoadJobConfig(schema=schema)\n",
    "# config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "# bq_table_instance = client.load_table_from_dataframe(result, table_ref, job_config=config)\n",
    "# time.sleep(5)\n",
    "\n",
    "# drop_sql = f\"\"\"delete from `{project_id}.{dataset_id}.{score_table}` where score_date = '{score_date_dash}'\"\"\"  # .format(project_id, dataset_id, score_date_dash)\n",
    "# client.query(drop_sql)\n",
    "# #\n",
    "# load_sql = f\"\"\"insert into `{project_id}.{dataset_id}.{score_table}`\n",
    "#               select * from `{project_id}.{dataset_id}.{temp_table}`\"\"\"\n",
    "# client.query(load_sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bf0cd1-afbc-42e6-9a2f-fd8b727c2cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce20d37b-0e62-4c81-b446-a88539baf2d9",
   "metadata": {},
   "source": [
    "### save the model in gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46e0caa-94a8-497c-a395-15a1ac041f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model in GCS\n",
    "from datetime import datetime\n",
    "models_dict = {}\n",
    "create_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "models_dict['create_time'] = create_time\n",
    "models_dict['model'] = xgb_model\n",
    "models_dict['features'] = features\n",
    "\n",
    "with open('model_dict.pkl', 'wb') as handle:\n",
    "    pickle.dump(models_dict, handle)\n",
    "handle.close()\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(file_bucket)\n",
    "\n",
    "MODEL_PATH = '{}_xgb_models/'.format(service_type)\n",
    "blob = bucket.blob(MODEL_PATH)\n",
    "if not blob.exists(storage_client):\n",
    "    blob.upload_from_string('')\n",
    "\n",
    "model_name_onbkt = '{}{}_models_xgb_{}'.format(MODEL_PATH, service_type, models_dict['create_time'])\n",
    "blob = bucket.blob(model_name_onbkt)\n",
    "blob.upload_from_filename('model_dict.pkl')\n",
    "\n",
    "print(f\"....model loaded to GCS done at {str(create_time)}\")\n",
    "\n",
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c02b2b6-b059-4dde-9393-ad457eebf60e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3d9d01-f009-45c6-90fe-95a61cc169cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb40188a-3865-4993-be01-fb27f7d07dd6",
   "metadata": {},
   "source": [
    "### load the latest saved xgb_model to the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f240186-29e9-4d12-8c11-bb468cae8067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH = '{}_xgb_models/'.format(service_type)\n",
    "# df_score = pd.read_csv('gs://{}/{}_score.csv.gz'.format(file_bucket, service_type), compression='gzip')\n",
    "# df_score.dropna(subset=['ban'], inplace=True)\n",
    "# df_score.reset_index(drop=True, inplace=True)\n",
    "# print('......scoring data loaded:{}'.format(df_score.shape))\n",
    "# time.sleep(10)\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "MODEL_PATH = '{}_xgb_models/'.format(service_type)\n",
    "\n",
    "def load_model(file_bucket: str, service_type: str): \n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(file_bucket)\n",
    "    blobs = storage_client.list_blobs(file_bucket, prefix='{}{}_models_xgb_'.format(MODEL_PATH, service_type))\n",
    "\n",
    "    model_lists = []\n",
    "    for blob in blobs:\n",
    "        model_lists.append(blob.name)\n",
    "\n",
    "    blob = bucket.blob(model_lists[-1])\n",
    "    blob_in = blob.download_as_string()\n",
    "    model_dict = pickle.loads(blob_in)\n",
    "    xgb_model = model_dict['model']\n",
    "    features = model_dict['features']\n",
    "    print('...... model loaded')\n",
    "    time.sleep(10)\n",
    "    \n",
    "    return xgb_model, features\n",
    "\n",
    "xgb_model, features = load_model(file_bucket = FILE_BUCKET, service_type = SERVICE_TYPE) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625bbaf0-6a67-4151-8504-a187037aa4f8",
   "metadata": {},
   "source": [
    "### backup codes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ced94be-d4ec-4eb5-8aed-8199f09bdbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "    token = os.popen('gcloud auth print-access-token').read()\n",
    "    token = re.sub(f'\\n$', '', token)\n",
    "    credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    if use_local_credential:\n",
    "        bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "    return bq_client\n",
    "\n",
    "client = get_gcp_bqclient(project_id)\n",
    "\n",
    "#instantiate df_target_train and df_target_test\n",
    "sql_train = ''' SELECT * FROM `{}.{}.bq_tos_cross_sell_targets_q3` '''.format(project_id, dataset_id)\n",
    "df_target_train = client.query(sql_train).to_dataframe()\n",
    "df_target_train = df_target_train.loc[\n",
    "    df_target_train['YEAR_MONTH'] == \"2022-Q3\"] #'-'.join(score_date_dash.split('-')[:2])]  # score_date_dash = '2022-08-31'\n",
    "\n",
    "#set up df_train and df_test (add 'target')\n",
    "df_target_train['ban'] = df_target_train['ban'].astype('int64')\n",
    "df_target_train = df_target_train.groupby('ban').tail(1)\n",
    "\n",
    "df_train = df_train.merge(df_target_train[['ban', 'product_crosssell_ind']], on='ban', how='left')\n",
    "df_train.rename(columns={'product_crosssell_ind': 'target'}, inplace=True)\n",
    "df_train.dropna(subset=['target'], inplace=True)\n",
    "df_train['target'] = df_train['target'].astype(int)\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32473d2-a75d-48ff-afc1-f34af20e2cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b9b389-df11-402a-ab09-20bd55af3d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "    token = os.popen('gcloud auth print-access-token').read()\n",
    "    token = re.sub(f'\\n$', '', token)\n",
    "    credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    if use_local_credential:\n",
    "        bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "    return bq_client\n",
    "\n",
    "client = get_gcp_bqclient(project_id)\n",
    "sql_test = ''' SELECT * FROM `{}.{}.bq_tos_cross_sell_targets_q4` '''.format(project_id, dataset_id)\n",
    "df_target_test = client.query(sql_test).to_dataframe()\n",
    "df_target_test = df_target_test.loc[\n",
    "    df_target_test['YEAR_MONTH'] == \"2022-Q4\"] #'-'.join(score_date_val_dash.split('-')[:2])]  # score_date_dash = '2022-09-30'\n",
    "\n",
    "#set up df_train and df_test (add 'target')\n",
    "df_target_test['ban'] = df_target_test['ban'].astype('int64')\n",
    "df_target_test = df_target_test.groupby('ban').tail(1)\n",
    "\n",
    "df_test = df_test.merge(df_target_test[['ban', 'product_crosssell_ind']], on='ban', how='left')\n",
    "df_test.rename(columns={'product_crosssell_ind': 'target'}, inplace=True)\n",
    "df_test.dropna(subset=['target'], inplace=True)\n",
    "df_test['target'] = df_test['target'].astype(int)\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62170f7-10b3-47f6-9610-7d758e68b6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_test['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426e602c-d3dd-4748-9617-410bc68b193e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
