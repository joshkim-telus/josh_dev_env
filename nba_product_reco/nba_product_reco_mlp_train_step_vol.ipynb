{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b0c603b-3f78-40d0-bc02-7dac3ea2a76c",
   "metadata": {},
   "source": [
    "### Import Libraries, declare variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf87789-5a0e-488a-a4bf-7969f104635e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "CREATE OR REPLACE TABLE `divg-groovyhoon-pr-d2eab4.nba_product_reco_model.nba_training_dataset` AS \n",
    "\n",
    "SELECT split_type\n",
    ", model_scenario\n",
    ", ref_dt\n",
    ", cust_id\n",
    ", cust_src_id\n",
    ", ban\n",
    ", ban_src_id\n",
    ", lpds_id\n",
    ", fms_address_id\n",
    ", label\n",
    ", label_dt\n",
    ", avg_usg_cnt_4w_news\n",
    ", avg_usg_dl_4w_news\n",
    ", avg_usg_ul_4w_news\n",
    ", avg_usg_cnt_4w_sports\n",
    ", avg_usg_dl_4w_sports\n",
    ", avg_usg_ul_4w_sports\n",
    ", `avg_usg_cnt_4w_tv and movies` AS avg_usg_cnt_4w_tv_and_movies\n",
    ", `avg_usg_dl_4w_tv and movies` AS avg_usg_dl_4w_tv_and_movies\n",
    ", `avg_usg_ul_4w_tv and movies` AS avg_usg_ul_4w_tv_and_movies\n",
    ", bill_wln_zscore_ban_subtotal_amt\n",
    ", CAST(bill_wln_avg_ban_subtotal_amt AS FLOAT64) AS bill_wln_avg_ban_subtotal_amt\n",
    ", bill_wln_zscore_ban_debit_amt\n",
    ", CAST(bill_wln_avg_ban_debit_amt AS FLOAT64) AS bill_wln_avg_ban_debit_amt\n",
    ", bill_wln_zscore_ban_discount_amt\n",
    ", CAST(bill_wln_avg_ban_discount_amt AS FLOAT64) AS bill_wln_avg_ban_discount_amt\n",
    ", bill_wln_zscore_ban_credit_amt\n",
    ", CAST(bill_wln_avg_ban_credit_amt AS FLOAT64) AS bill_wln_avg_ban_credit_amt\n",
    ", CAST(bill_wln_avg_sing_debit_amt AS FLOAT64) AS bill_wln_avg_sing_debit_amt\n",
    ", CAST(bill_wln_avg_sing_discount_amt AS FLOAT64) AS bill_wln_avg_sing_discount_amt\n",
    ", CAST(bill_wln_avg_sing_credit_amt AS FLOAT64) AS bill_wln_avg_sing_credit_amt\n",
    ", CAST(bill_wln_avg_hsic_debit_amt AS FLOAT64) AS bill_wln_avg_hsic_debit_amt\n",
    ", CAST(bill_wln_avg_hsic_discount_amt AS FLOAT64) AS bill_wln_avg_hsic_discount_amt\n",
    ", CAST(bill_wln_avg_hsic_credit_amt AS FLOAT64) AS bill_wln_avg_hsic_credit_amt\n",
    ", CAST(bill_wln_avg_ttv_debit_amt AS FLOAT64) AS bill_wln_avg_ttv_debit_amt\n",
    ", CAST(bill_wln_avg_ttv_discount_amt AS FLOAT64) AS bill_wln_avg_ttv_discount_amt\n",
    ", CAST(bill_wln_avg_ttv_credit_amt AS FLOAT64) AS bill_wln_avg_ttv_credit_amt\n",
    ", CAST(bill_wln_avg_smhm_debit_amt AS FLOAT64) AS bill_wln_avg_smhm_debit_amt\n",
    ", CAST(bill_wln_avg_smhm_discount_amt AS FLOAT64) AS bill_wln_avg_smhm_discount_amt\n",
    ", CAST(bill_wln_avg_smhm_credit_amt AS FLOAT64) AS bill_wln_avg_smhm_credit_amt\n",
    ", CAST(bill_wln_avg_sing_ld_na_call_cnt AS FLOAT64) AS bill_wln_avg_sing_ld_na_call_cnt\n",
    ", CAST(bill_wln_avg_sing_ld_intl_call_cnt AS FLOAT64) AS bill_wln_avg_sing_ld_intl_call_cnt\n",
    ", CAST(bill_wln_avg_hsic_usg_gb AS FLOAT64) AS bill_wln_avg_hsic_usg_gb\n",
    ", CAST(bill_wln_avg_ttv_ppv_cnt AS FLOAT64) AS bill_wln_avg_ttv_ppv_cnt\n",
    ", CAST(bill_wln_avg_ttv_vod_cnt AS FLOAT64) AS bill_wln_avg_ttv_vod_cnt\n",
    ", clk_wls_tot_cnt_r30d\n",
    ", clk_wls_plan_cnt_r30d\n",
    ", clk_wls_device_cnt_r30d\n",
    ", clk_wls_smartwatch_cnt_r30d\n",
    ", clk_wls_tablet_cnt_r30d\n",
    ", clk_wln_tot_cnt_r30d\n",
    ", clk_wln_eligibility_cnt_r30d\n",
    ", clk_wln_sing_cnt_r30d\n",
    ", clk_wln_hsic_cnt_r30d\n",
    ", clk_wln_fibre_cnt_r30d\n",
    ", clk_wln_whsia_cnt_r30d\n",
    ", clk_wln_wifi_plus_cnt_r30d\n",
    ", clk_wln_tv_cnt_r30d\n",
    ", clk_wln_optik_cnt_r30d\n",
    ", clk_wln_pik_cnt_r30d\n",
    ", clk_wln_streamplus_cnt_r30d\n",
    ", clk_wln_streaming_cnt_r30d\n",
    ", clk_wln_security_cnt_r30d\n",
    ", clk_wln_smarthome_security_cnt_r30d\n",
    ", clk_wln_online_security_cnt_r30d\n",
    ", clk_wln_smartwear_security_cnt_r30d\n",
    ", clk_deal_tot_cnt_r30d\n",
    ", clk_deal_wls_cnt_r30d\n",
    ", clk_deal_wln_cnt_r30d\n",
    ", clk_deal_wln_sing_cnt_r30d\n",
    ", clk_deal_wln_hsic_cnt_r30d\n",
    ", clk_deal_wln_whsia_cnt_r30d\n",
    ", clk_deal_wln_tv_cnt_r30d\n",
    ", clk_deal_wln_security_cnt_r30d\n",
    ", clk_upgr_tot_cnt_r30d\n",
    ", clk_upgr_wls_cnt_r30d\n",
    ", clk_upgr_wln_cnt_r30d\n",
    ", clk_upgr_wln_sing_cnt_r30d\n",
    ", clk_upgr_wln_hsic_cnt_r30d\n",
    ", clk_upgr_wln_whsia_cnt_r30d\n",
    ", clk_upgr_wln_tv_cnt_r30d\n",
    ", clk_upgr_wln_security_cnt_r30d\n",
    ", clk_chg_tot_cnt_r30d\n",
    ", clk_chg_wls_cnt_r30d\n",
    ", clk_chg_wln_cnt_r30d\n",
    ", clk_chg_wln_sing_cnt_r30d\n",
    ", clk_chg_wln_hsic_cnt_r30d\n",
    ", clk_chg_wln_whsia_cnt_r30d\n",
    ", clk_chg_wln_tv_cnt_r30d\n",
    ", clk_chg_wln_security_cnt_r30d\n",
    ", clk_health_livingwell_cnt_r30d\n",
    ", clk_health_mypet_cnt_r30d\n",
    ", clk_travel_cnt_r30d\n",
    ", clk_billing_cnt_r30d\n",
    ", clk_service_agreement_cnt_r30d\n",
    ", acct_cr_risk_txt\n",
    ", acct_ebill_ind\n",
    ", cust_cr_val_txt\n",
    ", cust_pref_lang_txt\n",
    ", cust_prov_state_cd\n",
    ", cust_age_yr_num\n",
    ", demogr_med_age\n",
    ", demogr_avg_child\n",
    ", demogr_pct_family_with_child_living_at_home\n",
    ", demogr_employment_rate\n",
    ", demogr_avg_household_size\n",
    ", demogr_avg_income\n",
    ", demogr_med_income\n",
    ", demogr_urban_flag\n",
    ", demogr_rural_flag\n",
    ", demogr_family_flag\n",
    ", demogr_lsname_large_diverse_families\n",
    ", demogr_lsname_younger_singles_and_couples\n",
    ", demogr_lsname_very_young_singles_and_couples\n",
    ", demogr_lsname_older_families_and_empty_nests\n",
    ", demogr_lsname_middle_age_families\n",
    ", demogr_lsname_mature_singles_and_couples\n",
    ", demogr_lsname_young_families\n",
    ", demogr_lsname_school_age_families\n",
    ", demogr_retired_pstl_cd_ind\n",
    ", demogr_census_division_typ\n",
    ", demogr_lifestage_sort\n",
    ", CAST(hs_usg_avg_tot_gb AS FLOAT64) AS hs_usg_avg_tot_gb\n",
    ", CAST(hs_usg_avg_dl_gb AS FLOAT64) AS hs_usg_avg_dl_gb\n",
    ", CAST(hs_usg_avg_ul_gb AS FLOAT64) AS hs_usg_avg_ul_gb\n",
    ", prod_latest_actvn_dt\n",
    ", prod_latest_deactvn_dt\n",
    ", prod_tot_cnt\n",
    ", prod_wln_cnt\n",
    ", prod_wls_cnt\n",
    ", prod_mob_cnt\n",
    ", prod_sing_cnt\n",
    ", prod_hsic_cnt\n",
    ", prod_whsia_cnt\n",
    ", prod_ttv_cnt\n",
    ", prod_smhm_cnt\n",
    ", prod_tos_cnt\n",
    ", prod_wifiplus_cnt\n",
    ", prod_stv_cnt\n",
    ", prod_other_cnt\n",
    ", prod_deact_prod_cnt\n",
    ", prod_act_prod_cnt_r7d\n",
    ", prod_act_wln_cnt_r7d\n",
    ", prod_deact_prod_cnt_r7d\n",
    ", prod_deact_wln_cnt_r7d\n",
    ", hsic_tenure_days\n",
    ", contract_end_date_hsic\n",
    ", sing_tenure_days\n",
    ", contract_end_date_sing\n",
    ", ttv_tenure_days\n",
    ", contract_end_date_ttv\n",
    ", smhm_tenure_days\n",
    ", contract_end_date_smhm\n",
    ", ffh_tenure\n",
    ", new_account_ind\n",
    "\n",
    "FROM `divg-groovyhoon-pr-d2eab4.nba_product_reco_model.nba_training_dataset` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1294f3f-ee1c-4c57-bbb1-635ef08b52d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import google\n",
    "from google.oauth2 import credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2.service_account import Credentials\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# build model\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "SERVICE_TYPE = 'nba_product_reco_model'\n",
    "DATASET_ID = 'nba_product_reco_model'\n",
    "PROJECT_ID = 'divg-groovyhoon-pr-d2eab4' #mapping['PROJECT_ID']\n",
    "RESOURCE_BUCKET = 'divg-groovyhoon-pr-d2eab4-default' #mapping['resources_bucket']\n",
    "FILE_BUCKET = 'divg-groovyhoon-pr-d2eab4-default' #mapping['gcs_csv_bucket']\n",
    "REGION = 'northamerica-northeast1' #mapping['REGION']\n",
    "MODEL_ID = '9999'\n",
    "FOLDER_NAME = 'nba_product_reco_model'.format(MODEL_ID)\n",
    "QUERIES_PATH = 'vertex_pipelines/' + FOLDER_NAME + '/queries/'\n",
    "TRAIN_TABLE_ID = 'nba_training_dataset_v7'\n",
    "VAL_TABLE_ID = 'nba_val_dataset_v7'\n",
    "TEST_TABLE_ID = 'nba_test_dataset_v7'\n",
    "SCORE_TABLE_ID = 'bq_product_reco_scores'\n",
    "\n",
    "# scoringDate = date(2023, 10, 13)  # date.today() - relativedelta(days=2)- relativedelta(months=30)\n",
    "# valScoringDate = date(2023, 11, 13)  # scoringDate - relativedelta(days=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b842a8-9b05-410f-8fe5-f6d722a8c78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30300f-ff71-417d-956f-6d3dcede6c29",
   "metadata": {},
   "source": [
    "### import bq to dataframe function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e098413-7211-4346-b1c9-eafd38ef076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import credentials\n",
    "\n",
    "def import_bq_to_dataframe(project_id, dataset_id, table_id, client): \n",
    "    \n",
    "    \"\"\"\n",
    "    Imports a specific table from BigQuery to a DataFrame. \n",
    "    \n",
    "    Args: \n",
    "        project_id: The name of the project_id where the table is located.\n",
    "        dataset_id: The name of the dataset_id where the table is located.\n",
    "        table_id: The name of the table_id you wish to import to DataFrame.\n",
    "        client: A BigQuery client instance. e.g. client = bigquery.Client(project=project_id).\n",
    "\n",
    "    Returns: \n",
    "        A DataFrame\n",
    "        \n",
    "    Example: \n",
    "        import_bq_to_dataframe('bi-stg-divg-speech-pr-9d940b', 'call_to_retention_dataset', 'bq_ctr_pipeline_dataset')\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    sql = f\"SELECT * FROM `{project_id}.{dataset_id}.{table_id}`\"\n",
    "    \n",
    "    df_return = client.query(sql).to_dataframe()\n",
    "\n",
    "    return df_return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a31e45-47f9-445e-aab4-1e2d8243e1f9",
   "metadata": {},
   "source": [
    "### define get_lift function, import df_train and df_test from gcs bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3837b0-0180-4a8a-978d-b56402062efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "project_id = PROJECT_ID\n",
    "region = REGION\n",
    "resource_bucket = RESOURCE_BUCKET\n",
    "file_bucket = FILE_BUCKET\n",
    "service_type=SERVICE_TYPE\n",
    "project_id=PROJECT_ID\n",
    "dataset_id=DATASET_ID\n",
    "train_table_id = TRAIN_TABLE_ID\n",
    "val_table_id = VAL_TABLE_ID\n",
    "test_table_id = TEST_TABLE_ID\n",
    "\n",
    "def get_lift(prob, y_test, q):\n",
    "    result = pd.DataFrame(columns=['Prob', 'Churn'])\n",
    "    result['Prob'] = prob\n",
    "    result['Churn'] = y_test\n",
    "    # result['Decile'] = pd.qcut(1-result['Prob'], 10, labels = False)\n",
    "    result['Decile'] = pd.qcut(result['Prob'], q, labels=[i for i in range(q, 0, -1)])\n",
    "    add = pd.DataFrame(result.groupby('Decile')['Churn'].mean()).reset_index()\n",
    "    add.columns = ['Decile', 'avg_real_churn_rate']\n",
    "    result = result.merge(add, on='Decile', how='left')\n",
    "    result.sort_values('Decile', ascending=True, inplace=True)\n",
    "    lg = pd.DataFrame(result.groupby('Decile')['Prob'].mean()).reset_index()\n",
    "    lg.columns = ['Decile', 'avg_model_pred_churn_rate']\n",
    "    lg.sort_values('Decile', ascending=False, inplace=True)\n",
    "    lg['avg_churn_rate_total'] = result['Churn'].mean()\n",
    "    lg = lg.merge(add, on='Decile', how='left')\n",
    "    lg['lift'] = lg['avg_real_churn_rate'] / lg['avg_churn_rate_total']\n",
    "\n",
    "    return lg\n",
    "\n",
    "def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "    token = os.popen('gcloud auth print-access-token').read()\n",
    "    token = re.sub(f'\\n$', '', token)\n",
    "    credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    if use_local_credential:\n",
    "        bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "    return bq_client\n",
    "\n",
    "client = get_gcp_bqclient(project_id)\n",
    "\n",
    "df_train = import_bq_to_dataframe(project_id, dataset_id, train_table_id, client) #-- Jan to Oct \n",
    "df_val = import_bq_to_dataframe(project_id, dataset_id, val_table_id, client) #-- Nov and Dec\n",
    "df_test = import_bq_to_dataframe(project_id, dataset_id, test_table_id, client) #-- Nov and Dec\n",
    "\n",
    "scenario_to_target = {\n",
    "    'hsic_acquisition': 0,\n",
    "    'ttv_acquisition': 1,\n",
    "    'sing_acquisition': 2,\n",
    "    'shs_acquisition': 3, \n",
    "    'tos_acquisition': 4, \n",
    "    'wifi_acquisition': 5, \n",
    "    'lwc_acquisition': 6, \n",
    "    'sws_acquisition': 7, \n",
    "    'hpro_acquisition': 8, \n",
    "    'whsia_acquisition': 9, \n",
    "    'ttv_upsell': 10, \n",
    "    'shs_renewal': 11, \n",
    "    'shs_upsell': 12\n",
    "    #'mobility_acquisition': 8,\n",
    "    #'tos_upsell': 8\n",
    "}\n",
    "\n",
    "df_train['target'] = df_train['model_scenario'].map(scenario_to_target)\n",
    "df_val['target'] = df_val['model_scenario'].map(scenario_to_target)\n",
    "df_test['target'] = df_test['model_scenario'].map(scenario_to_target)\n",
    "\n",
    "print(f'df_train: {df_train.shape}')\n",
    "print(f'df_val: {df_val.shape}')\n",
    "print(f'df_test: {df_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c652ae8-e1ab-4aa4-8d54-47361e239156",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['demogr_lifestage_sort'].fillna(6, inplace=True)\n",
    "df_train['cust_pref_lang_txt'].fillna('NOT AVAILABLE', inplace=True)\n",
    "df_train['demogr_census_division_typ'].fillna('NA', inplace=True)\n",
    "df_train['cust_prov_state_cd'].fillna('N/AVAIL', inplace=True)\n",
    "df_train['cust_cr_val_txt'].fillna('NOT AVAILABLE', inplace=True)\n",
    "df_train['acct_ebill_ind'].fillna('N', inplace=True)\n",
    "\n",
    "df_val['demogr_lifestage_sort'].fillna(6, inplace=True)\n",
    "df_val['cust_pref_lang_txt'].fillna('NOT AVAILABLE', inplace=True)\n",
    "df_val['demogr_census_division_typ'].fillna('NA', inplace=True)\n",
    "df_val['cust_prov_state_cd'].fillna('N/AVAIL', inplace=True)\n",
    "df_val['cust_cr_val_txt'].fillna('NOT AVAILABLE', inplace=True)\n",
    "df_val['acct_ebill_ind'].fillna('N', inplace=True)\n",
    "\n",
    "df_test['demogr_lifestage_sort'].fillna(6, inplace=True)\n",
    "df_test['cust_pref_lang_txt'].fillna('NOT AVAILABLE', inplace=True)\n",
    "df_test['demogr_census_division_typ'].fillna('NA', inplace=True)\n",
    "df_test['cust_prov_state_cd'].fillna('N/AVAIL', inplace=True)\n",
    "df_test['cust_cr_val_txt'].fillna('NOT AVAILABLE', inplace=True)\n",
    "df_test['acct_ebill_ind'].fillna('N', inplace=True)\n",
    "\n",
    "print(f'df_train: {df_train.shape}')\n",
    "print(f'df_val: {df_val.shape}')\n",
    "print(f'df_test: {df_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94961cc0-7e92-44eb-9e1e-15d16d2024ed",
   "metadata": {},
   "source": [
    "### add targets to df_train and df_target \n",
    "\n",
    "- df_target_train is from `divg-josh-pr-d1cc3a.tos_crosssell.bq_tos_cross_sell_targets_q3` \n",
    "- df_target_test is from `divg-josh-pr-d1cc3a.tos_crosssell.bq_tos_cross_sell_targets_q4` \n",
    "- some parts of the code and sql queries need to be dynamically adjusted to be included in the deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc272d97-768b-401b-9100-923cc63cf1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "# df train - Jan to Aug \n",
    "# df train - Sep to Oct \n",
    "df_train, df_test = train_test_split(df_train, test_size=0.3, random_state=42, stratify=df_train['target'])\n",
    "\n",
    "# df_train.to_csv('gs://{}/{}/{}_train_final.csv'.format(FILE_BUCKET, SERVICE_TYPE, SERVICE_TYPE), index=False)\n",
    "# df_test.to_csv('gs://{}/{}/{}_test_final.csv'.format(FILE_BUCKET, SERVICE_TYPE, SERVICE_TYPE), index=False)\n",
    "# df_val.to_csv('gs://{}/{}/{}_val_final.csv'.format(FILE_BUCKET, SERVICE_TYPE, SERVICE_TYPE), index=False)\n",
    "\n",
    "#set up features (list)\n",
    "cols_1 = df_train.columns.values\n",
    "cols_2 = df_val.columns.values\n",
    "cols_3 = df_test.columns.values\n",
    "\n",
    "cols = set(cols_1).intersection(set(cols_2))\n",
    "cols = set(cols).intersection(set(cols_3))\n",
    "\n",
    "features_to_exclude = ['split_type','model_scenario','ref_dt','cust_id','cust_src_id','ban','ban_src_id','lpds_id',\n",
    "                       'fms_address_id','label','label_dt', 'prod_latest_actvn_dt', 'prod_latest_deactvn_dt', 'target', \n",
    "                       'contract_end_date_hsic', 'contract_end_date_hsic', 'contract_end_date_sing', 'contract_end_date_ttv', 'contract_end_date_smhm'] \n",
    "\n",
    "features = [f for f in cols if f not in features_to_exclude]\n",
    "\n",
    "ban_train = df_train[['ban', 'lpds_id']]\n",
    "X_train = df_train[features]\n",
    "y_train = np.squeeze(df_train['target'].values)\n",
    "target_train = df_train['target']\n",
    "\n",
    "ban_val = df_val[['ban', 'lpds_id']]\n",
    "X_val = df_val[features]\n",
    "y_val = np.squeeze(df_val['target'].values)\n",
    "target_val = df_val['target']\n",
    "\n",
    "ban_test = df_test[['ban', 'lpds_id']]\n",
    "X_test = df_test[features]\n",
    "y_test = np.squeeze(df_test['target'].values)\n",
    "target_test = df_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee477829-f647-4302-b142-67f33d33dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a591fa94-0f6d-47a7-9bb5-53d4a39ca42b",
   "metadata": {},
   "source": [
    "### preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38820930-8f64-4d21-bd82-54aef8e6d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to transform the features of the feature store.\n",
    "def encode_categorical_features(train_df, test_df, val_df):\n",
    "    # Get a list of all categorical columns\n",
    "    cat_columns = train_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # Encode each categorical column\n",
    "    for col in cat_columns:\n",
    "        le = LabelEncoder()\n",
    "        train_df[col] = le.fit_transform(train_df[col])\n",
    "        test_df[col] = le.fit_transform(test_df[col])\n",
    "        val_df[col] = le.fit_transform(val_df[col])\n",
    "        \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "#excluding the customer ID so it doesn't get encoded\n",
    "train_label_data=X_train[X_train.columns.difference(['cust_id','ban','lpds_id','ref_dt','model_scenario'])]\n",
    "val_label_data=X_val[X_val.columns.difference(['cust_id','ban','lpds_id','ref_dt','model_scenario'])]\n",
    "test_label_data=X_test[X_test.columns.difference(['cust_id','ban','lpds_id','ref_dt','model_scenario'])]\n",
    "\n",
    "X_train, X_val, X_test = encode_categorical_features(train_label_data, val_label_data, test_label_data)\n",
    "\n",
    "#set up features (list)\n",
    "cols_1 = X_train.columns.values\n",
    "cols_2 = X_val.columns.values\n",
    "cols_3 = X_test.columns.values\n",
    "\n",
    "cols = set(cols_1).intersection(set(cols_2))\n",
    "cols = set(cols).intersection(set(cols_3))\n",
    "\n",
    "features = [f for f in cols if f not in features_to_exclude]\n",
    "\n",
    "# features = ['contract_type_MTM'\n",
    "# , 'contract_type_BYOD'\n",
    "# , 'contract_type_On_Contract'\n",
    "# , 'cntrct_end_recency'\n",
    "# , 'cntrct_start_recency'\n",
    "# , 'ban_tenure'\n",
    "# , 'contract_mth'\n",
    "# , 'urbn_rur_ind_Rural'\n",
    "# , 'sub_tenure'\n",
    "# , 'urbn_rur_ind_Urban'\n",
    "# , 'dvc_non_telus_ind_Y'\n",
    "# , 'demo_lsname_Large_Diverse_Families'\n",
    "# , 'dvc_non_telus_ind_N'\n",
    "# , 'demo_sgname_Diverse_Urban_Fringe'\n",
    "# , 'demo_sgname_Lower_Middle_Rural'\n",
    "# , 'data_usg_trend_unknown'\n",
    "# , 'demo_sgname_Midscale_Urban_Fringe'\n",
    "# , 'clk_app_offer'\n",
    "# , 'bacct_delinq_ind_Y'\n",
    "# , 'revenue_band_D'\n",
    "# , 'payment_mthd_R'\n",
    "# , 'clk_app_usage'\n",
    "# , 'demo_sgname_Upper_Middle_Suburbia'\n",
    "# , 'demo_sgname_Urban_Diversity'\n",
    "# , 'easy_pymt_avg'\n",
    "# , 'rate_plan_amt'\n",
    "# , 'demo_sgname_Upscale_Suburban_Diversity'\n",
    "# , 'sub_cnt'\n",
    "# , 'clk_app_ovrview'\n",
    "# , 'credit_class_D'\n",
    "# , 'demo_sgname_Town_Mix'\n",
    "# , 'demo_sgname_Young_Urban_Core'\n",
    "# , 'payment_mthd_C'\n",
    "# , 'bacct_delinq_ind_N'\n",
    "# , 'artm_min_qty_avg'\n",
    "# , 'credit_class_V'\n",
    "# , 'hm_call_cnt_avg'\n",
    "# , 'wls_data_shr_plan_ind_Y'\n",
    "# , 'demo_lsname_Older_Families_and_Empty_Nests'\n",
    "# , 'dnc_sms_ind_N'\n",
    "# , 'demo_lsname_School_Age_Families'\n",
    "# , 'credit_class_X'\n",
    "# , 'clk_web_plan'\n",
    "# , 'hm_data_usg_avg'\n",
    "# , 'revenue_band_F'\n",
    "# , 'demo_lsname_Young_Families'\n",
    "# , 'demo_lsname_Middle_Age_Families'\n",
    "# , 'clk_app_subslct'\n",
    "# , 'cr_disc_amt_avg'\n",
    "# , 'tot_data_usg_avg'\n",
    "# , 'age'\n",
    "# , 'demo_avg_income'\n",
    "# , 'data_usg_trend_increasing'\n",
    "# , 'revenue_band_N'\n",
    "# , 'revenue_band_C'\n",
    "# , 'dvc_non_telus_ind_U'\n",
    "# , 'wls_data_plan_ind_N'\n",
    "# , 'ebill_ind_N'\n",
    "# , 'net_inv_amt_avg'\n",
    "# , 'clk_app_bill'\n",
    "# , 'clk_app_changefg'\n",
    "# , 'revenue_band_NA'\n",
    "# , 'demo_sgname_Older_Urban_Francophone'\n",
    "# , 'demo_sgname_Upper_Middle_Rural'\n",
    "# , 'sms_dnc_recency'\n",
    "# , 'demo_sgname_Upper_Middle_Suburban_Francophone'\n",
    "# , 'demo_sgname_Unassigned'\n",
    "# , 'clk_web_bill'\n",
    "# , 'clk_app_drawer'\n",
    "# , 'demo_sgname_Rural_Francophone'\n",
    "# , 'clk_web_phnumber'\n",
    "# , 'ld_min_qty_avg'\n",
    "# , 'em_dnc_recency'\n",
    "# , 'effort_durtn_sec_qty_s'\n",
    "# ]\n",
    "\n",
    "X_train = X_train[features] \n",
    "X_val = X_val[features]\n",
    "X_test = X_test[features] \n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fb3c25-0efe-4842-9dd2-53da180d7155",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# del df_train, df_val, df_test\n",
    "del df_train, df_test, df_val\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfd29e3-d1b3-4cbe-983c-b0033ed8554a",
   "metadata": {},
   "source": [
    "### fit training data in xgboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7616d7-5909-4cb8-8e74-821d3e92dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build model and fit in training data\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=1000,\n",
    "    max_depth=10, \n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='multi:softproba',\n",
    "    num_class=3, \n",
    "    eval_metric='mlogloss', \n",
    "    nthread=4,\n",
    "    scale_pos_weight=1,\n",
    "    seed=27,\n",
    "    early_stopping_rounds=10,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# xgb_model = xgb.XGBClassifier(\n",
    "#     learning_rate=0.1,\n",
    "#     n_estimators=1000,\n",
    "#     max_depth=5,\n",
    "#     min_child_weight=1,\n",
    "#     gamma=0,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     objective='multi:softproba',\n",
    "#     num_class=3, \n",
    "#     eval_metric='mlogloss', \n",
    "#     nthread=4,\n",
    "#     scale_pos_weight=1,\n",
    "#     seed=27\n",
    "# )\n",
    "\n",
    "# xgb_model.fit(X_train, y_train)\n",
    "\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)]) \n",
    "\n",
    "\n",
    "print('xgb training done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5abc939-f3aa-408a-b3a6-0beb1ff89c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca769f9-add0-4dc1-b9ee-b773e64f1573",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_model.predict_proba(X_val, ntree_limit=xgb_model.best_iteration)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbafd6fe-c34b-451b-8d42-c8d4e80e9584",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_0 = y_pred[:, 0]\n",
    "y_pred_1 = y_pred[:, 1]\n",
    "y_pred_2 = y_pred[:, 2]\n",
    "y_pred_3 = y_pred[:, 3]\n",
    "y_pred_4 = y_pred[:, 4]\n",
    "y_pred_5 = y_pred[:, 5]\n",
    "y_pred_6 = y_pred[:, 6]\n",
    "y_pred_7 = y_pred[:, 7]\n",
    "y_pred_8 = y_pred[:, 8]\n",
    "y_pred_9 = y_pred[:, 9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42baa80-bee6-4aa7-80e3-2c341142c818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #predictions on X_val\n",
    "# y_pred = xgb_model.predict_proba(X_val, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "\n",
    "df_ban_val = ban_val\n",
    "df_val_exp = df_ban_val.join(X_val) \n",
    "df_val_exp['y_test'] = y_val\n",
    "df_val_exp['y_pred_proba_0'] = y_pred_0\n",
    "df_val_exp['y_pred_proba_1'] = y_pred_1\n",
    "df_val_exp['y_pred_proba_2'] = y_pred_2\n",
    "df_val_exp['y_pred_proba_3'] = y_pred_3\n",
    "df_val_exp['y_pred_proba_4'] = y_pred_4\n",
    "df_val_exp['y_pred_proba_5'] = y_pred_5\n",
    "df_val_exp['y_pred_proba_6'] = y_pred_6\n",
    "df_val_exp['y_pred_proba_7'] = y_pred_7\n",
    "df_val_exp['y_pred_proba_8'] = y_pred_8\n",
    "df_val_exp['y_pred_proba_9'] = y_pred_9\n",
    "# df_val_exp['y_pred_proba_10'] = y_pred_10\n",
    "# df_val_exp['y_pred_proba_11'] = y_pred_11\n",
    "# df_val_exp['y_pred_proba_12'] = y_pred_12\n",
    "\n",
    "\n",
    "# df_val_exp['y_pred'] = (df_val_exp['y_pred_proba'] > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e194cad1-eba7-46ad-8052-692b08f32107",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_exp.to_csv(\"gs://divg-groovyhoon-pr-d2eab4-default/downloads/df_val_exp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2217c02c-846f-4bec-9b92-0819257a812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_model.predict_proba(X_test, ntree_limit=xgb_model.best_iteration)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81897c94-69be-4066-9613-8509dcaaafae",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_0 = y_pred[:, 0]\n",
    "y_pred_1 = y_pred[:, 1]\n",
    "y_pred_2 = y_pred[:, 2]\n",
    "y_pred_3 = y_pred[:, 3]\n",
    "y_pred_4 = y_pred[:, 4]\n",
    "y_pred_5 = y_pred[:, 5]\n",
    "y_pred_6 = y_pred[:, 6]\n",
    "y_pred_7 = y_pred[:, 7]\n",
    "y_pred_8 = y_pred[:, 8]\n",
    "y_pred_9 = y_pred[:, 9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291408bb-7da9-4100-a8f4-a6cc5bea02d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #predictions on X_test\n",
    "# y_pred = xgb_model.predict_proba(X_test, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "\n",
    "df_ban_test = ban_test\n",
    "df_test_exp = df_ban_test.join(X_test) \n",
    "df_test_exp['y_test'] = y_test\n",
    "df_test_exp['y_pred_proba_0'] = y_pred_0\n",
    "df_test_exp['y_pred_proba_1'] = y_pred_1\n",
    "df_test_exp['y_pred_proba_2'] = y_pred_2\n",
    "df_test_exp['y_pred_proba_3'] = y_pred_3\n",
    "df_test_exp['y_pred_proba_4'] = y_pred_4\n",
    "df_test_exp['y_pred_proba_5'] = y_pred_5\n",
    "df_test_exp['y_pred_proba_6'] = y_pred_6\n",
    "df_test_exp['y_pred_proba_7'] = y_pred_7\n",
    "df_test_exp['y_pred_proba_8'] = y_pred_8\n",
    "\n",
    "df_test_exp['y_pred_proba_9'] = y_pred_9\n",
    "df_test_exp['y_pred_proba_10'] = y_pred_10\n",
    "df_test_exp['y_pred_proba_11'] = y_pred_11\n",
    "df_test_exp['y_pred_proba_12'] = y_pred_12\n",
    "\n",
    "# df_test_exp['y_pred'] = (df_test_exp['y_pred_proba'] > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a79a59f-8a7c-42b9-81d2-06e4854ea513",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_exp.to_csv(\"gs://divg-groovyhoon-pr-d2eab4-default/downloads/df_test_exp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5858d0ad-ecc2-4ed5-bca8-cede4aa49bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from xgboost model\n",
    "importances = xgb_model.feature_importances_\n",
    "\n",
    "# Get the index of importances from greatest importance to least\n",
    "sorted_index = np.argsort(importances)[::-1]\n",
    "x = range(len(importances))\n",
    "\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create tick labels \n",
    "labels = np.array(feature_names)[sorted_index]\n",
    "\n",
    "for idx in sorted_index: \n",
    "    print(f'{feature_names[idx]}, {importances[idx]}', end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0bb1d3-4362-47c1-ac15-94c0b9299d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716cb25e-044f-4479-a78f-f825702ba5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create an XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=3)\n",
    "\n",
    "# Define the hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'n_estimators': [30, 50, 100, 200, 500, 1000],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'subsample': [0.8, 0.9],\n",
    "    'colsample_bytree': [0.8, 0.9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'eval_metric': ['mlogloss']\n",
    "}\n",
    "\n",
    "# Use GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on Test Set: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdabeac-b4ad-44d8-8dcf-9cd2dbc31c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_model.best_model(X_val)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1ab70b-10a5-402e-b9f6-4d857dcbe927",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_0 = y_pred[:, 0]\n",
    "y_pred_1 = y_pred[:, 1]\n",
    "y_pred_2 = y_pred[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccd0f27-b35d-415e-8f4c-3a0483e0b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #predictions on X_val\n",
    "# y_pred = xgb_model.predict_proba(X_val, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "\n",
    "df_ban_val = ban_val\n",
    "df_val_exp = df_ban_val.join(X_val) \n",
    "df_val_exp['y_val'] = y_val\n",
    "df_val_exp['y_pred_proba_0'] = y_pred_0\n",
    "df_val_exp['y_pred_proba_1'] = y_pred_1\n",
    "df_val_exp['y_pred_proba_2'] = y_pred_2\n",
    "\n",
    "# df_test_exp['y_pred'] = (df_test_exp['y_pred_proba'] > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1cb419-d530-4d37-83a1-19343b27cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_exp.to_csv(\"gs://divg-groovyhoon-pr-d2eab4-default/downloads/df_val_exp_v2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0ea7ec-74db-4419-84bb-0d44545fec56",
   "metadata": {},
   "source": [
    "### make predictions on X_test set, assign deciles to the predicted values, and save in df_test_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3c55fb-754b-4fa9-b0ac-cdcf45a3dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions on X_test\n",
    "y_pred = xgb_model.predict_proba(X_test, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "\n",
    "#join ban_test, X_test, y_test and y_pred and print to csv\n",
    "#CHECK THE SIZE OF EACH COMPONENT BEFORE JOINING\n",
    "q=10\n",
    "df_ban_test = ban_test\n",
    "df_test_exp = df_ban_test.join(X_test) \n",
    "df_test_exp['y_test'] = y_test\n",
    "df_test_exp['y_pred_proba'] = y_pred\n",
    "df_test_exp['y_pred'] = (df_test_exp['y_pred_proba'] > 0.5).astype(int)\n",
    "df_test_exp['decile'] = pd.qcut(df_test_exp['y_pred_proba'], q, labels=[i for i in range(q, 0, -1)])\n",
    "\n",
    "lg = get_lift(y_pred, y_test, q)\n",
    "\n",
    "lg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2767b8f5-8bdb-4d5f-9b5b-2b340289cb94",
   "metadata": {
    "tags": []
   },
   "source": [
    "### export df_test_exp and lift scores to gcs bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2015d1-d1a5-410f-95c5-2b5054e2abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_exp.to_csv('gs://{}/{}/{}_df_test_exp.csv'.format(file_bucket, SERVICE_TYPE, SERVICE_TYPE, index=True))\n",
    "print(\"....df_test_exp exported\")\n",
    "\n",
    "lg.to_csv('gs://{}/{}/{}_lift_on_test_data.csv'.format(file_bucket, SERVICE_TYPE, SERVICE_TYPE, index=False))\n",
    "print(\"....lift_to_csv done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a67377-2467-47f9-aa71-2063716eda1f",
   "metadata": {},
   "source": [
    "### make predictions on X_val set, assign deciles to the predicted values, and save in df_val_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7231aba-49a5-4a16-9c57-2061ebe27892",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions on X_val\n",
    "y_pred = xgb_model.predict_proba(X_val, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "\n",
    "#join ban_val, X_val, y_val and y_pred and print to csv\n",
    "#CHECK THE SIZE OF EACH COMPONENT BEFORE JOINING\n",
    "q=10\n",
    "df_ban_val = ban_val\n",
    "df_val_exp = df_ban_val.join(X_val) \n",
    "df_val_exp['y_val'] = y_val\n",
    "df_val_exp['y_pred_proba'] = y_pred\n",
    "df_val_exp['y_pred'] = (df_val_exp['y_pred_proba'] > 0.5).astype(int)\n",
    "df_val_exp['decile'] = pd.qcut(df_val_exp['y_pred_proba'], q, labels=[i for i in range(q, 0, -1)])\n",
    "\n",
    "lg = get_lift(y_pred, y_val, q)\n",
    "\n",
    "lg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca9c17-8af8-4f29-b3a1-d35a1c50cf19",
   "metadata": {},
   "source": [
    "### export df_val_exp and lift scores to gcs bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570c3887-fc3a-44a7-b037-4bdfd61eb568",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_exp.to_csv('gs://{}/{}/{}_df_val_exp.csv'.format(file_bucket, SERVICE_TYPE, SERVICE_TYPE, index=True))\n",
    "print(\"....df_val_exp exported\")\n",
    "\n",
    "lg.to_csv('gs://{}/{}/{}_lift_on_val_data.csv'.format(file_bucket, SERVICE_TYPE, SERVICE_TYPE, index=False))\n",
    "print(\"....lift_to_csv done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8864bda6-bda0-4032-9704-f513da84e132",
   "metadata": {},
   "source": [
    "### get feature importances from xgboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43768ef0-d6cc-40e4-a3a9-dbcdbceedb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from xgboost model\n",
    "importances = xgb_model.feature_importances_\n",
    "\n",
    "# Get the index of importances from greatest importance to least\n",
    "sorted_index = np.argsort(importances)[::-1]\n",
    "x = range(len(importances))\n",
    "\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create tick labels \n",
    "labels = np.array(feature_names)[sorted_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb976c57-68c2-4d13-84c6-7f138ae560bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in sorted_index: \n",
    "    print(f'{feature_names[idx]}, {importances[idx]}', end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f8515b-07cd-4c76-9d79-944145ef3de1",
   "metadata": {},
   "source": [
    "### Load results to BQ - WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514b9b52-17c6-4c7f-8a36-9afdfc17f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORE_TABLE_ID = 'bq_telus_12_months_churn_scores'\n",
    "\n",
    "project_id = PROJECT_ID \n",
    "dataset_id = DATASET_ID\n",
    "score_table_id = SCORE_TABLE_ID\n",
    "\n",
    "# get full score to cave into bucket\n",
    "pred_prob = xgb_model.predict_proba(X_val, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "result = pd.DataFrame(columns=['ban', 'subscriber_no', 'score_date', 'y_true', 'y_pred'])\n",
    "\n",
    "result['ban'] = list(ban_val['ban'])\n",
    "result['ban'] = result['ban'].astype('str')\n",
    "\n",
    "result['subscriber_no'] = list(ban_val['ban'])\n",
    "result['subscriber_no'] = result['subscriber_no'].astype('str')\n",
    "\n",
    "result['score_date'] = \"2023-11-04\"\n",
    "\n",
    "result['y_true'] = list(y_val)\n",
    "result['y_true'] = result['y_true'].fillna(0.0).astype('float64')\n",
    "\n",
    "result['y_pred'] = list(pred_prob)\n",
    "result['y_pred'] = result['y_pred'].fillna(0.0).astype('float64')\n",
    "\n",
    "############# updated up to here ############\n",
    "\n",
    "result.to_csv('gs://{}/{}/ucar/{}_prediction_v2.csv'.format(file_bucket, service_type, service_type), index=False)\n",
    "\n",
    "# define dtype_bq_mapping\n",
    "dtype_bq_mapping = {np.dtype('int64'): 'INTEGER', \n",
    "np.dtype('float64'):  'FLOAT', \n",
    "np.dtype('float32'):  'FLOAT', \n",
    "np.dtype('object'):  'STRING', \n",
    "np.dtype('bool'):  'BOOLEAN', \n",
    "np.dtype('datetime64[ns]'):  'DATE', \n",
    "pd.Int64Dtype(): 'INTEGER'} \n",
    "\n",
    "# export df_final to bigquery \n",
    "schema_list = [] \n",
    "for column in result.columns: \n",
    "    schema_list.append(bigquery.SchemaField(column, dtype_bq_mapping[result.dtypes[column]], mode='NULLABLE')) \n",
    "print(schema_list) \n",
    "\n",
    "dest_table = f'{project_id}.{dataset_id}.{score_table_id}'\n",
    "\n",
    "# Sending to bigquery \n",
    "client = get_gcp_bqclient(project_id)\n",
    "job_config = bigquery.LoadJobConfig(schema=schema_list, write_disposition='WRITE_TRUNCATE') \n",
    "job = client.load_table_from_dataframe(result, dest_table, job_config=job_config) \n",
    "job.result() \n",
    "\n",
    "table = client.get_table(dest_table) # Make an API request \n",
    "print(\"Loaded {} rows and {} columns to {}\".format(table.num_rows, len(table.schema), table_id)) \n",
    "\n",
    "time.sleep(60)\n",
    "\n",
    "# table_ref = f'{project_id}.{dataset_id}.{score_table}'\n",
    "# client = bigquery.Client(project=project_id)\n",
    "# table = client.get_table(table_ref)\n",
    "# schema = table.schema\n",
    "\n",
    "# ll = []\n",
    "# for item in schema:\n",
    "#     col = item.name\n",
    "#     d_type = item.field_type\n",
    "#     if 'float' in str(d_type).lower():\n",
    "#         d_type = 'FLOAT64'\n",
    "#     ll.append((col, d_type))\n",
    "\n",
    "#     if 'integer' in str(d_type).lower():\n",
    "#         result[col] = result[col].fillna(0).astype(int)\n",
    "#     if 'float' in str(d_type).lower():\n",
    "#         result[col] = result[col].fillna(0.0).astype(float)\n",
    "#     if 'string' in str(d_type).lower():\n",
    "#         result[col] = result[col].fillna('').astype(str)\n",
    "\n",
    "# table_ref = '{}.{}.{}'.format(project_id, dataset_id, temp_table)\n",
    "# client = bigquery.Client(project=project_id)\n",
    "# if if_tbl_exists(client, table_ref):\n",
    "#     client.delete_table(table_ref)\n",
    "\n",
    "# client.create_table(table_ref)\n",
    "# config = bigquery.LoadJobConfig(schema=schema)\n",
    "# config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "# bq_table_instance = client.load_table_from_dataframe(result, table_ref, job_config=config)\n",
    "# time.sleep(5)\n",
    "\n",
    "# drop_sql = f\"\"\"delete from `{project_id}.{dataset_id}.{score_table}` where score_date = '{score_date_dash}'\"\"\"  # .format(project_id, dataset_id, score_date_dash)\n",
    "# client.query(drop_sql)\n",
    "# #\n",
    "# load_sql = f\"\"\"insert into `{project_id}.{dataset_id}.{score_table}`\n",
    "#               select * from `{project_id}.{dataset_id}.{temp_table}`\"\"\"\n",
    "# client.query(load_sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bf0cd1-afbc-42e6-9a2f-fd8b727c2cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce20d37b-0e62-4c81-b446-a88539baf2d9",
   "metadata": {},
   "source": [
    "### save the model in gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46e0caa-94a8-497c-a395-15a1ac041f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model in GCS\n",
    "from datetime import datetime\n",
    "models_dict = {}\n",
    "create_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "models_dict['create_time'] = create_time\n",
    "models_dict['model'] = xgb_model\n",
    "models_dict['features'] = features\n",
    "\n",
    "with open('model_dict.pkl', 'wb') as handle:\n",
    "    pickle.dump(models_dict, handle)\n",
    "handle.close()\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(file_bucket)\n",
    "\n",
    "MODEL_PATH = '{}_xgb_models/'.format(service_type)\n",
    "blob = bucket.blob(MODEL_PATH)\n",
    "if not blob.exists(storage_client):\n",
    "    blob.upload_from_string('')\n",
    "\n",
    "model_name_onbkt = '{}{}_models_xgb_{}'.format(MODEL_PATH, service_type, models_dict['create_time'])\n",
    "blob = bucket.blob(model_name_onbkt)\n",
    "blob.upload_from_filename('model_dict.pkl')\n",
    "\n",
    "print(f\"....model loaded to GCS done at {str(create_time)}\")\n",
    "\n",
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c02b2b6-b059-4dde-9393-ad457eebf60e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3d9d01-f009-45c6-90fe-95a61cc169cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb40188a-3865-4993-be01-fb27f7d07dd6",
   "metadata": {},
   "source": [
    "### load the latest saved xgb_model to the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f240186-29e9-4d12-8c11-bb468cae8067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH = '{}_xgb_models/'.format(service_type)\n",
    "# df_score = pd.read_csv('gs://{}/{}_score.csv.gz'.format(file_bucket, service_type), compression='gzip')\n",
    "# df_score.dropna(subset=['ban'], inplace=True)\n",
    "# df_score.reset_index(drop=True, inplace=True)\n",
    "# print('......scoring data loaded:{}'.format(df_score.shape))\n",
    "# time.sleep(10)\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "MODEL_PATH = '{}_xgb_models/'.format(service_type)\n",
    "\n",
    "def load_model(file_bucket: str, service_type: str): \n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(file_bucket)\n",
    "    blobs = storage_client.list_blobs(file_bucket, prefix='{}{}_models_xgb_'.format(MODEL_PATH, service_type))\n",
    "\n",
    "    model_lists = []\n",
    "    for blob in blobs:\n",
    "        model_lists.append(blob.name)\n",
    "\n",
    "    blob = bucket.blob(model_lists[-1])\n",
    "    blob_in = blob.download_as_string()\n",
    "    model_dict = pickle.loads(blob_in)\n",
    "    xgb_model = model_dict['model']\n",
    "    features = model_dict['features']\n",
    "    print('...... model loaded')\n",
    "    time.sleep(10)\n",
    "    \n",
    "    return xgb_model, features\n",
    "\n",
    "xgb_model, features = load_model(file_bucket = FILE_BUCKET, service_type = SERVICE_TYPE) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625bbaf0-6a67-4151-8504-a187037aa4f8",
   "metadata": {},
   "source": [
    "### backup codes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ced94be-d4ec-4eb5-8aed-8199f09bdbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "    token = os.popen('gcloud auth print-access-token').read()\n",
    "    token = re.sub(f'\\n$', '', token)\n",
    "    credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    if use_local_credential:\n",
    "        bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "    return bq_client\n",
    "\n",
    "client = get_gcp_bqclient(project_id)\n",
    "\n",
    "#instantiate df_target_train and df_target_test\n",
    "sql_train = ''' SELECT * FROM `{}.{}.bq_tos_cross_sell_targets_q3` '''.format(project_id, dataset_id)\n",
    "df_target_train = client.query(sql_train).to_dataframe()\n",
    "df_target_train = df_target_train.loc[\n",
    "    df_target_train['YEAR_MONTH'] == \"2022-Q3\"] #'-'.join(score_date_dash.split('-')[:2])]  # score_date_dash = '2022-08-31'\n",
    "\n",
    "#set up df_train and df_test (add 'target')\n",
    "df_target_train['ban'] = df_target_train['ban'].astype('int64')\n",
    "df_target_train = df_target_train.groupby('ban').tail(1)\n",
    "\n",
    "df_train = df_train.merge(df_target_train[['ban', 'product_crosssell_ind']], on='ban', how='left')\n",
    "df_train.rename(columns={'product_crosssell_ind': 'target'}, inplace=True)\n",
    "df_train.dropna(subset=['target'], inplace=True)\n",
    "df_train['target'] = df_train['target'].astype(int)\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32473d2-a75d-48ff-afc1-f34af20e2cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b9b389-df11-402a-ab09-20bd55af3d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "    token = os.popen('gcloud auth print-access-token').read()\n",
    "    token = re.sub(f'\\n$', '', token)\n",
    "    credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    if use_local_credential:\n",
    "        bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "    return bq_client\n",
    "\n",
    "client = get_gcp_bqclient(project_id)\n",
    "sql_test = ''' SELECT * FROM `{}.{}.bq_tos_cross_sell_targets_q4` '''.format(project_id, dataset_id)\n",
    "df_target_test = client.query(sql_test).to_dataframe()\n",
    "df_target_test = df_target_test.loc[\n",
    "    df_target_test['YEAR_MONTH'] == \"2022-Q4\"] #'-'.join(score_date_val_dash.split('-')[:2])]  # score_date_dash = '2022-09-30'\n",
    "\n",
    "#set up df_train and df_test (add 'target')\n",
    "df_target_test['ban'] = df_target_test['ban'].astype('int64')\n",
    "df_target_test = df_target_test.groupby('ban').tail(1)\n",
    "\n",
    "df_test = df_test.merge(df_target_test[['ban', 'product_crosssell_ind']], on='ban', how='left')\n",
    "df_test.rename(columns={'product_crosssell_ind': 'target'}, inplace=True)\n",
    "df_test.dropna(subset=['target'], inplace=True)\n",
    "df_test['target'] = df_test['target'].astype(int)\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62170f7-10b3-47f6-9610-7d758e68b6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_test['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426e602c-d3dd-4748-9617-410bc68b193e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m116",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m116"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
