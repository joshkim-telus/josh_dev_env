{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c6dc6e5-5e6d-47d1-92b4-145c5fcf5a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install matplotlib\n",
    "# !pip install retrying\n",
    "# !pip install scikit-learn\n",
    "# !pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a02b5d6a-8a6a-432c-aa9f-01c46c9f9b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import global modules\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from yaml import safe_load\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.metrics import Recall\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "\n",
    "# Set global vars\n",
    "pth_project = Path(os.getcwd().split('notebooks')[0])\n",
    "pth_data = pth_project / 'data'\n",
    "pth_utils = pth_project / 'utils'\n",
    "pth_queries = pth_project / 'queries'\n",
    "pth_creds = pth_project / 'conf' / 'local' / 'project_config.yaml'\n",
    "pth_recommenders = pth_data / 'recommenders'\n",
    "sys.path.insert(0, pth_project.as_posix())\n",
    "d_config = safe_load(pth_creds.open())\n",
    "\n",
    "# import local modules\n",
    "from utils.gcp import connect_bq_services, connect_pandas_bq_services\n",
    "from utils.extract import extract_bq_data\n",
    "from utils.modeling import process_features, extract_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bc69db0-7ad3-48a0-90ac-4f803d34d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_client = connect_bq_services(d_config['gcp-project-name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c1f062a-260b-4add-b76f-af134a9e9bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b6ad1-e8fb-4a16-86c7-ea8f573cfa44",
   "metadata": {},
   "source": [
    "#### Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5f54d5d-7114-4173-afb2-fba807c0c37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(462524, 274)\n",
      "(54231, 274)\n",
      "(54231, 274)\n"
     ]
    }
   ],
   "source": [
    "# extract trainning data\n",
    "sql = f\"\"\"\n",
    "  select *\n",
    "    from `divg-groovyhoon-pr-d2eab4.nba_product_reco_model.nba_training_dataset_v9`\n",
    "\"\"\"\n",
    "df_train = extract_bq_data(bq_client, sql)\n",
    "print(df_train.shape)\n",
    "\n",
    "# extract validation data\n",
    "sql = f\"\"\"\n",
    "  select *\n",
    "    from `divg-groovyhoon-pr-d2eab4.nba_product_reco_model.nba_val_dataset_v9`\n",
    "\"\"\"\n",
    "df_validation = extract_bq_data(bq_client, sql)\n",
    "print(df_validation.shape)\n",
    "\n",
    "# extract test data\n",
    "sql = f\"\"\"\n",
    "  select *\n",
    "    from `divg-groovyhoon-pr-d2eab4.nba_product_reco_model.nba_test_dataset_v9`\n",
    "\"\"\"\n",
    "df_test = extract_bq_data(bq_client, sql)\n",
    "print(df_validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe0f0f-daef-481a-b7e5-d3cb71b87d1d",
   "metadata": {},
   "source": [
    "#### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67c60ae5-a241-4401-ae9a-88a8a5f259ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_target_mapping = {\n",
    " 'sing_acquisition': 0,\n",
    " 'shs_acquisition': 1,\n",
    " 'tos_acquisition': 2,\n",
    " 'wifi_acquisition': 3,\n",
    " 'ttv_acquisition': 4,\n",
    " 'sws_acquisition': 5,\n",
    " 'hsic_acquisition': 6,\n",
    " 'lwc_acquisition': 7,\n",
    " 'hpro_acquisition': 8,\n",
    " 'whsia_acquisition': 9\n",
    "}\n",
    "\n",
    "# load features metadata\n",
    "d_features_metadata = safe_load((pth_utils / 'parameters' / 'acquisition_features_v7.yaml').open())\n",
    "\n",
    "# process training data\n",
    "df_train_processed = process_features(df_train, d_features_metadata, 'model_scenario', d_target_mapping)\n",
    "df_validation_processed = process_features(df_validation, d_features_metadata, 'model_scenario', d_target_mapping)\n",
    "df_test_processed = process_features(df_test, d_features_metadata, 'model_scenario', d_target_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1971f546-1957-40df-8b07-c538fef00d20",
   "metadata": {},
   "source": [
    "### Prepare data for CNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "503faf53-e054-4bab-8f44-d941d4c7f88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) attach ids to df_processed\n",
    "# 2) transform df_processed \n",
    "# 3) reattach features by joining on id cols\n",
    "\n",
    "# attach id cols to df_train_processed\n",
    "df_train_processed_ids  = pd.merge(df_train[['split_type', 'model_scenario',  'ref_dt', 'cust_id', 'ban', 'lpds_id', 'label', 'label_dt']], df_train_processed, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "# # attach id cols to df_validation_processed\n",
    "# df_validation_processed_ids \n",
    "df_validation_processed_ids = pd.merge(df_validation[['split_type', 'model_scenario',  'ref_dt', 'cust_id', 'ban', 'lpds_id', 'label', 'label_dt']], df_validation_processed, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "# # attach id cols to df_test_processed\n",
    "df_test_processed_ids = pd.merge(df_test[['split_type', 'model_scenario',  'ref_dt', 'cust_id', 'ban', 'lpds_id', 'label', 'label_dt']], df_test_processed, left_index=True, right_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe74659-9df8-4342-8ac8-d432f7d2abd3",
   "metadata": {},
   "source": [
    "### Transform train, val, test sets so that each row is unique for one cust_id+ban+lpds_id AND target labels are one-hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b5618991-0993-4088-96e7-e9f41f0ac85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(353126, 99)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_processed_unique = df_train_processed_ids.drop(['split_type', 'model_scenario', 'ref_dt', 'label', 'label_dt', 'target'], axis=1)\n",
    "df_validation_processed_unique = df_validation_processed_ids.drop(['split_type', 'model_scenario', 'ref_dt', 'label', 'label_dt', 'target'], axis=1)\n",
    "df_test_processed_unique = df_test_processed_ids.drop(['split_type', 'model_scenario', 'ref_dt', 'label', 'label_dt', 'target'], axis=1)\n",
    "\n",
    "df_train_processed_unique = df_train_processed_ids.drop_duplicates(subset=['cust_id', 'ban', 'lpds_id'])\n",
    "df_validation_processed_unique = df_validation_processed_ids.drop_duplicates(subset=['cust_id', 'ban', 'lpds_id'])\n",
    "df_test_processed_unique = df_test_processed_ids.drop_duplicates(subset=['cust_id', 'ban', 'lpds_id'])\n",
    "\n",
    "df_train_processed_unique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a5b6ce69-d231-401e-91d7-eab5378eb989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(351430, 110)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_grouped = df_train_processed_ids.groupby(['lpds_id', 'ban'])['model_scenario'].\\\n",
    "    apply(lambda x: ','.join(map(str, x))).reset_index()\n",
    "\n",
    "for label in d_target_mapping.keys():\n",
    "    df_train_grouped[label] = df_train_grouped.apply(\n",
    "        lambda row: 1 if label in row['model_scenario'] else 0, axis = 1\n",
    "    )\n",
    "\n",
    "df_train_grouped_final = pd.merge(df_train_grouped, df_train_processed_unique, on=['ban', 'lpds_id'], how='left')\n",
    "\n",
    "df_train_grouped_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "11dc8227-4279-4e6d-8cc1-f7f06b924b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46960, 110)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation_grouped = df_validation_processed_ids.groupby(['lpds_id', 'ban'])['model_scenario'].\\\n",
    "    apply(lambda x: ','.join(map(str, x))).reset_index()\n",
    "\n",
    "for label in d_target_mapping.keys():\n",
    "    df_validation_grouped[label] = df_validation_grouped.apply(\n",
    "        lambda row: 1 if label in row['model_scenario'] else 0, axis = 1\n",
    "    )\n",
    "\n",
    "df_validation_grouped_final = pd.merge(df_validation_grouped, df_validation_processed_unique, on=['ban', 'lpds_id'], how='left')\n",
    "\n",
    "df_validation_grouped_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f38565ee-7142-4566-af97-6647d3a05151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47930, 110)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_grouped = df_test_processed_ids.groupby(['lpds_id', 'ban'])['model_scenario'].\\\n",
    "    apply(lambda x: ','.join(map(str, x))).reset_index()\n",
    "\n",
    "for label in d_target_mapping.keys():\n",
    "    df_test_grouped[label] = df_test_grouped.apply(\n",
    "        lambda row: 1 if label in row['model_scenario'] else 0, axis = 1\n",
    "    )\n",
    "\n",
    "df_test_grouped_final = pd.merge(df_test_grouped, df_test_processed_unique, on=['ban', 'lpds_id'], how='left')\n",
    "\n",
    "df_test_grouped_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "289bb24a-96d0-43f2-b00b-96e1857d1961",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_grouped_final.drop(['model_scenario_x', 'model_scenario_y', 'split_type', 'ref_dt', 'label', 'label_dt', 'target'], axis=1, inplace=True)\n",
    "df_validation_grouped_final.drop(['model_scenario_x', 'model_scenario_y', 'split_type', 'ref_dt', 'label', 'label_dt', 'target'], axis=1, inplace=True)\n",
    "df_test_grouped_final.drop(['model_scenario_x', 'model_scenario_y', 'split_type', 'ref_dt', 'label', 'label_dt', 'target'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "216acca2-f237-49b0-b5a0-c2e974a7731b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sing_acquisition',\n",
       " 'shs_acquisition',\n",
       " 'tos_acquisition',\n",
       " 'wifi_acquisition',\n",
       " 'ttv_acquisition',\n",
       " 'sws_acquisition',\n",
       " 'hsic_acquisition',\n",
       " 'lwc_acquisition',\n",
       " 'hpro_acquisition',\n",
       " 'whsia_acquisition',\n",
       " 'cust_id',\n",
       " 'ban',\n",
       " 'lpds_id']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_label_cols = [col for col in df_train_grouped_final.columns if '_acquisition' in col] \n",
    "l_id_cols = ['cust_id', 'ban', 'lpds_id'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "395c7fe1-43c7-4c2b-bce3-befb8793e47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sing_acquisition</th>\n",
       "      <th>shs_acquisition</th>\n",
       "      <th>tos_acquisition</th>\n",
       "      <th>wifi_acquisition</th>\n",
       "      <th>ttv_acquisition</th>\n",
       "      <th>sws_acquisition</th>\n",
       "      <th>hsic_acquisition</th>\n",
       "      <th>lwc_acquisition</th>\n",
       "      <th>hpro_acquisition</th>\n",
       "      <th>whsia_acquisition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sing_acquisition  shs_acquisition  tos_acquisition  wifi_acquisition  \\\n",
       "0                 0                0                0                 0   \n",
       "1                 0                0                0                 0   \n",
       "2                 0                0                0                 0   \n",
       "3                 0                0                0                 0   \n",
       "4                 1                0                0                 0   \n",
       "\n",
       "   ttv_acquisition  sws_acquisition  hsic_acquisition  lwc_acquisition  \\\n",
       "0                1                0                 0                0   \n",
       "1                0                0                 0                0   \n",
       "2                0                0                 0                0   \n",
       "3                0                0                 0                0   \n",
       "4                0                0                 0                0   \n",
       "\n",
       "   hpro_acquisition  whsia_acquisition  \n",
       "0                 0                  0  \n",
       "1                 0                  1  \n",
       "2                 0                  1  \n",
       "3                 0                  1  \n",
       "4                 0                  0  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df_train_grouped_final.drop(columns=l_label_cols + l_id_cols) \n",
    "y_train = df_train_grouped_final[l_label_cols] \n",
    "\n",
    "X_val = df_validation_grouped_final.drop(columns=l_label_cols + l_id_cols) \n",
    "y_val = df_validation_grouped_final[l_label_cols] \n",
    "\n",
    "X_test = df_test_grouped_final.drop(columns=l_label_cols + l_id_cols) \n",
    "y_test = df_test_grouped_final[l_label_cols] \n",
    "\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "148c2016-ea0c-4ccb-854c-ee3b67b3ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate normalization parameters on the training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Apply normalization to the training data\n",
    "X_train_normalized = scaler.transform(X_train)\n",
    "\n",
    "# Apply normalization to the validation data using the same parameters\n",
    "X_val_normalized = scaler.transform(X_val)\n",
    "X_test_normalized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9b4c78f8-8a66-46ba-a5ac-19acd5a341e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply random oversampling to balance the dataset\n",
    "# sampling = RandomOverSampler(random_state=42, sampling_strategy=sampling_strategy)\n",
    "# sampling = SMOTE(random_state=42)\n",
    "\n",
    "# X_train_resampled, y_train_resampled = sampling.fit_resample(X_train_normalized.astype('float'), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8c527d-366a-4fb3-930d-c4b12ea28f2d",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d7e876c5-df41-48b3-9351-e182cc78eb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the input data to include the sequence length dimension\n",
    "X_train_reshaped = np.reshape(X_train_normalized, (X_train_normalized.shape[0], X_train_normalized.shape[1], 1))\n",
    "\n",
    "# Define the input shape based on the reshaped data\n",
    "input_shape = X_train_reshaped.shape[1:]  # Shape excluding batch size\n",
    "\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "b09a130f-4b3a-4ced-9e46-181590c5d8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sing_acquisition</th>\n",
       "      <th>shs_acquisition</th>\n",
       "      <th>tos_acquisition</th>\n",
       "      <th>wifi_acquisition</th>\n",
       "      <th>ttv_acquisition</th>\n",
       "      <th>sws_acquisition</th>\n",
       "      <th>hsic_acquisition</th>\n",
       "      <th>lwc_acquisition</th>\n",
       "      <th>hpro_acquisition</th>\n",
       "      <th>whsia_acquisition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46955</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46956</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46957</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46958</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46959</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46960 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sing_acquisition  shs_acquisition  tos_acquisition  wifi_acquisition  \\\n",
       "0                     0                1                0                 0   \n",
       "1                     1                0                0                 0   \n",
       "2                     0                0                0                 0   \n",
       "3                     0                1                0                 0   \n",
       "4                     0                1                0                 0   \n",
       "...                 ...              ...              ...               ...   \n",
       "46955                 0                0                1                 0   \n",
       "46956                 0                0                1                 0   \n",
       "46957                 0                0                1                 0   \n",
       "46958                 0                0                1                 0   \n",
       "46959                 0                0                1                 0   \n",
       "\n",
       "       ttv_acquisition  sws_acquisition  hsic_acquisition  lwc_acquisition  \\\n",
       "0                    0                0                 0                0   \n",
       "1                    0                0                 0                0   \n",
       "2                    0                0                 0                0   \n",
       "3                    0                0                 0                0   \n",
       "4                    0                0                 0                0   \n",
       "...                ...              ...               ...              ...   \n",
       "46955                0                0                 0                0   \n",
       "46956                0                0                 0                0   \n",
       "46957                0                0                 0                0   \n",
       "46958                0                0                 0                0   \n",
       "46959                0                0                 0                0   \n",
       "\n",
       "       hpro_acquisition  whsia_acquisition  \n",
       "0                     0                  0  \n",
       "1                     0                  0  \n",
       "2                     0                  1  \n",
       "3                     0                  0  \n",
       "4                     0                  0  \n",
       "...                 ...                ...  \n",
       "46955                 0                  0  \n",
       "46956                 0                  0  \n",
       "46957                 0                  0  \n",
       "46958                 0                  0  \n",
       "46959                 0                  0  \n",
       "\n",
       "[46960 rows x 10 columns]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4339cd-0489-445c-8ae6-8bb8f86a3c16",
   "metadata": {},
   "source": [
    "### CNN: Current Capture Rate of ~78%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "b5327af4-818a-4697-9018-1cbe714cfd78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_33 (Conv1D)          (None, 88, 32)            128       \n",
      "                                                                 \n",
      " global_max_pooling1d_9 (Gl  (None, 32)                0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 458 (1.79 KB)\n",
      "Trainable params: 458 (1.79 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "550/550 [==============================] - 6s 9ms/step - loss: 2.6266 - top_k_categorical_accuracy: 0.6394 - val_loss: 2.5569 - val_top_k_categorical_accuracy: 0.7070\n",
      "Epoch 2/20\n",
      "550/550 [==============================] - 5s 9ms/step - loss: 2.7411 - top_k_categorical_accuracy: 0.6398 - val_loss: 2.8852 - val_top_k_categorical_accuracy: 0.7106\n",
      "Epoch 3/20\n",
      "550/550 [==============================] - 5s 10ms/step - loss: 2.9576 - top_k_categorical_accuracy: 0.6374 - val_loss: 2.9770 - val_top_k_categorical_accuracy: 0.7214\n",
      "Epoch 4/20\n",
      "550/550 [==============================] - 5s 9ms/step - loss: 3.0952 - top_k_categorical_accuracy: 0.6357 - val_loss: 3.1216 - val_top_k_categorical_accuracy: 0.7397\n",
      "Epoch 5/20\n",
      "550/550 [==============================] - 5s 9ms/step - loss: 3.2382 - top_k_categorical_accuracy: 0.6306 - val_loss: 3.2656 - val_top_k_categorical_accuracy: 0.7494\n",
      "Epoch 6/20\n",
      "550/550 [==============================] - 5s 9ms/step - loss: 3.3779 - top_k_categorical_accuracy: 0.6301 - val_loss: 3.4403 - val_top_k_categorical_accuracy: 0.7534\n",
      "Epoch 7/20\n",
      "550/550 [==============================] - 5s 9ms/step - loss: 3.5361 - top_k_categorical_accuracy: 0.6349 - val_loss: 3.6929 - val_top_k_categorical_accuracy: 0.7364\n",
      "Epoch 8/20\n",
      "550/550 [==============================] - 5s 9ms/step - loss: 3.7119 - top_k_categorical_accuracy: 0.6388 - val_loss: 4.0052 - val_top_k_categorical_accuracy: 0.7502\n",
      "Epoch 9/20\n",
      "550/550 [==============================] - 5s 9ms/step - loss: 3.8917 - top_k_categorical_accuracy: 0.6443 - val_loss: 4.1467 - val_top_k_categorical_accuracy: 0.7495\n",
      "Epoch 10/20\n",
      "550/550 [==============================] - 5s 9ms/step - loss: 4.0812 - top_k_categorical_accuracy: 0.6466 - val_loss: 4.3387 - val_top_k_categorical_accuracy: 0.7521\n",
      "Epoch 11/20\n",
      "550/550 [==============================] - 5s 9ms/step - loss: 4.3384 - top_k_categorical_accuracy: 0.6470 - val_loss: 4.6635 - val_top_k_categorical_accuracy: 0.7385\n",
      "Epoch 12/20\n",
      "550/550 [==============================] - 5s 9ms/step - loss: 4.5673 - top_k_categorical_accuracy: 0.6503 - val_loss: 5.0185 - val_top_k_categorical_accuracy: 0.7640\n",
      "Epoch 13/20\n",
      "550/550 [==============================] - 5s 9ms/step - loss: 4.7886 - top_k_categorical_accuracy: 0.6529 - val_loss: 5.3424 - val_top_k_categorical_accuracy: 0.7139\n",
      "Epoch 14/20\n",
      "550/550 [==============================] - 5s 9ms/step - loss: 5.0678 - top_k_categorical_accuracy: 0.6558 - val_loss: 5.6603 - val_top_k_categorical_accuracy: 0.7254\n",
      "Epoch 15/20\n",
      "550/550 [==============================] - 5s 9ms/step - loss: 5.3289 - top_k_categorical_accuracy: 0.6559 - val_loss: 6.0145 - val_top_k_categorical_accuracy: 0.7611\n",
      "Epoch 16/20\n",
      "550/550 [==============================] - 5s 9ms/step - loss: 5.5969 - top_k_categorical_accuracy: 0.6589 - val_loss: 6.3666 - val_top_k_categorical_accuracy: 0.7639\n",
      "Epoch 17/20\n",
      "550/550 [==============================] - 5s 9ms/step - loss: 5.9425 - top_k_categorical_accuracy: 0.6602 - val_loss: 6.7706 - val_top_k_categorical_accuracy: 0.7693\n",
      "Epoch 18/20\n",
      "550/550 [==============================] - 5s 9ms/step - loss: 6.2636 - top_k_categorical_accuracy: 0.6600 - val_loss: 7.1398 - val_top_k_categorical_accuracy: 0.7616\n",
      "Epoch 19/20\n",
      "550/550 [==============================] - 5s 9ms/step - loss: 6.5747 - top_k_categorical_accuracy: 0.6628 - val_loss: 7.8608 - val_top_k_categorical_accuracy: 0.6852\n",
      "Epoch 20/20\n",
      "550/550 [==============================] - 5s 9ms/step - loss: 6.9264 - top_k_categorical_accuracy: 0.6627 - val_loss: 8.1029 - val_top_k_categorical_accuracy: 0.6819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f2bb1dde560>"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Conv2D, Flatten\n",
    "\n",
    "# Reshape the input data to include the sequence length dimension\n",
    "X_train_reshaped = np.reshape(X_train_normalized, (X_train_normalized.shape[0], X_train_normalized.shape[1], 1))\n",
    "\n",
    "# Define the input shape based on the reshaped data\n",
    "input_shape = X_train_reshaped.shape[1:]  # Shape excluding batch size\n",
    "\n",
    "# Define your CNN model\n",
    "cnn_model = models.Sequential()\n",
    "\n",
    "# # Create a 1D CNN model\n",
    "cnn_model.add(layers.Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "# cnn_model.add(layers.Conv1D(64, kernel_size=3, activation='relu'))\n",
    "# # cnn_model.add(layers.Conv1D(128, kernel_size=3, activation='relu'))\n",
    "\n",
    "# Add global max pooling layer\n",
    "cnn_model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "# # Add dense layers\n",
    "# cnn_model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "cnn_model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.TopKCategoricalAccuracy(k=3)])\n",
    "\n",
    "# Print the model summary\n",
    "cnn_model.summary()\n",
    "\n",
    "# Train the model\n",
    "cnn_model.fit(X_train_reshaped, y_train, epochs=20, batch_size=512, validation_split = 0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3746ac26-136c-45e1-ad4b-c7dd83cc24b6",
   "metadata": {},
   "source": [
    "### More Experiments with CNN 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "151416f3-6c13-4386-a4c1-815132058156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models\n",
    "# from tensorflow.keras.layers import Conv2D, Flatten\n",
    "\n",
    "# # Reshape the input data to include the sequence length dimension\n",
    "# X_train_reshaped = np.reshape(X_train_normalized, (X_train_normalized.shape[0], X_train_normalized.shape[1], 1))\n",
    "\n",
    "# # Define the input shape based on the reshaped data\n",
    "# input_shape = X_train_reshaped.shape[1:]  # Shape excluding batch size\n",
    "\n",
    "# # Define your CNN model\n",
    "# cnn_model = models.Sequential()\n",
    "\n",
    "# # Create a 1D CNN model\n",
    "# cnn_model.add(layers.Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "# # cnn_model.add(layers.Conv1D(64, kernel_size=3, activation='relu'))\n",
    "# # cnn_model.add(layers.Conv1D(128, kernel_size=3, activation='relu'))\n",
    "\n",
    "# # # Add global max pooling layer\n",
    "# # cnn_model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "# # # Add dense layers\n",
    "# # cnn_model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "# cnn_model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "# # Compile the model\n",
    "# cnn_model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "#               metrics=[tf.keras.metrics.TopKCategoricalAccuracy(k=3)])\n",
    "\n",
    "# # Print the model summary\n",
    "# cnn_model.summary()\n",
    "\n",
    "# # Train the model\n",
    "# cnn_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=512, validation_split = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "6c0e3b46-2bd7-4964-941f-750c2b3124ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_values(arr, i):\n",
    "    \"\"\"\n",
    "    Convert values in a NumPy array.\n",
    "    \n",
    "    Args:\n",
    "    - arr: NumPy array\n",
    "    \n",
    "    Returns:\n",
    "    - Converted NumPy array\n",
    "    \"\"\"\n",
    "    # Make a copy of the input array to avoid modifying the original array\n",
    "    converted_arr = arr.copy()\n",
    "    \n",
    "    # Use boolean indexing to identify elements with value 1\n",
    "    indices = converted_arr == 1\n",
    "    \n",
    "    # Replace elements with value 1 with 2\n",
    "    converted_arr[indices] = i\n",
    "    \n",
    "    return converted_arr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "3e68cb17-c0fc-4007-b1ae-13194eef5d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_conv = y_val.copy()\n",
    "y_val_conv['sing_acquisition'] = convert_values(np.array(y_val['sing_acquisition']), 1)\n",
    "y_val_conv['shs_acquisition'] = convert_values(np.array(y_val['shs_acquisition']), 2)\n",
    "y_val_conv['tos_acquisition'] = convert_values(np.array(y_val['tos_acquisition']), 3)\n",
    "y_val_conv['wifi_acquisition'] = convert_values(np.array(y_val['wifi_acquisition']), 4)\n",
    "y_val_conv['ttv_acquisition'] = convert_values(np.array(y_val['ttv_acquisition']), 5)\n",
    "y_val_conv['sws_acquisition'] = convert_values(np.array(y_val['sws_acquisition']), 6)\n",
    "y_val_conv['hsic_acquisition'] = convert_values(np.array(y_val['hsic_acquisition']), 7)\n",
    "y_val_conv['lwc_acquisition'] = convert_values(np.array(y_val['lwc_acquisition']), 8)\n",
    "y_val_conv['hpro_acquisition'] = convert_values(np.array(y_val['hpro_acquisition']), 9)\n",
    "y_val_conv['whsia_acquisition'] = convert_values(np.array(y_val['whsia_acquisition']), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "2ef86064-c609-4f0b-9da7-8eb647dd6e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_conv = y_test.copy()\n",
    "y_test_conv['sing_acquisition'] = convert_values(np.array(y_test['sing_acquisition']), 1)\n",
    "y_test_conv['shs_acquisition'] = convert_values(np.array(y_test['shs_acquisition']), 2)\n",
    "y_test_conv['tos_acquisition'] = convert_values(np.array(y_test['tos_acquisition']), 3)\n",
    "y_test_conv['wifi_acquisition'] = convert_values(np.array(y_test['wifi_acquisition']), 4)\n",
    "y_test_conv['ttv_acquisition'] = convert_values(np.array(y_test['ttv_acquisition']), 5)\n",
    "y_test_conv['sws_acquisition'] = convert_values(np.array(y_test['sws_acquisition']), 6)\n",
    "y_test_conv['hsic_acquisition'] = convert_values(np.array(y_test['hsic_acquisition']), 7)\n",
    "y_test_conv['lwc_acquisition'] = convert_values(np.array(y_test['lwc_acquisition']), 8)\n",
    "y_test_conv['hpro_acquisition'] = convert_values(np.array(y_test['hpro_acquisition']), 9)\n",
    "y_test_conv['whsia_acquisition'] = convert_values(np.array(y_test['whsia_acquisition']), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "61b9e54d-811a-4727-acc8-d95bea707c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1468/1468 [==============================] - 3s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# y_pred = model.predict(X_val_normalized)\n",
    "y_pred = cnn_model.predict(X_val_normalized)\n",
    "\n",
    "y_pred_0 = y_pred[:, 0]\n",
    "y_pred_1 = y_pred[:, 1]\n",
    "y_pred_2 = y_pred[:, 2]\n",
    "y_pred_3 = y_pred[:, 3]\n",
    "y_pred_4 = y_pred[:, 4]\n",
    "y_pred_5 = y_pred[:, 5]\n",
    "y_pred_6 = y_pred[:, 6]\n",
    "y_pred_7 = y_pred[:, 7]\n",
    "y_pred_8 = y_pred[:, 8]\n",
    "y_pred_9 = y_pred[:, 9]\n",
    "\n",
    "df_X_val_normalized = pd.DataFrame(X_val_normalized, columns=[f'col_{i}' for i in range(X_val_normalized.shape[1])])\n",
    "\n",
    "df_val_exp = pd.merge(df_X_val_normalized, y_val_conv, left_index=True, right_index=True, how='inner')\n",
    "# df_val_exp['y_val'] = y_val\n",
    "df_val_exp['y_pred_proba_0'] = y_pred_0\n",
    "df_val_exp['y_pred_proba_1'] = y_pred_1\n",
    "df_val_exp['y_pred_proba_2'] = y_pred_2\n",
    "df_val_exp['y_pred_proba_3'] = y_pred_3\n",
    "df_val_exp['y_pred_proba_4'] = y_pred_4\n",
    "df_val_exp['y_pred_proba_5'] = y_pred_5\n",
    "df_val_exp['y_pred_proba_6'] = y_pred_6\n",
    "df_val_exp['y_pred_proba_7'] = y_pred_7\n",
    "df_val_exp['y_pred_proba_8'] = y_pred_8\n",
    "df_val_exp['y_pred_proba_9'] = y_pred_9\n",
    "\n",
    "df_val_exp.to_csv(\"gs://divg-groovyhoon-pr-d2eab4-default/downloads/df_val_exp_dl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "03117630-55b3-44b9-b187-7a33f05f63b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1498/1498 [==============================] - 2s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# y_pred = model.predict(X_val_normalized)\n",
    "y_pred = cnn_model.predict(X_test_normalized)\n",
    "\n",
    "y_pred_0 = y_pred[:, 0]\n",
    "y_pred_1 = y_pred[:, 1]\n",
    "y_pred_2 = y_pred[:, 2]\n",
    "y_pred_3 = y_pred[:, 3]\n",
    "y_pred_4 = y_pred[:, 4]\n",
    "y_pred_5 = y_pred[:, 5]\n",
    "y_pred_6 = y_pred[:, 6]\n",
    "y_pred_7 = y_pred[:, 7]\n",
    "y_pred_8 = y_pred[:, 8]\n",
    "y_pred_9 = y_pred[:, 9]\n",
    "\n",
    "df_X_test_normalized = pd.DataFrame(X_test_normalized, columns=[f'col_{i}' for i in range(X_test_normalized.shape[1])])\n",
    "\n",
    "df_test_exp = pd.merge(df_X_test_normalized, y_test_conv, left_index=True, right_index=True, how='inner')\n",
    "# df_val_exp['y_val'] = y_val\n",
    "df_test_exp['y_pred_proba_0'] = y_pred_0\n",
    "df_test_exp['y_pred_proba_1'] = y_pred_1\n",
    "df_test_exp['y_pred_proba_2'] = y_pred_2\n",
    "df_test_exp['y_pred_proba_3'] = y_pred_3\n",
    "df_test_exp['y_pred_proba_4'] = y_pred_4\n",
    "df_test_exp['y_pred_proba_5'] = y_pred_5\n",
    "df_test_exp['y_pred_proba_6'] = y_pred_6\n",
    "df_test_exp['y_pred_proba_7'] = y_pred_7\n",
    "df_test_exp['y_pred_proba_8'] = y_pred_8\n",
    "df_test_exp['y_pred_proba_9'] = y_pred_9\n",
    "\n",
    "df_val_exp.to_csv(\"gs://divg-groovyhoon-pr-d2eab4-default/downloads/df_test_exp_dl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "4ad9e078-2ced-4ef2-a97c-5562c769a07e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[273], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43merror\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'error' is not defined"
     ]
    }
   ],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d5694086-cf64-4aa2-be5a-5ddf4747fa88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1468/1468 [==============================] - 2s 1ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (10) does not match length of index (46960)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m  model\u001b[38;5;241m.\u001b[39mpredict(X_val_normalized)\n\u001b[1;32m      3\u001b[0m results_ranked \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(\u001b[38;5;241m-\u001b[39mprobabilities, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m display(\u001b[43mextract_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults_ranked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_target_mapping\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/home/jupyter/josh_dev_env/nba_product_reco/utils/modeling.py:143\u001b[0m, in \u001b[0;36mextract_stats\u001b[0;34m(n, predictions_ranked, true_values, d_target_mapping)\u001b[0m\n\u001b[1;32m    141\u001b[0m df_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(true_values)\n\u001b[1;32m    142\u001b[0m df_results \u001b[38;5;241m=\u001b[39m df_results\u001b[38;5;241m.\u001b[39mrename(columns \u001b[38;5;241m=\u001b[39m {df_results\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m0\u001b[39m]: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m--> 143\u001b[0m \u001b[43mdf_results\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mis_prediction_in_top_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mn\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m l_results\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# aggregate by label\u001b[39;00m\n\u001b[1;32m    146\u001b[0m df_stats \u001b[38;5;241m=\u001b[39m df_results\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39magg({\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_prediction_in_top_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m     }\n\u001b[1;32m    153\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:4299\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4296\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   4297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4298\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 4299\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:4512\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4502\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4503\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4504\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4505\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4510\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4511\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4512\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4515\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4516\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4517\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m   4518\u001b[0m     ):\n\u001b[1;32m   4519\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4520\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:5253\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   5252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 5253\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5254\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   5255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5256\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[1;32m   5257\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5260\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[1;32m   5261\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/common.py:571\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    572\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (10) does not match length of index (46960)"
     ]
    }
   ],
   "source": [
    "for n in (1, 2, 3):\n",
    "    probabilities =  model.predict(X_val_normalized)\n",
    "    results_ranked = np.argsort(-probabilities, axis=1)\n",
    "    display(extract_stats(n, results_ranked, y_val, d_target_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "82a698a0-d342-49af-9e96-8f52e5a1625b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1468/1468 [==============================] - 2s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_val_normalized)\n",
    "\n",
    "y_pred_0 = y_pred[:, 0]\n",
    "y_pred_1 = y_pred[:, 1]\n",
    "y_pred_2 = y_pred[:, 2]\n",
    "y_pred_3 = y_pred[:, 3]\n",
    "y_pred_4 = y_pred[:, 4]\n",
    "y_pred_5 = y_pred[:, 5]\n",
    "y_pred_6 = y_pred[:, 6]\n",
    "y_pred_7 = y_pred[:, 7]\n",
    "y_pred_8 = y_pred[:, 8]\n",
    "y_pred_9 = y_pred[:, 9]\n",
    "\n",
    "df_X_val_normalized = pd.DataFrame(X_val_normalized, columns=[f'col_{i}' for i in range(X_val_normalized.shape[1])])\n",
    "\n",
    "df_val_exp = pd.merge(df_X_val_normalized, y_val, left_index=True, right_index=True, how='inner')\n",
    "# df_val_exp['y_val'] = y_val\n",
    "df_val_exp['y_pred_proba_0'] = y_pred_0\n",
    "df_val_exp['y_pred_proba_1'] = y_pred_1\n",
    "df_val_exp['y_pred_proba_2'] = y_pred_2\n",
    "df_val_exp['y_pred_proba_3'] = y_pred_3\n",
    "df_val_exp['y_pred_proba_4'] = y_pred_4\n",
    "df_val_exp['y_pred_proba_5'] = y_pred_5\n",
    "df_val_exp['y_pred_proba_6'] = y_pred_6\n",
    "df_val_exp['y_pred_proba_7'] = y_pred_7\n",
    "df_val_exp['y_pred_proba_8'] = y_pred_8\n",
    "df_val_exp['y_pred_proba_9'] = y_pred_9\n",
    "\n",
    "df_val_exp.to_csv(\"gs://divg-groovyhoon-pr-d2eab4-default/downloads/df_val_exp_dl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a88803a-ca40-4470-bced-d4b9cd6ca12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_normalized)\n",
    "\n",
    "y_pred_0 = y_pred[:, 0]\n",
    "y_pred_1 = y_pred[:, 1]\n",
    "y_pred_2 = y_pred[:, 2]\n",
    "y_pred_3 = y_pred[:, 3]\n",
    "y_pred_4 = y_pred[:, 4]\n",
    "y_pred_5 = y_pred[:, 5]\n",
    "y_pred_6 = y_pred[:, 6]\n",
    "y_pred_7 = y_pred[:, 7]\n",
    "y_pred_8 = y_pred[:, 8]\n",
    "y_pred_9 = y_pred[:, 9]\n",
    "\n",
    "df_val_exp = pd.merge(X_test_normalized, y_test, left_index=True, right_index=True, how='inner')\n",
    "# df_val_exp['y_test'] = y_test\n",
    "df_val_exp['y_pred_proba_0'] = y_pred_0\n",
    "df_val_exp['y_pred_proba_1'] = y_pred_1\n",
    "df_val_exp['y_pred_proba_2'] = y_pred_2\n",
    "df_val_exp['y_pred_proba_3'] = y_pred_3\n",
    "df_val_exp['y_pred_proba_4'] = y_pred_4\n",
    "df_val_exp['y_pred_proba_5'] = y_pred_5\n",
    "df_val_exp['y_pred_proba_6'] = y_pred_6\n",
    "df_val_exp['y_pred_proba_7'] = y_pred_7\n",
    "df_val_exp['y_pred_proba_8'] = y_pred_8\n",
    "df_val_exp['y_pred_proba_9'] = y_pred_9\n",
    "\n",
    "df_val_exp.to_csv(\"gs://divg-groovyhoon-pr-d2eab4-default/downloads/y_test_exp_dl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da443cf4-4ed6-4eca-8db1-9d62f6414b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bb57ea-a4f5-4d6f-99e0-ecc918e19c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2ee210-34a3-4b11-afdd-7339b197de52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8676c-7e42-40d3-af8d-5a3c0bf1a16f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d198335c-ce17-46d6-899b-85ed22dac93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n= 3\n",
    "probabilities =  model.predict(X_val_normalized)\n",
    "results_ranked = np.argsort(-probabilities, axis=1)\n",
    "display(extract_stats(n, results_ranked, y_val, d_target_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54c8ab4-6605-40f7-853b-bb4442b8c920",
   "metadata": {},
   "outputs": [],
   "source": [
    "n= 3\n",
    "probabilities =  model.predict(X_val_normalized)\n",
    "results_ranked = np.argsort(-probabilities, axis=1)\n",
    "display(extract_stats(n, results_ranked, y_val, d_target_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56317423-0616-4481-9d86-9c5abb325d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "n= 3\n",
    "probabilities =  model.predict(X_val_normalized)\n",
    "results_ranked = np.argsort(-probabilities, axis=1)\n",
    "display(extract_stats(n, results_ranked, y_val, d_target_mapping))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59a00872-c96a-44cf-9f81-fd7146049ae7",
   "metadata": {},
   "source": [
    "n= 3\n",
    "probabilities =  model.predict(X_val_normalized)\n",
    "results_ranked = np.argsort(-probabilities, axis=1)\n",
    "display(extract_stats(n, results_ranked, y_val, d_target_mapping))\n",
    "\n",
    "#### Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7325edc-554a-4555-ad9e-f8bd8c15a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow scikeras scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea6f7ee-1902-4b28-96f6-612a2688a62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5b00dc-4013-4931-bab2-8a693bbfcc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_size, output_size, hidden_layer_dim, activation):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation=activation, input_dim=input_size))\n",
    "    model.add(Dense(hidden_layer_dim, activation=activation))\n",
    "    model.add(Dense(output_size, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126cad17-501a-4bdb-ad40-02c315cd7b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create KerasClassifier\n",
    "model = KerasClassifier(\n",
    "    create_model,\n",
    "    input_size=input_size,\n",
    "    output_size=output_size,\n",
    "    hidden_layer_dim=100,\n",
    "    activation='relu',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy(k=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f22fccf-3ca8-4400-933b-f34d0eb43a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import top_k_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181dd733-4c1f-444c-9585-22e7faf839a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'hidden_layer_dim': [50, 100, 200, 300, 400, 500],\n",
    "    'optimizer': ['adam', 'sgd'],\n",
    "    'optimizer__learning_rate': [0.0001, 0.0005, 0.001],\n",
    "    'activation': ['relu', 'sigmoid'],\n",
    "    'batch_size': [128, 256, 512, 1024, 2048,4096]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model, \n",
    "    param_grid=param_grid, \n",
    "    cv=5, \n",
    "    scoring=make_scorer(top_k_accuracy_score, k=3, response_method='predict_proba')\n",
    "    #scoring=make_scorer(top_k_accuracy_score, k=3, labels=list(d_target_mapping.values()))\n",
    ")\n",
    "grid_search.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8200e85-c50f-418c-86d2-f7c2ec4e599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_score_, grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf88b66-0939-4aa9-a5cf-320a0bf058cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best parameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6984641a-960f-4365-82de-066badecb39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n= 3\n",
    "probabilities =  best_model.predict_proba(X_val_normalized)\n",
    "results_ranked = np.argsort(-probabilities, axis=1)\n",
    "display(extract_stats(n, results_ranked, y_val, d_target_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf541313-16b2-49c3-9c82-adbc3875a708",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the best model\n",
    "best_model.fit(X_train_normalized, y_train, epochs=1000, validation_data=(X_test_normalized, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cf7806-c262-47b8-b29c-6fc137e35f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "n= 3\n",
    "probabilities =  best_model.predict_proba(X_val_normalized)\n",
    "results_ranked = np.argsort(-probabilities, axis=1)\n",
    "display(extract_stats(n, results_ranked, y_val, d_target_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d591c7a-917a-4b82-9348-1ffc7fe5d83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba5761b-16fa-460c-85af-bf60cbc2ff87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
