{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b0c603b-3f78-40d0-bc02-7dac3ea2a76c",
   "metadata": {},
   "source": [
    "### Import Libraries, declare variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf87789-5a0e-488a-a4bf-7969f104635e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "CREATE OR REPLACE TABLE `divg-groovyhoon-pr-d2eab4.nba_product_reco_model.nba_training_dataset` AS \n",
    "\n",
    "SELECT split_type\n",
    ", model_scenario\n",
    ", ref_dt\n",
    ", cust_id\n",
    ", cust_src_id\n",
    ", ban\n",
    ", ban_src_id\n",
    ", lpds_id\n",
    ", fms_address_id\n",
    ", label\n",
    ", label_dt\n",
    ", avg_usg_cnt_4w_news\n",
    ", avg_usg_dl_4w_news\n",
    ", avg_usg_ul_4w_news\n",
    ", avg_usg_cnt_4w_sports\n",
    ", avg_usg_dl_4w_sports\n",
    ", avg_usg_ul_4w_sports\n",
    ", `avg_usg_cnt_4w_tv and movies` AS avg_usg_cnt_4w_tv_and_movies\n",
    ", `avg_usg_dl_4w_tv and movies` AS avg_usg_dl_4w_tv_and_movies\n",
    ", `avg_usg_ul_4w_tv and movies` AS avg_usg_ul_4w_tv_and_movies\n",
    ", bill_wln_zscore_ban_subtotal_amt\n",
    ", CAST(bill_wln_avg_ban_subtotal_amt AS FLOAT64) AS bill_wln_avg_ban_subtotal_amt\n",
    ", bill_wln_zscore_ban_debit_amt\n",
    ", CAST(bill_wln_avg_ban_debit_amt AS FLOAT64) AS bill_wln_avg_ban_debit_amt\n",
    ", bill_wln_zscore_ban_discount_amt\n",
    ", CAST(bill_wln_avg_ban_discount_amt AS FLOAT64) AS bill_wln_avg_ban_discount_amt\n",
    ", bill_wln_zscore_ban_credit_amt\n",
    ", CAST(bill_wln_avg_ban_credit_amt AS FLOAT64) AS bill_wln_avg_ban_credit_amt\n",
    ", CAST(bill_wln_avg_sing_debit_amt AS FLOAT64) AS bill_wln_avg_sing_debit_amt\n",
    ", CAST(bill_wln_avg_sing_discount_amt AS FLOAT64) AS bill_wln_avg_sing_discount_amt\n",
    ", CAST(bill_wln_avg_sing_credit_amt AS FLOAT64) AS bill_wln_avg_sing_credit_amt\n",
    ", CAST(bill_wln_avg_hsic_debit_amt AS FLOAT64) AS bill_wln_avg_hsic_debit_amt\n",
    ", CAST(bill_wln_avg_hsic_discount_amt AS FLOAT64) AS bill_wln_avg_hsic_discount_amt\n",
    ", CAST(bill_wln_avg_hsic_credit_amt AS FLOAT64) AS bill_wln_avg_hsic_credit_amt\n",
    ", CAST(bill_wln_avg_ttv_debit_amt AS FLOAT64) AS bill_wln_avg_ttv_debit_amt\n",
    ", CAST(bill_wln_avg_ttv_discount_amt AS FLOAT64) AS bill_wln_avg_ttv_discount_amt\n",
    ", CAST(bill_wln_avg_ttv_credit_amt AS FLOAT64) AS bill_wln_avg_ttv_credit_amt\n",
    ", CAST(bill_wln_avg_smhm_debit_amt AS FLOAT64) AS bill_wln_avg_smhm_debit_amt\n",
    ", CAST(bill_wln_avg_smhm_discount_amt AS FLOAT64) AS bill_wln_avg_smhm_discount_amt\n",
    ", CAST(bill_wln_avg_smhm_credit_amt AS FLOAT64) AS bill_wln_avg_smhm_credit_amt\n",
    ", CAST(bill_wln_avg_sing_ld_na_call_cnt AS FLOAT64) AS bill_wln_avg_sing_ld_na_call_cnt\n",
    ", CAST(bill_wln_avg_sing_ld_intl_call_cnt AS FLOAT64) AS bill_wln_avg_sing_ld_intl_call_cnt\n",
    ", CAST(bill_wln_avg_hsic_usg_gb AS FLOAT64) AS bill_wln_avg_hsic_usg_gb\n",
    ", CAST(bill_wln_avg_ttv_ppv_cnt AS FLOAT64) AS bill_wln_avg_ttv_ppv_cnt\n",
    ", CAST(bill_wln_avg_ttv_vod_cnt AS FLOAT64) AS bill_wln_avg_ttv_vod_cnt\n",
    ", clk_wls_tot_cnt_r30d\n",
    ", clk_wls_plan_cnt_r30d\n",
    ", clk_wls_device_cnt_r30d\n",
    ", clk_wls_smartwatch_cnt_r30d\n",
    ", clk_wls_tablet_cnt_r30d\n",
    ", clk_wln_tot_cnt_r30d\n",
    ", clk_wln_eligibility_cnt_r30d\n",
    ", clk_wln_sing_cnt_r30d\n",
    ", clk_wln_hsic_cnt_r30d\n",
    ", clk_wln_fibre_cnt_r30d\n",
    ", clk_wln_whsia_cnt_r30d\n",
    ", clk_wln_wifi_plus_cnt_r30d\n",
    ", clk_wln_tv_cnt_r30d\n",
    ", clk_wln_optik_cnt_r30d\n",
    ", clk_wln_pik_cnt_r30d\n",
    ", clk_wln_streamplus_cnt_r30d\n",
    ", clk_wln_streaming_cnt_r30d\n",
    ", clk_wln_security_cnt_r30d\n",
    ", clk_wln_smarthome_security_cnt_r30d\n",
    ", clk_wln_online_security_cnt_r30d\n",
    ", clk_wln_smartwear_security_cnt_r30d\n",
    ", clk_deal_tot_cnt_r30d\n",
    ", clk_deal_wls_cnt_r30d\n",
    ", clk_deal_wln_cnt_r30d\n",
    ", clk_deal_wln_sing_cnt_r30d\n",
    ", clk_deal_wln_hsic_cnt_r30d\n",
    ", clk_deal_wln_whsia_cnt_r30d\n",
    ", clk_deal_wln_tv_cnt_r30d\n",
    ", clk_deal_wln_security_cnt_r30d\n",
    ", clk_upgr_tot_cnt_r30d\n",
    ", clk_upgr_wls_cnt_r30d\n",
    ", clk_upgr_wln_cnt_r30d\n",
    ", clk_upgr_wln_sing_cnt_r30d\n",
    ", clk_upgr_wln_hsic_cnt_r30d\n",
    ", clk_upgr_wln_whsia_cnt_r30d\n",
    ", clk_upgr_wln_tv_cnt_r30d\n",
    ", clk_upgr_wln_security_cnt_r30d\n",
    ", clk_chg_tot_cnt_r30d\n",
    ", clk_chg_wls_cnt_r30d\n",
    ", clk_chg_wln_cnt_r30d\n",
    ", clk_chg_wln_sing_cnt_r30d\n",
    ", clk_chg_wln_hsic_cnt_r30d\n",
    ", clk_chg_wln_whsia_cnt_r30d\n",
    ", clk_chg_wln_tv_cnt_r30d\n",
    ", clk_chg_wln_security_cnt_r30d\n",
    ", clk_health_livingwell_cnt_r30d\n",
    ", clk_health_mypet_cnt_r30d\n",
    ", clk_travel_cnt_r30d\n",
    ", clk_billing_cnt_r30d\n",
    ", clk_service_agreement_cnt_r30d\n",
    ", acct_cr_risk_txt\n",
    ", acct_ebill_ind\n",
    ", cust_cr_val_txt\n",
    ", cust_pref_lang_txt\n",
    ", cust_prov_state_cd\n",
    ", cust_age_yr_num\n",
    ", demogr_med_age\n",
    ", demogr_avg_child\n",
    ", demogr_pct_family_with_child_living_at_home\n",
    ", demogr_employment_rate\n",
    ", demogr_avg_household_size\n",
    ", demogr_avg_income\n",
    ", demogr_med_income\n",
    ", demogr_urban_flag\n",
    ", demogr_rural_flag\n",
    ", demogr_family_flag\n",
    ", demogr_lsname_large_diverse_families\n",
    ", demogr_lsname_younger_singles_and_couples\n",
    ", demogr_lsname_very_young_singles_and_couples\n",
    ", demogr_lsname_older_families_and_empty_nests\n",
    ", demogr_lsname_middle_age_families\n",
    ", demogr_lsname_mature_singles_and_couples\n",
    ", demogr_lsname_young_families\n",
    ", demogr_lsname_school_age_families\n",
    ", demogr_retired_pstl_cd_ind\n",
    ", demogr_census_division_typ\n",
    ", demogr_lifestage_sort\n",
    ", CAST(hs_usg_avg_tot_gb AS FLOAT64) AS hs_usg_avg_tot_gb\n",
    ", CAST(hs_usg_avg_dl_gb AS FLOAT64) AS hs_usg_avg_dl_gb\n",
    ", CAST(hs_usg_avg_ul_gb AS FLOAT64) AS hs_usg_avg_ul_gb\n",
    ", prod_latest_actvn_dt\n",
    ", prod_latest_deactvn_dt\n",
    ", prod_tot_cnt\n",
    ", prod_wln_cnt\n",
    ", prod_wls_cnt\n",
    ", prod_mob_cnt\n",
    ", prod_sing_cnt\n",
    ", prod_hsic_cnt\n",
    ", prod_whsia_cnt\n",
    ", prod_ttv_cnt\n",
    ", prod_smhm_cnt\n",
    ", prod_tos_cnt\n",
    ", prod_wifiplus_cnt\n",
    ", prod_stv_cnt\n",
    ", prod_other_cnt\n",
    ", prod_deact_prod_cnt\n",
    ", prod_act_prod_cnt_r7d\n",
    ", prod_act_wln_cnt_r7d\n",
    ", prod_deact_prod_cnt_r7d\n",
    ", prod_deact_wln_cnt_r7d\n",
    ", hsic_tenure_days\n",
    ", contract_end_date_hsic\n",
    ", sing_tenure_days\n",
    ", contract_end_date_sing\n",
    ", ttv_tenure_days\n",
    ", contract_end_date_ttv\n",
    ", smhm_tenure_days\n",
    ", contract_end_date_smhm\n",
    ", ffh_tenure\n",
    ", new_account_ind\n",
    "\n",
    "FROM `divg-groovyhoon-pr-d2eab4.nba_product_reco_model.nba_training_dataset` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1294f3f-ee1c-4c57-bbb1-635ef08b52d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import google\n",
    "from google.oauth2 import credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2.service_account import Credentials\n",
    "import datetime\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# build model\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances\n",
    "import warnings\n",
    "\n",
    "SERVICE_TYPE = 'nba_product_reco_model'\n",
    "DATASET_ID = 'nba_product_reco_model'\n",
    "PROJECT_ID = 'divg-groovyhoon-pr-d2eab4' #mapping['PROJECT_ID']\n",
    "RESOURCE_BUCKET = 'divg-groovyhoon-pr-d2eab4-default' #mapping['resources_bucket']\n",
    "FILE_BUCKET = 'divg-groovyhoon-pr-d2eab4-default' #mapping['gcs_csv_bucket']\n",
    "REGION = 'northamerica-northeast1' #mapping['REGION']\n",
    "MODEL_ID = '9999'\n",
    "FOLDER_NAME = 'nba_product_reco_model'.format(MODEL_ID)\n",
    "QUERIES_PATH = 'vertex_pipelines/' + FOLDER_NAME + '/queries/'\n",
    "TRAIN_TABLE_ID = 'nba_training_dataset_v3'\n",
    "VAL_TABLE_ID = 'nba_test_dataset_v3'\n",
    "SCORE_TABLE_ID = 'bq_product_reco_scores'\n",
    "\n",
    "scoringDate = date(2023, 10, 13)  # date.today() - relativedelta(days=2)- relativedelta(months=30)\n",
    "valScoringDate = date(2023, 11, 13)  # scoringDate - relativedelta(days=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30300f-ff71-417d-956f-6d3dcede6c29",
   "metadata": {},
   "source": [
    "### import bq to dataframe function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e098413-7211-4346-b1c9-eafd38ef076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import credentials\n",
    "\n",
    "def import_bq_to_dataframe(project_id, dataset_id, table_id, client): \n",
    "    \n",
    "    \"\"\"\n",
    "    Imports a specific table from BigQuery to a DataFrame. \n",
    "    \n",
    "    Args: \n",
    "        project_id: The name of the project_id where the table is located.\n",
    "        dataset_id: The name of the dataset_id where the table is located.\n",
    "        table_id: The name of the table_id you wish to import to DataFrame.\n",
    "        client: A BigQuery client instance. e.g. client = bigquery.Client(project=project_id).\n",
    "\n",
    "    Returns: \n",
    "        A DataFrame\n",
    "        \n",
    "    Example: \n",
    "        import_bq_to_dataframe('bi-stg-divg-speech-pr-9d940b', 'call_to_retention_dataset', 'bq_ctr_pipeline_dataset')\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    sql = f\"SELECT * FROM `{project_id}.{dataset_id}.{table_id}`\"\n",
    "    \n",
    "    df_return = client.query(sql).to_dataframe()\n",
    "\n",
    "    return df_return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a31e45-47f9-445e-aab4-1e2d8243e1f9",
   "metadata": {},
   "source": [
    "### define get_lift function, import df_train and df_test from gcs bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3837b0-0180-4a8a-978d-b56402062efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "project_id = PROJECT_ID\n",
    "region = REGION\n",
    "resource_bucket = RESOURCE_BUCKET\n",
    "file_bucket = FILE_BUCKET\n",
    "service_type=SERVICE_TYPE\n",
    "project_id=PROJECT_ID\n",
    "dataset_id=DATASET_ID\n",
    "train_table_id = TRAIN_TABLE_ID\n",
    "val_table_id = VAL_TABLE_ID\n",
    "\n",
    "def get_lift(prob, y_test, q):\n",
    "    result = pd.DataFrame(columns=['Prob', 'Churn'])\n",
    "    result['Prob'] = prob\n",
    "    result['Churn'] = y_test\n",
    "    # result['Decile'] = pd.qcut(1-result['Prob'], 10, labels = False)\n",
    "    result['Decile'] = pd.qcut(result['Prob'], q, labels=[i for i in range(q, 0, -1)])\n",
    "    add = pd.DataFrame(result.groupby('Decile')['Churn'].mean()).reset_index()\n",
    "    add.columns = ['Decile', 'avg_real_churn_rate']\n",
    "    result = result.merge(add, on='Decile', how='left')\n",
    "    result.sort_values('Decile', ascending=True, inplace=True)\n",
    "    lg = pd.DataFrame(result.groupby('Decile')['Prob'].mean()).reset_index()\n",
    "    lg.columns = ['Decile', 'avg_model_pred_churn_rate']\n",
    "    lg.sort_values('Decile', ascending=False, inplace=True)\n",
    "    lg['avg_churn_rate_total'] = result['Churn'].mean()\n",
    "    lg = lg.merge(add, on='Decile', how='left')\n",
    "    lg['lift'] = lg['avg_real_churn_rate'] / lg['avg_churn_rate_total']\n",
    "\n",
    "    return lg\n",
    "\n",
    "def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "    token = os.popen('gcloud auth print-access-token').read()\n",
    "    token = re.sub(f'\\n$', '', token)\n",
    "    credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    if use_local_credential:\n",
    "        bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "    return bq_client\n",
    "\n",
    "client = get_gcp_bqclient(project_id)\n",
    "\n",
    "df_train = import_bq_to_dataframe(project_id, dataset_id, train_table_id, client)\n",
    "df_val = import_bq_to_dataframe(project_id, dataset_id, val_table_id, client)\n",
    "\n",
    "scenario_to_target = {\n",
    "    'hsic_acquisition': 0,\n",
    "    'ttv_acquisition': 1,\n",
    "    'sing_acquisition': 2,\n",
    "    'shs_acquisition': 3, \n",
    "    'tos_acquisition': 4, \n",
    "    'wifi_acquisition': 5, \n",
    "    'lwc_acquisition': 6, \n",
    "    'sws_acquisition': 7, \n",
    "    'hpro_acquisition': 8\n",
    "}\n",
    "\n",
    "df_train['target'] = df_train['model_scenario'].map(scenario_to_target)\n",
    "df_val['target'] = df_val['model_scenario'].map(scenario_to_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e270188-696f-4b14-8f4a-6463bf1552f4",
   "metadata": {},
   "source": [
    "### Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214d3a22-8203-4a36-be64-8e2f5f5054ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['demogr_lifestage_sort'].fillna(6, inplace=True)\n",
    "df_train['cust_pref_lang_txt'].fillna('NOT AVAILABLE', inplace=True)\n",
    "df_train['demogr_census_division_typ'].fillna('NA', inplace=True)\n",
    "df_train['cust_prov_state_cd'].fillna('N/AVAIL', inplace=True)\n",
    "df_train['cust_cr_val_txt'].fillna('NOT AVAILABLE', inplace=True)\n",
    "df_train['acct_ebill_ind'].fillna('N', inplace=True)\n",
    "\n",
    "df_val['demogr_lifestage_sort'].fillna(6, inplace=True)\n",
    "df_val['cust_pref_lang_txt'].fillna('NOT AVAILABLE', inplace=True)\n",
    "df_val['demogr_census_division_typ'].fillna('NA', inplace=True)\n",
    "df_val['cust_prov_state_cd'].fillna('N/AVAIL', inplace=True)\n",
    "df_val['cust_cr_val_txt'].fillna('NOT AVAILABLE', inplace=True)\n",
    "df_val['acct_ebill_ind'].fillna('N', inplace=True)\n",
    "\n",
    "print(f'df_train: {df_train.shape}')\n",
    "print(f'df_val: {df_val.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94961cc0-7e92-44eb-9e1e-15d16d2024ed",
   "metadata": {},
   "source": [
    "### define train, test, and val dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc272d97-768b-401b-9100-923cc63cf1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "df_train, df_test = train_test_split(df_train, test_size=0.3, random_state=42, stratify=df_train['target'])\n",
    "\n",
    "cols_to_impute = ['demogr_avg_child','demogr_avg_household_size','demogr_avg_income','demogr_employment_rate'\n",
    "                        ,'demogr_med_age','demogr_med_income','demogr_pct_family_with_child_living_at_home']\n",
    "\n",
    "def impute_with_average(df, cols_to_impute):\n",
    "    \n",
    "    for col in cols_to_impute:\n",
    "        column_mean = df[col].mean()\n",
    "        df[col].fillna(column_mean, inplace=True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "df_train = impute_with_average(df_train, cols_to_impute)\n",
    "df_test = impute_with_average(df_test, cols_to_impute)\n",
    "df_val = impute_with_average(df_val, cols_to_impute)\n",
    "\n",
    "df_train.to_csv('gs://{}/{}/{}_train_final.csv'.format(FILE_BUCKET, SERVICE_TYPE, SERVICE_TYPE), index=False)\n",
    "df_test.to_csv('gs://{}/{}/{}_test_final.csv'.format(FILE_BUCKET, SERVICE_TYPE, SERVICE_TYPE), index=False)\n",
    "df_val.to_csv('gs://{}/{}/{}_val_final.csv'.format(FILE_BUCKET, SERVICE_TYPE, SERVICE_TYPE), index=False)\n",
    "\n",
    "#set up features (list)\n",
    "cols_1 = df_train.columns.values\n",
    "cols_2 = df_test.columns.values\n",
    "cols_3 = df_val.columns.values\n",
    "\n",
    "cols = set(cols_1).intersection(set(cols_2))\n",
    "cols = set(cols).intersection(set(cols_3))\n",
    "\n",
    "id_variables = ['split_type', 'model_scenario', 'ref_dt', 'cust_id', 'cust_src_id', 'ban', 'ban_src_id', 'lpds_id', 'fms_address_id', 'label', 'label_dt', 'target']\n",
    "\n",
    "features_to_exclude = ['split_type', 'cust_src_id', 'ban_src_id', 'fms_address_id', 'label', 'label_dt', 'avg_usg_cnt_4w_news', \n",
    "'avg_usg_dl_4w_news', 'avg_usg_ul_4w_news', 'avg_usg_cnt_4w_sports', 'avg_usg_dl_4w_sports', 'avg_usg_ul_4w_sports', 'avg_usg_cnt_4w_tv_and_movies', 'avg_usg_dl_4w_tv_and_movies', \n",
    "'avg_usg_ul_4w_tv_and_movies', 'bill_wln_avg_zscore_ban_subtotal_amt', 'bill_wln_stddev_ban_subtotal_amt', 'bill_wln_avg_zscore_ban_debit_amt', 'bill_wln_stddev_ban_debit_amt', \n",
    "'bill_wln_avg_zscore_ban_discount_amt', 'bill_wln_stddev_ban_discount_amt', 'bill_wln_avg_zscore_ban_credit_amt', 'bill_wln_stddev_ban_credit_amt', 'bill_wln_avg_zscore_sing_debit_amt', \n",
    "'bill_wln_avg_zscore_sing_discount_amt', 'bill_wln_avg_zscore_sing_credit_amt', 'bill_wln_avg_zscore_hsic_debit_amt', 'bill_wln_avg_zscore_hsic_discount_amt', 'bill_wln_avg_zscore_hsic_credit_amt', \n",
    "'bill_wln_avg_zscore_ttv_debit_amt', 'bill_wln_avg_zscore_ttv_discount_amt', 'bill_wln_avg_zscore_ttv_credit_amt', 'bill_wln_avg_zscore_smhm_debit_amt', 'bill_wln_avg_zscore_smhm_discount_amt', \n",
    "'bill_wln_avg_zscore_smhm_credit_amt', 'bill_wln_avg_zscore_sing_ld_na_call_cnt', 'bill_wln_avg_zscore_sing_ld_intl_call_cnt', 'bill_wln_avg_zscore_hsic_usg_gb', 'bill_wln_avg_zscore_ttv_ppv_cnt', \n",
    "'bill_wln_avg_zscore_ttv_vod_cnt', 'bill_wln_stddev_sing_debit_amt', 'bill_wln_stddev_sing_discount_amt', 'bill_wln_stddev_sing_credit_amt', 'bill_wln_stddev_hsic_debit_amt', \n",
    "'bill_wln_stddev_hsic_discount_amt', 'bill_wln_stddev_hsic_credit_amt', 'bill_wln_stddev_ttv_debit_amt', 'bill_wln_stddev_ttv_discount_amt', 'bill_wln_stddev_ttv_credit_amt', \n",
    "'bill_wln_stddev_smhm_debit_amt', 'bill_wln_stddev_smhm_discount_amt', 'bill_wln_stddev_smhm_credit_amt', 'bill_wln_stddev_sing_ld_na_call_cnt', 'bill_wln_stddev_sing_ld_intl_call_cnt', \n",
    "'bill_wln_stddev_hsic_usg_gb', 'bill_wln_stddev_ttv_ppv_cnt', 'bill_wln_stddev_ttv_vod_cnt', 'clk_wls_smartwatch_cnt_r30d', 'clk_wls_tablet_cnt_r30d', 'clk_deal_wls_cnt_r30d', \n",
    "'clk_deal_wln_cnt_r30d', 'clk_deal_wln_sing_cnt_r30d', 'clk_deal_wln_hsic_cnt_r30d', 'clk_deal_wln_whsia_cnt_r30d', 'clk_deal_wln_tv_cnt_r30d', 'clk_deal_wln_security_cnt_r30d', 'clk_upgr_wls_cnt_r30d', \n",
    "'clk_upgr_wln_cnt_r30d', 'clk_upgr_wln_sing_cnt_r30d', 'clk_upgr_wln_hsic_cnt_r30d', 'clk_upgr_wln_whsia_cnt_r30d', 'clk_upgr_wln_tv_cnt_r30d', 'clk_upgr_wln_security_cnt_r30d', 'clk_chg_wls_cnt_r30d', \n",
    "'clk_chg_wln_cnt_r30d', 'clk_chg_wln_sing_cnt_r30d', 'clk_chg_wln_hsic_cnt_r30d', 'clk_chg_wln_whsia_cnt_r30d', 'clk_chg_wln_tv_cnt_r30d', 'clk_chg_wln_security_cnt_r30d', 'clk_health_livingwell_cnt_r30d', \n",
    "'clk_health_mypet_cnt_r30d', 'clk_travel_cnt_r30d', 'acct_cr_risk_txt', 'cust_age_yr_num', 'demogr_lsname_large_diverse_families', 'demogr_lsname_younger_singles_and_couples', 'demogr_lsname_very_young_singles_and_couples', \n",
    "'demogr_lsname_older_families_and_empty_nests', 'demogr_lsname_middle_age_families', 'demogr_lsname_mature_singles_and_couples', 'demogr_lsname_young_families', 'demogr_lsname_school_age_families', \n",
    "'demogr_retired_pstl_cd_ind', 'prod_latest_actvn_dt', 'prod_latest_deactvn_dt', 'prod_deact_prod_cnt', 'prod_act_prod_cnt_r7d', 'prod_act_wln_cnt_r7d', 'prod_deact_prod_cnt_r7d', 'prod_deact_wln_cnt_r7d', \n",
    "'contract_end_date_hsic', 'contract_end_date_sing', 'contract_end_date_ttv', 'contract_end_date_smhm'] \n",
    "\n",
    "features = [f for f in cols if f not in features_to_exclude]\n",
    "\n",
    "train_id = df_train[id_variables]\n",
    "ban_train = df_train[['ban', 'lpds_id']]\n",
    "X_train = df_train[features]\n",
    "y_train = np.squeeze(df_train['target'].values)\n",
    "target_train = df_train['target']\n",
    "\n",
    "test_id = df_test[id_variables]\n",
    "ban_test = df_test[['ban', 'lpds_id']]\n",
    "X_test = df_test[features]\n",
    "y_test = np.squeeze(df_test['target'].values)\n",
    "target_test = df_test['target']\n",
    "\n",
    "val_id = df_val[id_variables]\n",
    "ban_val = df_val[['ban', 'lpds_id']]\n",
    "X_val = df_val[features]\n",
    "y_val = np.squeeze(df_val['target'].values)\n",
    "target_val = df_val['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee477829-f647-4302-b142-67f33d33dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11027df1-f59a-47e9-9db7-32325ac3b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to transform the features of the feature store.\n",
    "def encode_categorical_features(train_df, test_df, val_df):\n",
    "    # Get a list of all categorical columns\n",
    "    cat_columns = train_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # Encode each categorical column\n",
    "    for col in cat_columns:\n",
    "        le = LabelEncoder()\n",
    "        train_df[col] = le.fit_transform(train_df[col])\n",
    "        test_df[col] = le.fit_transform(test_df[col])\n",
    "        val_df[col] = le.fit_transform(val_df[col])\n",
    "        \n",
    "    return train_df, test_df, val_df\n",
    "\n",
    "#excluding the customer ID so it doesn't get encoded\n",
    "train_label_data=X_train[X_train.columns.difference(['cust_id','ban','lpds_id','ref_dt','model_scenario'])]\n",
    "test_label_data=X_test[X_test.columns.difference(['cust_id','ban','lpds_id','ref_dt','model_scenario'])]\n",
    "val_label_data=X_val[X_val.columns.difference(['cust_id','ban','lpds_id','ref_dt','model_scenario'])]\n",
    "\n",
    "X_train_feat_enc, X_test_feat_enc, X_val_feat_enc = encode_categorical_features(train_label_data,test_label_data,val_label_data)\n",
    "\n",
    "X_train_feat_enc.fillna(0, inplace=True)\n",
    "X_test_feat_enc.fillna(0, inplace=True)\n",
    "X_val_feat_enc.fillna(0, inplace=True)\n",
    "\n",
    "##bringing back the customer ids keys\n",
    "X_train_feat_enc['cust_id'] = X_train['cust_id']\n",
    "X_train_feat_enc['ban'] = X_train['ban']\n",
    "X_train_feat_enc['lpds_id'] = X_train['lpds_id']\n",
    "X_train_feat_enc['ref_dt'] = X_train['ref_dt'] \n",
    "X_train_feat_enc['model_scenario'] = X_train['model_scenario']\n",
    "\n",
    "X_test_feat_enc['cust_id'] = X_test['cust_id']\n",
    "X_test_feat_enc['ban'] = X_test['ban'] \n",
    "X_test_feat_enc['lpds_id'] = X_test['lpds_id']\n",
    "X_test_feat_enc['ref_dt'] = X_test['ref_dt'] \n",
    "X_test_feat_enc['model_scenario'] = X_test['model_scenario'] \n",
    "\n",
    "X_val_feat_enc['cust_id'] = X_val['cust_id'] \n",
    "X_val_feat_enc['ban'] = X_val['ban']\n",
    "X_val_feat_enc['lpds_id'] = X_val['lpds_id'] \n",
    "X_val_feat_enc['ref_dt'] = X_val['ref_dt']\n",
    "X_val_feat_enc['model_scenario'] = X_val['model_scenario']\n",
    "\n",
    "train = pd.merge(X_train_feat_enc, train_id[['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id']],how = 'inner',on=['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id'])\n",
    "test = pd.merge(X_test_feat_enc, test_id[['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id']],how = 'inner',on=['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id'])\n",
    "val = pd.merge(X_train_feat_enc, val_id[['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id']],how = 'inner',on=['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f02caa-bf32-49b9-bc60-a024f7bf8639",
   "metadata": {},
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405d8eea-f186-48b8-8aaf-401908f1a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_a = ['a', 'b', 'c', 'd', 'e'] \n",
    "list_a[2, 3, 1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdfbb80-9392-49b4-93a8-48c2cf553832",
   "metadata": {},
   "source": [
    "### Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bcd340-8c16-41ce-a64d-25a6869db651",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def final_prod_reco(prod_mix, prod_reco_list): \n",
    "    \n",
    "    # This function accepts the customer's product mix and product recommenndations as input parameters, \n",
    "    # and filters out the product recommendations that the customer already has.\n",
    "    \n",
    "    # 1. check the customer's product intensity\n",
    "    cust_prod_list = []\n",
    "\n",
    "    if prod_mix[0] > 0: \n",
    "        cust_prod_list.append('hsic')\n",
    "    if prod_mix[1] + prod_mix[2] > 0: \n",
    "        cust_prod_list.append('ttv')\n",
    "    if prod_mix[3] > 0: \n",
    "        cust_prod_list.append('sing')\n",
    "    if prod_mix[4] > 0: \n",
    "        cust_prod_list.append('smhm')\n",
    "    if prod_mix[5] > 0: \n",
    "        cust_prod_list.append('tos')\n",
    "    if prod_mix[6] > 0: \n",
    "        cust_prod_list.append('wifi')\n",
    "    if prod_mix[7] > 0: \n",
    "        cust_prod_list.append('whsia')\n",
    "    ### add for lwc, sws, hrpo as product count labels become available ###\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('lwc')\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('sws')\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('hpro')\n",
    "\n",
    "    # 2. trim the prodduct reco list to remove \"_acquisition\" for comparison. \n",
    "    prod_reco_list = [value.replace('_acquisition', '') for value in prod_reco_list]\n",
    "\n",
    "    final_prod_reco_list = [item for item in prod_reco_list if item not in cust_prod_list]\n",
    "\n",
    "    return final_prod_reco_list\n",
    "\n",
    "# extract the feature vectors of all customers\n",
    "features = list(df.columns.difference(['target', 'cust_id', 'ban', 'lpds_id', 'ref_dt', 'model_scenario']))\n",
    "X = df[features].values\n",
    "\n",
    "# extract the feature vector of the given customer\n",
    "index = df[(df['ban'] == ban) & (df['ref_dt']==ref_dt) & (df['model_scenario']==model_scenario)].index[0]\n",
    "x = X[index]\n",
    "\n",
    "# compute the distances between the feature vectors\n",
    "if distance_func == 'euclidean':\n",
    "    distances = euclidean_distances(X, x.reshape(1, -1)).flatten()\n",
    "elif distance_func == 'manhattan':\n",
    "    distances = manhattan_distances(X, x.reshape(1, -1)).flatten()\n",
    "elif distance_func == 'cosine':\n",
    "    distances = 1 - cosine_similarity(X, x.reshape(1, -1)).flatten()\n",
    "else:\n",
    "    raise ValueError('Invalid distance function specified.')\n",
    "\n",
    "# find the indices of the n customers with lowest distance\n",
    "most_similar_indices = distances.argsort()[:n]\n",
    "\n",
    "# extract the customer data for the most similar customers\n",
    "similar_customers = df.iloc[most_similar_indices]\n",
    "\n",
    "similar_customers['distances'] = np.array(distances\n",
    "\n",
    "# merge with the id dataframe to select only the customers who did not churn\n",
    "similar_customers = pd.merge(similar_customers,df_id[['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id']],on=['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id'])\n",
    "    \n",
    "# calculate the target label distribution\n",
    "target_dist = df[['cust_id', 'ban', 'lpds_id', 'model_scenario', 'target']].groupby(['model_scenario', 'target'])['ban'].count().reset_index(name='ban_count_total').sort_values(by='ban_count_total', ascending=False)\n",
    "target_dist['perc_total_all'] = target_dist['ban_count_total']/target_dist['ban_count_total'].sum()\n",
    "\n",
    "#count the top offers of the similar customers\n",
    "# top_offers = similar_customers[['cust_id', 'ban', 'lpds_id', 'model_scenario', 'target_y']].groupby(['model_scenario', 'target_y']).agg({'cust_id':'count'}).reset_index().sort_values(by = 'cust_id', ascending = False)\n",
    "top_offers = similar_customers[['cust_id', 'ban', 'lpds_id', 'model_scenario', 'target']].groupby(['model_scenario', 'target'])['ban'].count().reset_index(name='ban_count').sort_values(by='ban_count', ascending=False)\n",
    "top_offers['perc_total'] = top_offers['ban_count']/top_offers['ban_count'].sum()\n",
    "\n",
    "# merge target_dist and top_offers into a single dataframe\n",
    "merged_df = pd.merge(target_dist, top_offers, on='model_scenario', how='inner')\n",
    "merged_df['lift'] = merged_df['perc_total_all'] / merged_df['perc_total'] \n",
    "\n",
    "merged_df = merged_df.sort_values(by='lift', ascending=False)\n",
    "\n",
    "print(merged_df['model_scenario'].head(3)) #shs, tos, sing\n",
    "\n",
    "# prod_mix = df[prod_mix_cols].values[index]\n",
    "# prod_reco_list = list(top_offers['model_scenario'][:])\n",
    "\n",
    "# final_prod_reco_list = final_prod_reco(prod_mix, prod_reco_list)\n",
    "\n",
    "# print(final_prod_reco_list)\n",
    "\n",
    "# # top_offers_min = top_offers[top_offers['perc_total'] > minimal_threshold].head(max_offers_to_return)\n",
    "\n",
    "# # top_offers_min['offer'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd20e60d-ae92-4bcd-a84e-eeba2c741a92",
   "metadata": {},
   "source": [
    "### 1. create get_recommended_offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63c9aba-6393-4a49-a46f-9581b8605022",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def final_prod_reco(prod_mix, prod_reco_list): \n",
    "    \n",
    "    # This function accepts the customer's product mix and product recommenndations as input parameters, \n",
    "    # and filters out the product recommendations that the customer already has.\n",
    "    \n",
    "    # 1. check the customer's product intensity\n",
    "    cust_prod_list = []\n",
    "\n",
    "    if prod_mix[0] > 0: \n",
    "        cust_prod_list.append('hsic')\n",
    "    if prod_mix[1] + prod_mix[2] > 0: \n",
    "        cust_prod_list.append('ttv')\n",
    "    if prod_mix[3] > 0: \n",
    "        cust_prod_list.append('sing')\n",
    "    if prod_mix[4] > 0: \n",
    "        cust_prod_list.append('smhm')\n",
    "    if prod_mix[5] > 0: \n",
    "        cust_prod_list.append('tos')\n",
    "    if prod_mix[6] > 0: \n",
    "        cust_prod_list.append('wifi')\n",
    "    if prod_mix[7] > 0: \n",
    "        cust_prod_list.append('whsia')\n",
    "    ### add for lwc, sws, hrpo as product count labels become available ###\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('lwc')\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('sws')\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('hpro')\n",
    "\n",
    "    # 2. trim the prodduct reco list to remove \"_acquisition\" for comparison. \n",
    "    prod_reco_list = [value.replace('_acquisition', '') for value in prod_reco_list]\n",
    "\n",
    "    final_prod_reco_list = [item for item in prod_reco_list if item not in cust_prod_list]\n",
    "\n",
    "    return final_prod_reco_list\n",
    "\n",
    "def get_recommended_offers (df: pd.DataFrame, \n",
    "                            id_cols: list,\n",
    "                            prod_mix_cols: list, \n",
    "                            ban: int, \n",
    "                            ref_dt, \n",
    "                            distance_func: str,\n",
    "                            n: int,\n",
    "                            minimal_threshold: float,\n",
    "                            max_offers_to_return: int\n",
    "                            ):\n",
    "    \n",
    "    # define df_id\n",
    "    df_id = df[id_cols]\n",
    "\n",
    "    # extract the feature vectors of all customers\n",
    "    features = list(df.columns.difference(id_cols))\n",
    "    X = df[features].values\n",
    "\n",
    "    # extract the feature vector of the given customer\n",
    "    index = df[(df['ban'] == ban) & (df['ref_dt']==ref_dt)].index[0]\n",
    "    x = X[index]\n",
    "\n",
    "    # compute the distances between the feature vectors\n",
    "    if distance_func == 'euclidean':\n",
    "        distances = euclidean_distances(X, x.reshape(1, -1)).flatten()\n",
    "    elif distance_func == 'manhattan':\n",
    "        distances = manhattan_distances(X, x.reshape(1, -1)).flatten()\n",
    "    elif distance_func == 'cosine':\n",
    "        distances = 1 - cosine_similarity(X, x.reshape(1, -1)).flatten()\n",
    "    else:\n",
    "        raise ValueError('Invalid distance function specified.')\n",
    "\n",
    "    # find the indices of the n customers with lowest distance\n",
    "    most_similar_indices = distances.argsort()[:n]\n",
    "\n",
    "    # extract the customer data for the most similar customers\n",
    "    similar_customers = df.iloc[most_similar_indices]\n",
    "    \n",
    "    # calculate the target label distribution\n",
    "    target_dist = df[['cust_id', 'ban', 'lpds_id', 'model_scenario', 'target']].groupby(['model_scenario', 'target'])['ban'].count().reset_index(name='ban_count_total').sort_values(by='ban_count_total', ascending=False)\n",
    "    target_dist['perc_total_all'] = target_dist['ban_count_total']/target_dist['ban_count_total'].sum()\n",
    "\n",
    "    # count the top offers of the similar customers\n",
    "    # top_offers = similar_customers[['cust_id', 'ban', 'lpds_id', 'model_scenario', 'target_y']].groupby(['model_scenario', 'target_y']).agg({'cust_id':'count'}).reset_index().sort_values(by = 'cust_id', ascending = False)\n",
    "    top_offers = similar_customers[['cust_id', 'ban', 'lpds_id', 'model_scenario', 'target']].groupby(['model_scenario', 'target'])['ban'].count().reset_index(name='ban_count').sort_values(by='ban_count', ascending=False)\n",
    "    top_offers['perc_total'] = top_offers['ban_count']/top_offers['ban_count'].sum()\n",
    "    \n",
    "    # merge target_dist and top_offers into a single dataframe\n",
    "    merged_df = pd.merge(target_dist, top_offers, on='model_scenario', how='inner')\n",
    "    merged_df['lift'] = merged_df['perc_total_all'] / merged_df['perc_total'] \n",
    "    \n",
    "    # sort by lift in descending order \n",
    "    merged_df = merged_df.sort_values(by='lift', ascending=False)\n",
    "    \n",
    "    # generate product recommendations list based on collaborative filtering\n",
    "    prod_mix = df[prod_mix_cols].values[index]\n",
    "    prod_reco_list = list(merged_df['model_scenario'][:])\n",
    "    \n",
    "    # filter out product recos that the customer already has\n",
    "    final_prod_reco_list = final_prod_reco(prod_mix, prod_reco_list)\n",
    "\n",
    "    return final_prod_reco_list \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860ec674-171e-438c-b660-c550bb1acf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_prod_reco(prod_mix, prod_reco_list): \n",
    "    \n",
    "    # This function accepts the customer's product mix and product recommenndations as input parameters, \n",
    "    # and filters out the product recommendations that the customer already has.\n",
    "    \n",
    "    # 1. check the customer's product intensity\n",
    "    cust_prod_list = []\n",
    "\n",
    "    if prod_mix[0] > 0: \n",
    "        cust_prod_list.append('hsic')\n",
    "    if prod_mix[1] + prod_mix[2] > 0: \n",
    "        cust_prod_list.append('ttv')\n",
    "    if prod_mix[3] > 0: \n",
    "        cust_prod_list.append('sing')\n",
    "    if prod_mix[4] > 0: \n",
    "        cust_prod_list.append('smhm')\n",
    "    if prod_mix[5] > 0: \n",
    "        cust_prod_list.append('tos')\n",
    "    if prod_mix[6] > 0: \n",
    "        cust_prod_list.append('wifi')\n",
    "    if prod_mix[7] > 0: \n",
    "        cust_prod_list.append('whsia')\n",
    "    ### add for lwc, sws, hrpo as product count labels become available ###\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('lwc')\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('sws')\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('hpro')\n",
    "\n",
    "    # 2. trim the prodduct reco list to remove \"_acquisition\" for comparison. \n",
    "    prod_reco_list = [value.replace('_acquisition', '') for value in prod_reco_list]\n",
    "\n",
    "    final_prod_reco_list = [item for item in prod_reco_list if item not in cust_prod_list]\n",
    "\n",
    "    return final_prod_reco_list\n",
    "\n",
    "def get_recommended_offers (df: pd.DataFrame, \n",
    "                            id_cols: list,\n",
    "                            prod_mix_cols: list, \n",
    "                            ban: int, \n",
    "                            ref_dt, \n",
    "                            distance_func: str,\n",
    "                            n: int,\n",
    "                            minimal_threshold: float,\n",
    "                            max_offers_to_return: int\n",
    "                            ):\n",
    "    \n",
    "    # define df_id\n",
    "    df_id = df[id_cols]\n",
    "\n",
    "    # extract the feature vectors of all customers\n",
    "    features = list(df.columns.difference(id_cols))\n",
    "    X = df[features].values\n",
    "\n",
    "    # extract the feature vector of the given customer\n",
    "    index = df[(df['ban'] == ban) & (df['ref_dt']==ref_dt)].index[0]\n",
    "    x = X[index]\n",
    "\n",
    "    # compute the distances between the feature vectors\n",
    "    if distance_func == 'euclidean':\n",
    "        distances = euclidean_distances(X, x.reshape(1, -1)).flatten()\n",
    "    elif distance_func == 'manhattan':\n",
    "        distances = manhattan_distances(X, x.reshape(1, -1)).flatten()\n",
    "    elif distance_func == 'cosine':\n",
    "        distances = 1 - cosine_similarity(X, x.reshape(1, -1)).flatten()\n",
    "    else:\n",
    "        raise ValueError('Invalid distance function specified.')\n",
    "\n",
    "    # find the indices of the n customers with lowest distance\n",
    "    most_similar_indices = distances.argsort()[:n]\n",
    "\n",
    "    # extract the customer data for the most similar customers\n",
    "    similar_customers = df.iloc[most_similar_indices]\n",
    "    \n",
    "    # calculate the target label distribution\n",
    "    target_dist = df[['cust_id', 'ban', 'lpds_id', 'model_scenario', 'target']].groupby(['model_scenario', 'target'])['ban'].count().reset_index(name='ban_count_total').sort_values(by='ban_count_total', ascending=False)\n",
    "    target_dist['perc_total_all'] = target_dist['ban_count_total']/target_dist['ban_count_total'].sum()\n",
    "\n",
    "    # count the top offers of the similar customers\n",
    "    # top_offers = similar_customers[['cust_id', 'ban', 'lpds_id', 'model_scenario', 'target_y']].groupby(['model_scenario', 'target_y']).agg({'cust_id':'count'}).reset_index().sort_values(by = 'cust_id', ascending = False)\n",
    "    top_offers = similar_customers[['cust_id', 'ban', 'lpds_id', 'model_scenario', 'target']].groupby(['model_scenario', 'target'])['ban'].count().reset_index(name='ban_count').sort_values(by='ban_count', ascending=False)\n",
    "    top_offers['perc_total'] = top_offers['ban_count']/top_offers['ban_count'].sum()\n",
    "    \n",
    "    # generate product recommendations list based on collaborative filtering\n",
    "    prod_mix = df[prod_mix_cols].values[index]\n",
    "    prod_reco_list = list(top_offers['model_scenario'][:])\n",
    "    \n",
    "    # filter out product recos that the customer already has\n",
    "    final_prod_reco_list = final_prod_reco(prod_mix, prod_reco_list)\n",
    "\n",
    "    return final_prod_reco_list \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3578682-ef3a-4cbc-833b-5e6511a29b89",
   "metadata": {},
   "source": [
    "### apply the function to a single customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3b6cea-72d2-4ad2-aa6b-677d1d4af197",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train\n",
    "# prod_mix_cols: internet, tv, tv, sing, smhm, tos, wifi, whsia\n",
    "prod_mix_cols = ['prod_hsic_cnt','prod_ttv_cnt','prod_stv_cnt','prod_sing_cnt','prod_smhm_cnt','prod_tos_cnt','prod_wifiplus_cnt','prod_whsia_cnt']\n",
    "id_cols=['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id', 'target']\n",
    "df_id = df[id_cols]\n",
    "ban = 208671688\n",
    "ref_dt = datetime.date(2023, 7, 1)\n",
    "model_scenario = \"ttv_acquisition\"\n",
    "distance_func = 'cosine'\n",
    "n = 500\n",
    "minimal_threshold = 0.10\n",
    "max_offers_to_return = 3\n",
    "\n",
    "# train the collaborative filtering using the training set, \n",
    "# apply the patterns learned in the training set to the scoring set\n",
    "get_recommended_offers(df = train,\n",
    "                        id_cols = id_cols,\n",
    "                        prod_mix_cols = prod_mix_cols,\n",
    "                        ban = ban,\n",
    "                        ref_dt = ref_dt,\n",
    "                        distance_func = distance_func,\n",
    "                        n = n,\n",
    "                        minimal_threshold = 0.10,\n",
    "                        max_offers_to_return = 3\n",
    "                        )\n",
    "\n",
    "product_type, count(*) from df group by product_type \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b0e012-3a7d-4bfc-8f8f-0ee7410e78ef",
   "metadata": {},
   "source": [
    "### 2. create get_recommended_offers_multiple (get_recommended_offers with bootstrapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2522e42-790b-457d-835e-e03490a5d946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommended_offers_multiple(df: pd.DataFrame, \n",
    "                                    id_cols: list,\n",
    "                                    prod_mix_cols: list, \n",
    "                                    ban: int, \n",
    "                                    ref_dt, \n",
    "                                    distance_func: list,\n",
    "                                    n_values: list,\n",
    "                                    minimal_threshold: float,\n",
    "                                    max_offers_to_return: int\n",
    "                                    ):\n",
    "    \"\"\"\n",
    "    Given a dataframe, a ban, ref_dt, n values, and distance functions,\n",
    "    run multiple iterations of the get_recommended_offers function with different parameter combinations,\n",
    "    and return the top 3 most common answers among those.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for n in n_values:\n",
    "        for distance_func in distance_funcs:\n",
    "            result = get_recommended_offers(df = df, id_cols = id_cols, prod_mix_cols = prod_mix_cols,\n",
    "                                            ban = ban, ref_dt = ref_dt, distance_func = distance_func,\n",
    "                                            n = n, minimal_threshold = 0.10, max_offers_to_return = 3)\n",
    "\n",
    "            results.append(result)\n",
    "            \n",
    "            # concatenate the arrays together\n",
    "            concatenated_array = np.concatenate(results)\n",
    "            \n",
    "            # convert the concatenated array to a Python list\n",
    "            result_list = list(concatenated_array)\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        result_counts = pd.Series(result_list).value_counts()\n",
    "\n",
    "        most_common_result = [result_counts.index[0],result_counts.index[1],result_counts.index[2]]\n",
    "        \n",
    "        return most_common_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd72935-7913-49cb-916c-6ae7519c5b60",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df\n",
    "id_cols = id_cols\n",
    "prod_mix_cols = prod_mix_cols\n",
    "ban = ban \n",
    "ref_dt = ref_dt\n",
    "distance_funcs = ['euclidean', 'manhattan', 'cosine']\n",
    "n_values = [500, 1000, 2000, 5000, 10000] \n",
    "minimal_threshold = 0.10\n",
    "max_offers_to_return = 3\n",
    "\n",
    "results = []\n",
    "\n",
    "for n in n_values:\n",
    "    \n",
    "    for distance_func in distance_funcs:\n",
    "        result = get_recommended_offers_multiple(df = df, id_cols = id_cols, prod_mix_cols = prod_mix_cols,\n",
    "                                        ban = ban, ref_dt = ref_dt, distance_func = distance_func,\n",
    "                                        n = n, minimal_threshold = 0.10, max_offers_to_return = 3)\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "        # concatenate the arrays together\n",
    "        concatenated_array = np.concatenate(results)\n",
    "\n",
    "        # convert the concatenated array to a Python list\n",
    "        result_list = list(concatenated_array)\n",
    "        print(result_list)\n",
    "\n",
    "if len(results) == 0:\n",
    "    \n",
    "    print(\" \")\n",
    "\n",
    "else:\n",
    "    result_counts = pd.Series(result_list).value_counts()\n",
    "    \n",
    "    print(result_counts)\n",
    "\n",
    "    most_common_result = [result_counts.index[0],result_counts.index[1],result_counts.index[2]]\n",
    "\n",
    "    print(most_common_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb27a1cd-f385-4428-9cef-233fbe538341",
   "metadata": {},
   "source": [
    "### 3. apply the function to the entire scoring set (without bootstrapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94542dd-5aba-4278-b262-58d778aa6826",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train\n",
    "# prod_mix_cols: internet, tv, tv, sing, smhm, tos, wifi, whsia\n",
    "prod_mix_cols = ['prod_hsic_cnt','prod_ttv_cnt','prod_stv_cnt','prod_sing_cnt','prod_smhm_cnt','prod_tos_cnt','prod_wifiplus_cnt','prod_whsia_cnt']\n",
    "id_cols=['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id', 'target']\n",
    "distance_func = 'cosine'\n",
    "n = 500\n",
    "minimal_threshold = 0.10\n",
    "max_offers_to_return = 3\n",
    "\n",
    "def production_model (df, train, test, id_cols, prod_mix_cols, distance_func, n, minimal_threshold, max_offers_to_return):\n",
    "    \n",
    "    frame = pd.DataFrame()\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # For each customer in each month\n",
    "    for cust in list(df['ban'].unique()):\n",
    "        i += 1\n",
    "        if i % 50 == 0: \n",
    "            print(f'{i} customers processed')\n",
    "        \n",
    "        for dt in list(df[df['ban']==cust]['ref_dt'].unique()):\n",
    "            #This part of the code adds the line we want to get offers to the training set, so we can use the distance formula\n",
    "            data = pd.DataFrame()\n",
    "            data = train.append(test[(test['ban']==cust)&(test['ref_dt']==dt)])\n",
    "            data= data.reset_index()\n",
    "\n",
    "            results = get_recommended_offers(data, id_cols, prod_mix_cols, cust, dt, distance_func, n, minimal_threshold=minimal_threshold, max_offers_to_return=max_offers_to_return)\n",
    "            data = {'ban': [cust],\n",
    "                  'dt': [dt],\n",
    "                  'offers': [results]}\n",
    "            \n",
    "            frame1 =  pd.DataFrame(data)\n",
    "            frame = frame.append(frame1)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "frame_production_100_samples = production_model(test.head(2000), train, test, id_cols, prod_mix_cols, distance_func, n, minimal_threshold, max_offers_to_return)\n",
    "frame_production_100_samples.to_csv('gs://divg-groovyhoon-pr-d2eab4-default/downloads/offer_recommendation_without_bootstap_2000.csv',index=False)\n",
    "print(f'csv saved in gs://divg-groovyhoon-pr-d2eab4-default/downloads/offer_recommendation_without_bootstap_2000.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f8c7c8-06b4-4d57-9d1e-5bf86ff9c234",
   "metadata": {},
   "source": [
    "### 4. apply the function to the entire scoring set (with bootstrapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef04a93-c294-4beb-aef6-5ef5fb91164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = train\n",
    "# prod_mix_cols: internet, tv, tv, sing, smhm, tos, wifi, whsia\n",
    "id_cols=['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id', 'target']\n",
    "prod_mix_cols = ['prod_hsic_cnt','prod_ttv_cnt','prod_stv_cnt','prod_sing_cnt','prod_smhm_cnt','prod_tos_cnt','prod_wifiplus_cnt','prod_whsia_cnt']\n",
    "distance_funcs = ['euclidean', 'manhattan', 'cosine']\n",
    "n_values = [500, 1000, 2000, 5000, 10000] \n",
    "minimal_threshold = 0.10\n",
    "max_offers_to_return = 3\n",
    "\n",
    "def production_model (df, train, test, id_cols, prod_mix_cols, distance_func, n, minimal_threshold, max_offers_to_return):\n",
    "    \n",
    "    frame = pd.DataFrame()\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # For each customer in each month\n",
    "    for cust in list(df['ban'].unique()):\n",
    "        i += 1\n",
    "        if i % 50 == 0: \n",
    "            print(f'{i} customers processed')\n",
    "        \n",
    "        for dt in list(df[df['ban']==cust]['ref_dt'].unique()):\n",
    "            #This part of the code adds the line we want to get offers to the training set, so we can use the distance formula\n",
    "            data = pd.DataFrame()\n",
    "            data = train.append(test[(test['ban']==cust)&(test['ref_dt']==dt)])\n",
    "            data= data.reset_index()\n",
    "\n",
    "            results = get_recommended_offers_multiple(data, id_cols, prod_mix_cols, cust, dt, distance_funcs, n_values, minimal_threshold=minimal_threshold, max_offers_to_return=max_offers_to_return)\n",
    "            data = {'ban': [cust],\n",
    "                  'dt': [dt],\n",
    "                  'offers': [results]}\n",
    "            \n",
    "            frame1 =  pd.DataFrame(data)\n",
    "            frame = frame.append(frame1)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "frame_production_100_samples = production_model(test.head(1000), train, test, id_cols, prod_mix_cols, distance_func, n, minimal_threshold, max_offers_to_return)\n",
    "frame_production_100_samples.to_csv('gs://divg-groovyhoon-pr-d2eab4-default/downloads/offer_recommendation_with_bootstap_1000.csv',index=False)\n",
    "print(f'csv saved in gs://divg-groovyhoon-pr-d2eab4-default/downloads/offer_recommendation_without_bootstap_1000.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d660e799-b94e-4388-b297-49be55ad7376",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['ban'] == 208671688].columns\n",
    "\n",
    "'hsic_acquisition': 0,\n",
    "'ttv_acquisition': 1,\n",
    "'sing_acquisition': 2,\n",
    "'shs_acquisition': 3, \n",
    "'tos_acquisition': 4, \n",
    "'wifi_acquisition': 5, \n",
    "'lwc_acquisition': 6, \n",
    "'sws_acquisition': 7, \n",
    "'hpro_acquisition': 8\n",
    "\n",
    "'prod_hsic_cnt'\n",
    "'prod_ttv_cnt'\n",
    "'prod_stv_cnt'\n",
    "'prod_sing_cnt'\n",
    "'prod_smhm_cnt'\n",
    "'prod_tos_cnt'\n",
    "'prod_wifiplus_cnt'\n",
    "# we need lwc and sws indicator or product count\n",
    "'prod_whsia_cnt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602c8e18-1de5-46ee-b647-f017395b3bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommended_offers (df:pd.DataFrame, \n",
    "                            df_id:pd.DataFrame,\n",
    "                            customer_id:str,\n",
    "                            month:int,\n",
    "                            distance_func:str,\n",
    "                            n,\n",
    "                            minimal_threshold:float,\n",
    "                            max_offers_to_return:int\n",
    "                            ):\n",
    "    \"\"\"\n",
    "    This function takes as parameters:\n",
    "    1. the dataframe where we'll be getting our data\n",
    "    2. the customer identifiers Customer Id and the Month we want to make an offer for\n",
    "    3. the distance function we want to use to calculate similaries between customers (see explanation below on how to choose it)\n",
    "    4. The number of other customers we want to base our recommendations on\n",
    "    5. The minimal threshold of prevalence of a given offer, in the similar group of customers, for it to be considered for recommendation (see explanation below on how to chose it)\n",
    "    \n",
    "    It returns:\n",
    "    An array with the list of offers that we could recommend to this customer\n",
    "    \"\"\"\n",
    "\n",
    "    # extract the feature vectors of all customers\n",
    "    features = list(df.columns.difference(['Customer ID','Month','Month of Joining','offer']))\n",
    "    X = df[features].values\n",
    "\n",
    "    # extract the feature vector of the given customer\n",
    "    index = df[(df['Customer ID'] == customer_id) & (df['Month']==month)].index[0]\n",
    "    x = X[index]\n",
    "\n",
    "    # compute the distances between the feature vectors\n",
    "    if distance_func == 'euclidean':\n",
    "      distances = euclidean_distances(X, x.reshape(1, -1)).flatten()\n",
    "    elif distance_func == 'manhattan':\n",
    "      distances = manhattan_distances(X, x.reshape(1, -1)).flatten()\n",
    "    elif distance_func == 'cosine':\n",
    "      distances = 1 - cosine_similarity(X, x.reshape(1, -1)).flatten()\n",
    "    else:\n",
    "      raise ValueError('Invalid distance function specified.')\n",
    "\n",
    "    # find the indices of the n customers with lowest distance\n",
    "    most_similar_indices = distances.argsort()[:n]\n",
    "            \n",
    "    # extract the customer data for the most similar customers\n",
    "    similar_customers = df.iloc[most_similar_indices]\n",
    "\n",
    "    # merge with the id dataframe to select only the customers who did not churn\n",
    "    similar_customers = pd.merge(similar_customers,df_id[['Customer ID','Month of Joining','Month','Churn Value']],on=['Customer ID','Month of Joining','Month','Churn Value'])\n",
    "\n",
    "    # select the customers that did not churn\n",
    "    similar_customers = similar_customers[similar_customers['Churn Value']==0]\n",
    "\n",
    "    #count the top offers of the non-churned customers\n",
    "    top_offers = similar_customers[['Customer ID','offer']].groupby(['offer']).agg({'Customer ID':'count'}).reset_index().sort_values(by = 'Customer ID', ascending = False)    \n",
    "    top_offers['perc_total'] = top_offers['Customer ID']/top_offers['Customer ID'].sum()\n",
    "    top_offers_min = top_offers[top_offers['perc_total']>minimal_threshold].head(max_offers_to_return)\n",
    "        \n",
    "    return top_offers_min['offer'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3bc683-2e76-4112-b493-0ca5c3903867",
   "metadata": {},
   "source": [
    "### create a function that checks for the customer's product intensity, then remove the product recommendations for products the customer already has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7666707-6ff8-425d-8170-5227e1e6d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_prod_reco(prod_mix, prod_reco_list): \n",
    "    \n",
    "    # This function accepts the customer's product mix and product recommenndations as input parameters, \n",
    "    # and filters out the product recommendations that the customer already has.\n",
    "    \n",
    "    # 1. check the customer's product intensity\n",
    "    cust_prod_list = []\n",
    "\n",
    "    if prod_mix[0] > 0: \n",
    "        cust_prod_list.append('hsic')\n",
    "    if prod_mix[1] + prod_mix[2] > 0: \n",
    "        cust_prod_list.append('ttv')\n",
    "    if prod_mix[3] > 0: \n",
    "        cust_prod_list.append('sing')\n",
    "    if prod_mix[4] > 0: \n",
    "        cust_prod_list.append('smhm')\n",
    "    if prod_mix[5] > 0: \n",
    "        cust_prod_list.append('tos')\n",
    "    if prod_mix[6] > 0: \n",
    "        cust_prod_list.append('wifi')\n",
    "    if prod_mix[7] > 0: \n",
    "        cust_prod_list.append('whsia')\n",
    "    ### add for lwc, sws, hrpo as product count labels become available ###\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('lwc')\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('sws')\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('hpro')\n",
    "\n",
    "    # 2. trim the prodduct reco list to remove \"_acquisition\" for comparison. \n",
    "    prod_reco_list = [value.replace('_acquisition', '') for value in prod_reco_list]\n",
    "\n",
    "    final_prod_reco_list = [item for item in prod_reco_list if item not in cust_prod_list]\n",
    "\n",
    "    return final_prod_reco_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189f467f-3d32-4a4e-8eb6-92b89de48265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. accept the customer's product intensity in the parameters as a list\n",
    "prod_mix = df[prod_mix_cols].values[index]\n",
    "prod_reco_list = list(top_offers['model_scenario'][:8])\n",
    "\n",
    "# 1. check the customer's product intensity\n",
    "cust_prod_list = []\n",
    "\n",
    "if prod_mix[0] > 0: \n",
    "    cust_prod_list.append('hsic')\n",
    "if prod_mix[1] + prod_mix[2] > 0: \n",
    "    cust_prod_list.append('ttv')\n",
    "if prod_mix[3] > 0: \n",
    "    cust_prod_list.append('sing')\n",
    "if prod_mix[4] > 0: \n",
    "    cust_prod_list.append('smhm')\n",
    "if prod_mix[5] > 0: \n",
    "    cust_prod_list.append('tos')\n",
    "if prod_mix[6] > 0: \n",
    "    cust_prod_list.append('wifi')\n",
    "if prod_mix[7] > 0: \n",
    "    cust_prod_list.append('whsia')\n",
    "    \n",
    "# add for lwc, sws, hrpo as product count labels become available\n",
    "\n",
    "# 2. accept the product recommendations in the parameters as a list \n",
    "prod_reco_list = list(top_offers['model_scenario'][:8])\n",
    "\n",
    "prod_reco_list = [value.replace('_acquisition', '') for value in prod_reco_list]\n",
    "\n",
    "final_prod_reco_list = [item for item in prod_reco_list if item not in cust_prod_list]\n",
    "\n",
    "print(final_prod_reco_list)\n",
    "\n",
    "# 3. remove the products that the customer already has \n",
    "\n",
    "\n",
    "# prod_mix_cols: internet, tv, tv, sing, smhm, tos, wifi, whsia\n",
    "\n",
    "# 'hsic_acquisition': 0,\n",
    "# 'ttv_acquisition': 1,\n",
    "# 'sing_acquisition': 2,\n",
    "# 'shs_acquisition': 3, \n",
    "# 'tos_acquisition': 4, \n",
    "# 'wifi_acquisition': 5, \n",
    "# 'lwc_acquisition': 6, \n",
    "# 'sws_acquisition': 7, \n",
    "# 'hpro_acquisition': 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fb3c25-0efe-4842-9dd2-53da180d7155",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# del df_train, df_val, df_test\n",
    "del df_train, df_test, df_val\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6eef83-a49d-4fb6-b2bb-088a7c6c8fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat_enc.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfd29e3-d1b3-4cbe-983c-b0033ed8554a",
   "metadata": {},
   "source": [
    "### fit training data in xgboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7616d7-5909-4cb8-8e74-821d3e92dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build model and fit in training data\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=250,\n",
    "    max_depth=7,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='multi:softproba',\n",
    "    num_class=3, \n",
    "    eval_metric='mlogloss', \n",
    "    nthread=4,\n",
    "    scale_pos_weight=1,\n",
    "    seed=27\n",
    ")\n",
    "\n",
    "# xgb_model = xgb.XGBClassifier(\n",
    "#     learning_rate=0.1,\n",
    "#     n_estimators=1000,\n",
    "#     max_depth=5,\n",
    "#     min_child_weight=1,\n",
    "#     gamma=0,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     objective='multi:softproba',\n",
    "#     num_class=3, \n",
    "#     eval_metric='mlogloss', \n",
    "#     nthread=4,\n",
    "#     scale_pos_weight=1,\n",
    "#     seed=27\n",
    "# )\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "print('xgb training done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5abc939-f3aa-408a-b3a6-0beb1ff89c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca769f9-add0-4dc1-b9ee-b773e64f1573",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_model.predict_proba(X_test, ntree_limit=xgb_model.best_iteration)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbafd6fe-c34b-451b-8d42-c8d4e80e9584",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_0 = y_pred[:, 0]\n",
    "y_pred_1 = y_pred[:, 1]\n",
    "y_pred_2 = y_pred[:, 2]\n",
    "y_pred_3 = y_pred[:, 3]\n",
    "y_pred_4 = y_pred[:, 4]\n",
    "y_pred_5 = y_pred[:, 5]\n",
    "y_pred_6 = y_pred[:, 6]\n",
    "y_pred_7 = y_pred[:, 7]\n",
    "y_pred_8 = y_pred[:, 8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42baa80-bee6-4aa7-80e3-2c341142c818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #predictions on X_test\n",
    "# y_pred = xgb_model.predict_proba(X_test, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "\n",
    "df_ban_test = ban_test\n",
    "df_test_exp = df_ban_test.join(X_test) \n",
    "df_test_exp['y_test'] = y_test\n",
    "df_test_exp['y_pred_proba_0'] = y_pred_0\n",
    "df_test_exp['y_pred_proba_1'] = y_pred_1\n",
    "df_test_exp['y_pred_proba_2'] = y_pred_2\n",
    "df_test_exp['y_pred_proba_3'] = y_pred_3\n",
    "df_test_exp['y_pred_proba_4'] = y_pred_4\n",
    "df_test_exp['y_pred_proba_5'] = y_pred_5\n",
    "df_test_exp['y_pred_proba_6'] = y_pred_6\n",
    "df_test_exp['y_pred_proba_7'] = y_pred_7\n",
    "df_test_exp['y_pred_proba_8'] = y_pred_8\n",
    "\n",
    "\n",
    "# df_test_exp['y_pred'] = (df_test_exp['y_pred_proba'] > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e194cad1-eba7-46ad-8052-692b08f32107",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_exp.to_csv(\"gs://divg-groovyhoon-pr-d2eab4-default/downloads/df_test_exp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2217c02c-846f-4bec-9b92-0819257a812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_model.predict_proba(X_val, ntree_limit=xgb_model.best_iteration)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81897c94-69be-4066-9613-8509dcaaafae",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_0 = y_pred[:, 0]\n",
    "y_pred_1 = y_pred[:, 1]\n",
    "y_pred_2 = y_pred[:, 2]\n",
    "y_pred_3 = y_pred[:, 3]\n",
    "y_pred_4 = y_pred[:, 4]\n",
    "y_pred_5 = y_pred[:, 5]\n",
    "y_pred_6 = y_pred[:, 6]\n",
    "y_pred_7 = y_pred[:, 7]\n",
    "y_pred_8 = y_pred[:, 8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291408bb-7da9-4100-a8f4-a6cc5bea02d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #predictions on X_val\n",
    "# y_pred = xgb_model.predict_proba(X_val, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "\n",
    "df_ban_val = ban_val\n",
    "df_val_exp = df_ban_val.join(X_val) \n",
    "df_val_exp['y_val'] = y_val\n",
    "df_val_exp['y_pred_proba_0'] = y_pred_0\n",
    "df_val_exp['y_pred_proba_1'] = y_pred_1\n",
    "df_val_exp['y_pred_proba_2'] = y_pred_2\n",
    "df_val_exp['y_pred_proba_3'] = y_pred_3\n",
    "df_val_exp['y_pred_proba_4'] = y_pred_4\n",
    "df_val_exp['y_pred_proba_5'] = y_pred_5\n",
    "df_val_exp['y_pred_proba_6'] = y_pred_6\n",
    "df_val_exp['y_pred_proba_7'] = y_pred_7\n",
    "df_val_exp['y_pred_proba_8'] = y_pred_8\n",
    "\n",
    "# df_test_exp['y_pred'] = (df_test_exp['y_pred_proba'] > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a79a59f-8a7c-42b9-81d2-06e4854ea513",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_exp.to_csv(\"gs://divg-groovyhoon-pr-d2eab4-default/downloads/df_val_exp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0bb1d3-4362-47c1-ac15-94c0b9299d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716cb25e-044f-4479-a78f-f825702ba5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create an XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=3)\n",
    "\n",
    "# Define the hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'n_estimators': [30, 50, 100, 200, 500, 1000],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'subsample': [0.8, 0.9],\n",
    "    'colsample_bytree': [0.8, 0.9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'eval_metric': ['mlogloss']\n",
    "}\n",
    "\n",
    "# Use GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on Test Set: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdabeac-b4ad-44d8-8dcf-9cd2dbc31c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_model.best_model(X_val)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1ab70b-10a5-402e-b9f6-4d857dcbe927",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_0 = y_pred[:, 0]\n",
    "y_pred_1 = y_pred[:, 1]\n",
    "y_pred_2 = y_pred[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccd0f27-b35d-415e-8f4c-3a0483e0b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #predictions on X_val\n",
    "# y_pred = xgb_model.predict_proba(X_val, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "\n",
    "df_ban_val = ban_val\n",
    "df_val_exp = df_ban_val.join(X_val) \n",
    "df_val_exp['y_val'] = y_val\n",
    "df_val_exp['y_pred_proba_0'] = y_pred_0\n",
    "df_val_exp['y_pred_proba_1'] = y_pred_1\n",
    "df_val_exp['y_pred_proba_2'] = y_pred_2\n",
    "\n",
    "# df_test_exp['y_pred'] = (df_test_exp['y_pred_proba'] > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1cb419-d530-4d37-83a1-19343b27cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_exp.to_csv(\"gs://divg-groovyhoon-pr-d2eab4-default/downloads/df_val_exp_v2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0ea7ec-74db-4419-84bb-0d44545fec56",
   "metadata": {},
   "source": [
    "### make predictions on X_test set, assign deciles to the predicted values, and save in df_test_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3c55fb-754b-4fa9-b0ac-cdcf45a3dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions on X_test\n",
    "y_pred = xgb_model.predict_proba(X_test, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "\n",
    "#join ban_test, X_test, y_test and y_pred and print to csv\n",
    "#CHECK THE SIZE OF EACH COMPONENT BEFORE JOINING\n",
    "q=10\n",
    "df_ban_test = ban_test\n",
    "df_test_exp = df_ban_test.join(X_test) \n",
    "df_test_exp['y_test'] = y_test\n",
    "df_test_exp['y_pred_proba'] = y_pred\n",
    "df_test_exp['y_pred'] = (df_test_exp['y_pred_proba'] > 0.5).astype(int)\n",
    "df_test_exp['decile'] = pd.qcut(df_test_exp['y_pred_proba'], q, labels=[i for i in range(q, 0, -1)])\n",
    "\n",
    "lg = get_lift(y_pred, y_test, q)\n",
    "\n",
    "lg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2767b8f5-8bdb-4d5f-9b5b-2b340289cb94",
   "metadata": {
    "tags": []
   },
   "source": [
    "### export df_test_exp and lift scores to gcs bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2015d1-d1a5-410f-95c5-2b5054e2abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_exp.to_csv('gs://{}/{}/{}_df_test_exp.csv'.format(file_bucket, SERVICE_TYPE, SERVICE_TYPE, index=True))\n",
    "print(\"....df_test_exp exported\")\n",
    "\n",
    "lg.to_csv('gs://{}/{}/{}_lift_on_test_data.csv'.format(file_bucket, SERVICE_TYPE, SERVICE_TYPE, index=False))\n",
    "print(\"....lift_to_csv done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a67377-2467-47f9-aa71-2063716eda1f",
   "metadata": {},
   "source": [
    "### make predictions on X_val set, assign deciles to the predicted values, and save in df_val_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7231aba-49a5-4a16-9c57-2061ebe27892",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions on X_val\n",
    "y_pred = xgb_model.predict_proba(X_val, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "\n",
    "#join ban_val, X_val, y_val and y_pred and print to csv\n",
    "#CHECK THE SIZE OF EACH COMPONENT BEFORE JOINING\n",
    "q=10\n",
    "df_ban_val = ban_val\n",
    "df_val_exp = df_ban_val.join(X_val) \n",
    "df_val_exp['y_val'] = y_val\n",
    "df_val_exp['y_pred_proba'] = y_pred\n",
    "df_val_exp['y_pred'] = (df_val_exp['y_pred_proba'] > 0.5).astype(int)\n",
    "df_val_exp['decile'] = pd.qcut(df_val_exp['y_pred_proba'], q, labels=[i for i in range(q, 0, -1)])\n",
    "\n",
    "lg = get_lift(y_pred, y_val, q)\n",
    "\n",
    "lg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca9c17-8af8-4f29-b3a1-d35a1c50cf19",
   "metadata": {},
   "source": [
    "### export df_val_exp and lift scores to gcs bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570c3887-fc3a-44a7-b037-4bdfd61eb568",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_exp.to_csv('gs://{}/{}/{}_df_val_exp.csv'.format(file_bucket, SERVICE_TYPE, SERVICE_TYPE, index=True))\n",
    "print(\"....df_val_exp exported\")\n",
    "\n",
    "lg.to_csv('gs://{}/{}/{}_lift_on_val_data.csv'.format(file_bucket, SERVICE_TYPE, SERVICE_TYPE, index=False))\n",
    "print(\"....lift_to_csv done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8864bda6-bda0-4032-9704-f513da84e132",
   "metadata": {},
   "source": [
    "### get feature importances from xgboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43768ef0-d6cc-40e4-a3a9-dbcdbceedb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from xgboost model\n",
    "importances = xgb_model.feature_importances_\n",
    "\n",
    "# Get the index of importances from greatest importance to least\n",
    "sorted_index = np.argsort(importances)[::-1]\n",
    "x = range(len(importances))\n",
    "\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create tick labels \n",
    "labels = np.array(feature_names)[sorted_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb976c57-68c2-4d13-84c6-7f138ae560bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in sorted_index: \n",
    "    print(f'{feature_names[idx]}, {importances[idx]}', end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e9b69c-0ab4-4595-b2f2-3b0a6387d3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38edf5cf-4f19-41c3-ab79-3b560cdd1f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9209e72-72ba-4c08-a7c6-c8317836fcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(importances)[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f8515b-07cd-4c76-9d79-944145ef3de1",
   "metadata": {},
   "source": [
    "### Load results to BQ - WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514b9b52-17c6-4c7f-8a36-9afdfc17f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORE_TABLE_ID = 'bq_telus_12_months_churn_scores'\n",
    "\n",
    "project_id = PROJECT_ID \n",
    "dataset_id = DATASET_ID\n",
    "score_table_id = SCORE_TABLE_ID\n",
    "\n",
    "# get full score to cave into bucket\n",
    "pred_prob = xgb_model.predict_proba(X_val, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "result = pd.DataFrame(columns=['ban', 'subscriber_no', 'score_date', 'y_true', 'y_pred'])\n",
    "\n",
    "result['ban'] = list(ban_val['ban'])\n",
    "result['ban'] = result['ban'].astype('str')\n",
    "\n",
    "result['subscriber_no'] = list(ban_val['ban'])\n",
    "result['subscriber_no'] = result['subscriber_no'].astype('str')\n",
    "\n",
    "result['score_date'] = \"2023-11-04\"\n",
    "\n",
    "result['y_true'] = list(y_val)\n",
    "result['y_true'] = result['y_true'].fillna(0.0).astype('float64')\n",
    "\n",
    "result['y_pred'] = list(pred_prob)\n",
    "result['y_pred'] = result['y_pred'].fillna(0.0).astype('float64')\n",
    "\n",
    "############# updated up to here ############\n",
    "\n",
    "result.to_csv('gs://{}/{}/ucar/{}_prediction_v2.csv'.format(file_bucket, service_type, service_type), index=False)\n",
    "\n",
    "# define dtype_bq_mapping\n",
    "dtype_bq_mapping = {np.dtype('int64'): 'INTEGER', \n",
    "np.dtype('float64'):  'FLOAT', \n",
    "np.dtype('float32'):  'FLOAT', \n",
    "np.dtype('object'):  'STRING', \n",
    "np.dtype('bool'):  'BOOLEAN', \n",
    "np.dtype('datetime64[ns]'):  'DATE', \n",
    "pd.Int64Dtype(): 'INTEGER'} \n",
    "\n",
    "# export df_final to bigquery \n",
    "schema_list = [] \n",
    "for column in result.columns: \n",
    "    schema_list.append(bigquery.SchemaField(column, dtype_bq_mapping[result.dtypes[column]], mode='NULLABLE')) \n",
    "print(schema_list) \n",
    "\n",
    "dest_table = f'{project_id}.{dataset_id}.{score_table_id}'\n",
    "\n",
    "# Sending to bigquery \n",
    "client = get_gcp_bqclient(project_id)\n",
    "job_config = bigquery.LoadJobConfig(schema=schema_list, write_disposition='WRITE_TRUNCATE') \n",
    "job = client.load_table_from_dataframe(result, dest_table, job_config=job_config) \n",
    "job.result() \n",
    "\n",
    "table = client.get_table(dest_table) # Make an API request \n",
    "print(\"Loaded {} rows and {} columns to {}\".format(table.num_rows, len(table.schema), table_id)) \n",
    "\n",
    "time.sleep(60)\n",
    "\n",
    "# table_ref = f'{project_id}.{dataset_id}.{score_table}'\n",
    "# client = bigquery.Client(project=project_id)\n",
    "# table = client.get_table(table_ref)\n",
    "# schema = table.schema\n",
    "\n",
    "# ll = []\n",
    "# for item in schema:\n",
    "#     col = item.name\n",
    "#     d_type = item.field_type\n",
    "#     if 'float' in str(d_type).lower():\n",
    "#         d_type = 'FLOAT64'\n",
    "#     ll.append((col, d_type))\n",
    "\n",
    "#     if 'integer' in str(d_type).lower():\n",
    "#         result[col] = result[col].fillna(0).astype(int)\n",
    "#     if 'float' in str(d_type).lower():\n",
    "#         result[col] = result[col].fillna(0.0).astype(float)\n",
    "#     if 'string' in str(d_type).lower():\n",
    "#         result[col] = result[col].fillna('').astype(str)\n",
    "\n",
    "# table_ref = '{}.{}.{}'.format(project_id, dataset_id, temp_table)\n",
    "# client = bigquery.Client(project=project_id)\n",
    "# if if_tbl_exists(client, table_ref):\n",
    "#     client.delete_table(table_ref)\n",
    "\n",
    "# client.create_table(table_ref)\n",
    "# config = bigquery.LoadJobConfig(schema=schema)\n",
    "# config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "# bq_table_instance = client.load_table_from_dataframe(result, table_ref, job_config=config)\n",
    "# time.sleep(5)\n",
    "\n",
    "# drop_sql = f\"\"\"delete from `{project_id}.{dataset_id}.{score_table}` where score_date = '{score_date_dash}'\"\"\"  # .format(project_id, dataset_id, score_date_dash)\n",
    "# client.query(drop_sql)\n",
    "# #\n",
    "# load_sql = f\"\"\"insert into `{project_id}.{dataset_id}.{score_table}`\n",
    "#               select * from `{project_id}.{dataset_id}.{temp_table}`\"\"\"\n",
    "# client.query(load_sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bf0cd1-afbc-42e6-9a2f-fd8b727c2cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce20d37b-0e62-4c81-b446-a88539baf2d9",
   "metadata": {},
   "source": [
    "### save the model in gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46e0caa-94a8-497c-a395-15a1ac041f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model in GCS\n",
    "from datetime import datetime\n",
    "models_dict = {}\n",
    "create_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "models_dict['create_time'] = create_time\n",
    "models_dict['model'] = xgb_model\n",
    "models_dict['features'] = features\n",
    "\n",
    "with open('model_dict.pkl', 'wb') as handle:\n",
    "    pickle.dump(models_dict, handle)\n",
    "handle.close()\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(file_bucket)\n",
    "\n",
    "MODEL_PATH = '{}_xgb_models/'.format(service_type)\n",
    "blob = bucket.blob(MODEL_PATH)\n",
    "if not blob.exists(storage_client):\n",
    "    blob.upload_from_string('')\n",
    "\n",
    "model_name_onbkt = '{}{}_models_xgb_{}'.format(MODEL_PATH, service_type, models_dict['create_time'])\n",
    "blob = bucket.blob(model_name_onbkt)\n",
    "blob.upload_from_filename('model_dict.pkl')\n",
    "\n",
    "print(f\"....model loaded to GCS done at {str(create_time)}\")\n",
    "\n",
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c02b2b6-b059-4dde-9393-ad457eebf60e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3d9d01-f009-45c6-90fe-95a61cc169cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb40188a-3865-4993-be01-fb27f7d07dd6",
   "metadata": {},
   "source": [
    "### load the latest saved xgb_model to the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f240186-29e9-4d12-8c11-bb468cae8067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH = '{}_xgb_models/'.format(service_type)\n",
    "# df_score = pd.read_csv('gs://{}/{}_score.csv.gz'.format(file_bucket, service_type), compression='gzip')\n",
    "# df_score.dropna(subset=['ban'], inplace=True)\n",
    "# df_score.reset_index(drop=True, inplace=True)\n",
    "# print('......scoring data loaded:{}'.format(df_score.shape))\n",
    "# time.sleep(10)\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "MODEL_PATH = '{}_xgb_models/'.format(service_type)\n",
    "\n",
    "def load_model(file_bucket: str, service_type: str): \n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(file_bucket)\n",
    "    blobs = storage_client.list_blobs(file_bucket, prefix='{}{}_models_xgb_'.format(MODEL_PATH, service_type))\n",
    "\n",
    "    model_lists = []\n",
    "    for blob in blobs:\n",
    "        model_lists.append(blob.name)\n",
    "\n",
    "    blob = bucket.blob(model_lists[-1])\n",
    "    blob_in = blob.download_as_string()\n",
    "    model_dict = pickle.loads(blob_in)\n",
    "    xgb_model = model_dict['model']\n",
    "    features = model_dict['features']\n",
    "    print('...... model loaded')\n",
    "    time.sleep(10)\n",
    "    \n",
    "    return xgb_model, features\n",
    "\n",
    "xgb_model, features = load_model(file_bucket = FILE_BUCKET, service_type = SERVICE_TYPE) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625bbaf0-6a67-4151-8504-a187037aa4f8",
   "metadata": {},
   "source": [
    "### backup codes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ced94be-d4ec-4eb5-8aed-8199f09bdbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "    token = os.popen('gcloud auth print-access-token').read()\n",
    "    token = re.sub(f'\\n$', '', token)\n",
    "    credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    if use_local_credential:\n",
    "        bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "    return bq_client\n",
    "\n",
    "client = get_gcp_bqclient(project_id)\n",
    "\n",
    "#instantiate df_target_train and df_target_test\n",
    "sql_train = ''' SELECT * FROM `{}.{}.bq_tos_cross_sell_targets_q3` '''.format(project_id, dataset_id)\n",
    "df_target_train = client.query(sql_train).to_dataframe()\n",
    "df_target_train = df_target_train.loc[\n",
    "    df_target_train['YEAR_MONTH'] == \"2022-Q3\"] #'-'.join(score_date_dash.split('-')[:2])]  # score_date_dash = '2022-08-31'\n",
    "\n",
    "#set up df_train and df_test (add 'target')\n",
    "df_target_train['ban'] = df_target_train['ban'].astype('int64')\n",
    "df_target_train = df_target_train.groupby('ban').tail(1)\n",
    "\n",
    "df_train = df_train.merge(df_target_train[['ban', 'product_crosssell_ind']], on='ban', how='left')\n",
    "df_train.rename(columns={'product_crosssell_ind': 'target'}, inplace=True)\n",
    "df_train.dropna(subset=['target'], inplace=True)\n",
    "df_train['target'] = df_train['target'].astype(int)\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32473d2-a75d-48ff-afc1-f34af20e2cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b9b389-df11-402a-ab09-20bd55af3d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "    token = os.popen('gcloud auth print-access-token').read()\n",
    "    token = re.sub(f'\\n$', '', token)\n",
    "    credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    if use_local_credential:\n",
    "        bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "    return bq_client\n",
    "\n",
    "client = get_gcp_bqclient(project_id)\n",
    "sql_test = ''' SELECT * FROM `{}.{}.bq_tos_cross_sell_targets_q4` '''.format(project_id, dataset_id)\n",
    "df_target_test = client.query(sql_test).to_dataframe()\n",
    "df_target_test = df_target_test.loc[\n",
    "    df_target_test['YEAR_MONTH'] == \"2022-Q4\"] #'-'.join(score_date_val_dash.split('-')[:2])]  # score_date_dash = '2022-09-30'\n",
    "\n",
    "#set up df_train and df_test (add 'target')\n",
    "df_target_test['ban'] = df_target_test['ban'].astype('int64')\n",
    "df_target_test = df_target_test.groupby('ban').tail(1)\n",
    "\n",
    "df_test = df_test.merge(df_target_test[['ban', 'product_crosssell_ind']], on='ban', how='left')\n",
    "df_test.rename(columns={'product_crosssell_ind': 'target'}, inplace=True)\n",
    "df_test.dropna(subset=['target'], inplace=True)\n",
    "df_test['target'] = df_test['target'].astype(int)\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62170f7-10b3-47f6-9610-7d758e68b6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_test['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426e602c-d3dd-4748-9617-410bc68b193e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
