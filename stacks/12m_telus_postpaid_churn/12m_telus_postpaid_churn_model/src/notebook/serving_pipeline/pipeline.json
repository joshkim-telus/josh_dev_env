{
  "pipelineSpec": {
    "components": {
      "comp-batch-prediction": {
        "executorLabel": "exec-batch-prediction",
        "inputDefinitions": {
          "parameters": {
            "dataset_id": {
              "type": "STRING"
            },
            "file_bucket": {
              "type": "STRING"
            },
            "model_uri": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "save_data_path": {
              "type": "STRING"
            },
            "score_date_dash": {
              "type": "STRING"
            },
            "score_table": {
              "type": "STRING"
            },
            "service_type": {
              "type": "STRING"
            },
            "table_id": {
              "type": "STRING"
            },
            "temp_table": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "metricsc": {
              "artifactType": {
                "schemaTitle": "system.ClassificationMetrics",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-bq-create-dataset": {
        "executorLabel": "exec-bq-create-dataset",
        "inputDefinitions": {
          "parameters": {
            "dataset_id": {
              "type": "STRING"
            },
            "environment": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "region": {
              "type": "STRING"
            },
            "score_date": {
              "type": "STRING"
            },
            "score_date_delta": {
              "type": "INT"
            },
            "token": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "parameters": {
            "col_list": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-load-ml-model": {
        "executorLabel": "exec-load-ml-model",
        "inputDefinitions": {
          "parameters": {
            "model_name": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "region": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "model": {
              "artifactType": {
                "schemaTitle": "system.Artifact",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "model_uri": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-postprocess": {
        "executorLabel": "exec-postprocess",
        "inputDefinitions": {
          "parameters": {
            "dataset_id": {
              "type": "STRING"
            },
            "file_bucket": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "score_date_dash": {
              "type": "STRING"
            },
            "service_type": {
              "type": "STRING"
            },
            "temp_table": {
              "type": "STRING"
            },
            "token": {
              "type": "STRING"
            },
            "ucar_score_table": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-preprocess": {
        "executorLabel": "exec-preprocess",
        "inputDefinitions": {
          "parameters": {
            "dataset_id": {
              "type": "STRING"
            },
            "pipeline_dataset": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "save_data_path": {
              "type": "STRING"
            },
            "score_date_dash": {
              "type": "STRING"
            }
          }
        }
      }
    },
    "deploymentSpec": {
      "executors": {
        "exec-batch-prediction": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "batch_prediction"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.18' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef batch_prediction(project_id: str\n                     , dataset_id: str\n                     , table_id: str\n                     , file_bucket: str\n                     , save_data_path: str\n                     , service_type: str\n                     , score_table: str\n                     , score_date_dash: str\n                     , temp_table: str\n                     , metrics: Output[Metrics]\n                     , metricsc: Output[ClassificationMetrics]\n                     , model_uri: str\n                     ):\n\n    import time\n    import pandas as pd\n    import numpy as np\n    import pickle\n    from datetime import date\n    from datetime import datetime\n    from dateutil.relativedelta import relativedelta\n    from google.cloud import bigquery\n    from google.cloud import storage\n\n    MODEL_ID = '5220'\n\n    def if_tbl_exists(bq_client, table_ref):\n        from google.cloud.exceptions import NotFound\n        try:\n            bq_client.get_table(table_ref)\n            return True\n        except NotFound:\n            return False\n\n    def upsert_table(project_id, dataset_id, table_id, sql, result):\n        new_values = ',\\n'.join(result.apply(lambda row: row_format(row), axis=1))\n        new_sql = sql.format(proj_id=project_id, dataset_id=dataset_id, table_id=table_id,\n                             new_values=new_values)\n        bq_client = bigquery.Client(project=project_id)\n        code = bq_client.query(new_sql)\n        time.sleep(5)\n\n    def row_format(row):\n        values = row.values\n        new_values = \"\"\n        v = str(values[0]) if not pd.isnull(values[0]) else 'NULL'\n        if 'str' in str(type(values[0])):\n            new_values += f\"'{v}'\"\n        else:\n            new_values += f\"{v}\"\n\n        for i in range(1, len(values)):\n            v = str(values[i]) if not pd.isnull(values[i]) else 'NULL'\n            if 'str' in str(type(values[i])):\n                new_values += f\",'{v}'\"\n            else:\n                new_values += f\",{v}\"\n        return '(' + new_values + ')'\n\n    def generate_sql_file(ll):\n        s = 'MERGE INTO `{proj_id}.{dataset_id}.{table_id}` a'\n        s += \" USING UNNEST(\"\n        s += \"[struct<\"\n        for i in range(len(ll) - 1):\n            v = ll[i]\n            s += \"{} {},\".format(v[0], v[1])\n        s += \"{} {}\".format(ll[-1][0], ll[-1][1])\n        s += \">{new_values}]\"\n        s += \") b\"\n        s += \" ON a.ban = b.ban and a.score_date = b.score_date\"\n        s += \" WHEN MATCHED THEN\"\n        s += \" UPDATE SET \"\n        s += \"a.{}=b.{},\".format(ll[0][0], ll[0][0])\n        for i in range(1, len(ll) - 1):\n            v = ll[i]\n            s += \"a.{}=b.{},\".format(v[0], v[0])\n        s += \"a.{}=b.{}\".format(ll[-1][0], ll[-1][0])\n        s += \" WHEN NOT MATCHED THEN\"\n        s += \" INSERT(\"\n        for i in range(len(ll) - 1):\n            v = ll[i]\n            s += \"{},\".format(v[0])\n        s += \"{})\".format(ll[-1][0])\n        s += \" VALUES(\"\n        for i in range(len(ll) - 1):\n            s += \"b.{},\".format(ll[i][0])\n        s += \"b.{}\".format(ll[-1][0])\n        s += \")\"\n\n        return s\n\n    def right(s, amount):\n        return s[-amount:]\n\n    # MODEL_PATH = '{}_xgb_models/'.format(service_type)\n    MODEL_PATH = right(model_uri, (len(model_uri) - 6 - len(file_bucket)))\n    df_score = pd.read_csv(save_data_path)\n    print(f'save_data_path: {save_data_path}')\n    df_score.dropna(subset=['ban'], inplace=True)\n    df_score.reset_index(drop=True, inplace=True)\n    print('......scoring data loaded:{}'.format(df_score.shape))\n    time.sleep(10)\n\n    # save backups\n    create_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    df_score.to_csv('gs://{}/{}/backup/{}_score_{}.csv'.format(file_bucket, service_type, service_type, create_time))\n\n    # load model to the notebook\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(file_bucket)\n    blobs = storage_client.list_blobs(file_bucket, prefix=MODEL_PATH)\n\n    model_lists = []\n    for blob in blobs:\n        model_lists.append(blob.name)\n\n    blob = bucket.blob(model_lists[-1])\n    blob_in = blob.download_as_string()\n    model_dict = pickle.loads(blob_in)\n    model_xgb = model_dict['model']\n    features = model_dict['features']\n    print('...... model loaded')\n    time.sleep(10)\n\n    ll = [('ban', 'string'), ('score_date', 'string'), ('model_id', 'string'), ('score', 'float64')]\n    sql = generate_sql_file(ll)\n\n    df_score['ban'] = df_score['ban'].astype(int)\n    print('.... scoring for {} FFH bans base'.format(len(df_score)))\n\n    # get full score to cave into bucket\n    pred_prob = model_xgb.predict_proba(df_score[features], ntree_limit=model_xgb.best_iteration)[:, 1]\n    result = pd.DataFrame(columns=['ban', 'score_date', 'model_id', 'score'])\n    result['score'] = list(pred_prob)\n    result['score'] = result['score'].fillna(0.0).astype('float64')\n    result['ban'] = list(df_score['ban'])\n    result['ban'] = result['ban'].astype('str')\n    result['score_date'] = score_date_dash\n    result['model_id'] = MODEL_ID\n\n    result.to_csv('gs://{}/ucar/{}_prediction.csv'.format(file_bucket, service_type), index=False)\n\n    # define df_final\n    df_final = df_score[features]\n\n    # define dtype_bq_mapping\n    dtype_bq_mapping = {np.dtype('int64'): 'INTEGER', \n    np.dtype('float64'):  'FLOAT', \n    np.dtype('float32'):  'FLOAT', \n    np.dtype('object'):  'STRING', \n    np.dtype('bool'):  'BOOLEAN', \n    np.dtype('datetime64[ns]'):  'DATE', \n    pd.Int64Dtype(): 'INTEGER'} \n\n    # export df_final to bigquery \n    schema_list = [] \n    for column in df_final.columns: \n        schema_list.append(bigquery.SchemaField(column, dtype_bq_mapping[df_final.dtypes[column]], mode='NULLABLE')) \n    print(schema_list) \n\n    dest_table = f'{dataset_id}.{table_id}' # 'churn_12_months.bq_c12m_serving_dataset_preprocessed'\n\n    # Sending to bigquery \n    client = bigquery.Client(project=project_id)\n    job_config = bigquery.LoadJobConfig(schema=schema_list, write_disposition='WRITE_TRUNCATE') \n    job = client.load_table_from_dataframe(df_final, dest_table, job_config=job_config) \n    job.result() \n    table = client.get_table(dest_table) # Make an API request \n    print(\"Loaded {} rows and {} columns to {}\".format(table.num_rows, len(table.schema), table_id)) \n\n    time.sleep(60)\n\n    table_ref = f'{project_id}.{dataset_id}.{score_table}'\n    client = bigquery.Client(project=project_id)\n    table = client.get_table(table_ref)\n    schema = table.schema\n\n    ll = []\n    for item in schema:\n        col = item.name\n        d_type = item.field_type\n        if 'float' in str(d_type).lower():\n            d_type = 'FLOAT64'\n        ll.append((col, d_type))\n\n        if 'integer' in str(d_type).lower():\n            result[col] = result[col].fillna(0).astype(int)\n        if 'float' in str(d_type).lower():\n            result[col] = result[col].fillna(0.0).astype(float)\n        if 'string' in str(d_type).lower():\n            result[col] = result[col].fillna('').astype(str)\n\n    table_ref = '{}.{}.{}'.format(project_id, dataset_id, temp_table)\n    client = bigquery.Client(project=project_id)\n    if if_tbl_exists(client, table_ref):\n        client.delete_table(table_ref)\n\n    client.create_table(table_ref)\n    config = bigquery.LoadJobConfig(schema=schema)\n    config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n    bq_table_instance = client.load_table_from_dataframe(result, table_ref, job_config=config)\n    time.sleep(20)\n\n    drop_sql = f\"\"\"delete from `{project_id}.{dataset_id}.{score_table}` where score_date = '{score_date_dash}'\"\"\"  # .format(project_id, dataset_id, score_date_dash)\n    client.query(drop_sql)\n    #\n    load_sql = f\"\"\"insert into `{project_id}.{dataset_id}.{score_table}`\n                  select * from `{project_id}.{dataset_id}.{temp_table}`\"\"\"\n    client.query(load_sql)\n\n"
            ],
            "image": "northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-pycaret-slim:latest",
            "resources": {
              "cpuLimit": 4.0,
              "memoryLimit": 32.0
            }
          }
        },
        "exec-bq-create-dataset": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "bq_create_dataset"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.18' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef bq_create_dataset(score_date: str\n                      , score_date_delta: int\n                      , project_id: str\n                      , dataset_id: str\n                      , region: str\n                      , environment: str\n                      , token: str\n                      ) -> NamedTuple(\"output\", [(\"col_list\", list)]):\n\n    from google.cloud import bigquery\n    import logging \n    from datetime import datetime\n\n    #### For wb\n    import google.oauth2.credentials\n    CREDENTIALS = google.oauth2.credentials.Credentials(token)\n\n    client = bigquery.Client(project=project_id, credentials=CREDENTIALS)\n    job_config = bigquery.QueryJobConfig()\n\n#     #### For prod \n#     client = bigquery.Client(project=project_id)\n#     job_config = bigquery.QueryJobConfig()\n\n    # Change dataset / table + sp table name to version in bi-layer\n    query =\\\n        f'''\n            DECLARE score_date DATE DEFAULT \"{score_date}\";\n\n            -- Change dataset / sp name to the version in the bi_layer\n            CALL {dataset_id}.bq_sp_tpc_{environment}_dataset(score_date);\n\n            SELECT\n                *\n            FROM {dataset_id}.INFORMATION_SCHEMA.PARTITIONS\n            WHERE table_name='bq_tpc_{environment}_dataset'\n        '''\n\n    df = client.query(query, job_config=job_config).to_dataframe()\n    logging.info(df.to_string())\n\n    logging.info(f\"Loaded {df.total_rows[0]} rows into \\\n             {df.table_catalog[0]}.{df.table_schema[0]}.{df.table_name[0]} on \\\n             {datetime.strftime((df.last_modified_time[0]), '%Y-%m-%d %H:%M:%S') } !\")\n\n    ######################### Save column list_##########################\n    query =\\\n        f'''\n           SELECT\n                *\n            FROM {dataset_id}.bq_tpc_{environment}_dataset\n            LIMIT 1\n        '''\n\n    df = client.query(query, job_config=job_config).to_dataframe()\n\n    col_list = list([col for col in df.columns])\n\n    return (col_list,)\n\n"
            ],
            "image": "northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-pycaret-slim:latest"
          }
        },
        "exec-load-ml-model": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "load_ml_model"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.18' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef load_ml_model(project_id: str\n                  , region: str\n                  , model_name: str\n                  , model: Output[Artifact]\n                  ) -> NamedTuple(\"output\", [(\"model_uri\", str)]):\n\n    from google.cloud import aiplatform\n\n    # List models with the given display name, order by update time\n    models = aiplatform.Model.list(\n        filter=f'display_name={model_name}', \n        order_by=\"update_time\",\n        location=region)\n\n    if not models:\n        raise ValueError(f\"No model found with display name: {model_name}\")\n\n    # Get the latest model's resource name\n    latest_model = models[-1]\n    model_uri = latest_model.resource_name\n\n    # Update the model URI and metadata\n    model.uri = model_uri\n    model.metadata['resourceName'] = model_uri\n    env_var = latest_model.to_dict()['containerSpec']['env'][1]['value']\n\n    # Return the model URI as part of the output\n    return (env_var,)\n\n"
            ],
            "image": "northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-load-model-slim:1.0.0",
            "resources": {
              "cpuLimit": 4.0,
              "memoryLimit": 32.0
            }
          }
        },
        "exec-postprocess": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "postprocess"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.18' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef postprocess(\n        project_id: str,\n        file_bucket: str,\n        dataset_id: str,\n        service_type: str,\n        score_date_dash: str,\n        temp_table: str, \n        ucar_score_table: str,\n        token: str\n):\n    import time\n    from datetime import date\n    from dateutil.relativedelta import relativedelta\n    import pandas as pd\n    from google.cloud import bigquery\n\n    def if_tbl_exists(client, table_ref):\n        from google.cloud.exceptions import NotFound\n        try:\n            client.get_table(table_ref)\n            return True\n        except NotFound:\n            return False\n\n    MODEL_ID = '5220'\n    file_name = 'gs://{}/ucar/{}_prediction.csv'.format(file_bucket, service_type)\n    df_orig = pd.read_csv(file_name, index_col=False)\n    df_orig.dropna(subset=['ban'], inplace=True)\n    df_orig.reset_index(drop=True, inplace=True)\n    df_orig['scoring_date'] = score_date_dash\n    df_orig.ban = df_orig.ban.astype(int)\n    df_orig = df_orig.rename(columns={'ban': 'bus_bacct_num', 'score': 'score_num'})\n    df_orig.score_num = df_orig.score_num.astype(float)\n    df_orig['decile_grp_num'] = pd.qcut(df_orig['score_num'], q=10, labels=[i for i in range(10, 0, -1)])\n    df_orig.decile_grp_num = df_orig.decile_grp_num.astype(int)\n    df_orig['percentile_pct'] = (1 - df_orig.score_num.rank(pct=True))*100\n    df_orig['percentile_pct'] = df_orig['percentile_pct'].apply(round, 0).astype(int)\n    df_orig['predict_model_nm'] = 'FFH CHURN 12 MONTHS Model - DIVG'\n    df_orig['model_type_cd'] = 'FFH'\n    df_orig['subscriber_no'] = \"\"\n    df_orig['prod_instnc_resrc_str'] = \"\"\n    df_orig['service_instnc_id'] = \"\"\n    df_orig['segment_nm'] = \"\"\n    df_orig['segment_id'] = \"\"\n    df_orig['classn_nm'] = \"\"\n    df_orig['predict_model_id'] = MODEL_ID\n    df_orig.drop(columns=['model_id', 'score_date'], axis=1, inplace=True)\n\n    get_cust_id = \"\"\"\n    WITH bq_snpsht_max_date AS(\n    SELECT PARSE_DATE('%Y%m%d', MAX(partition_id)) AS max_date\n        FROM `cio-datahub-enterprise-pr-183a.ent_cust_cust.INFORMATION_SCHEMA.PARTITIONS` \n    WHERE table_name = 'bq_prod_instnc_snpsht' \n        AND partition_id <> '__NULL__'\n    ),\n    -- BANs can have multiple Cust ID. Create rank by product type and status, prioritizing ban/cust id with active FFH products\n    rank_prod_type AS (\n    SELECT DISTINCT\n        bacct_bus_bacct_num,\n        consldt_cust_bus_cust_id AS cust_id,\n        CASE WHEN pi_prod_instnc_resrc_typ_cd IN ('SING', 'HSIC', 'TTV', 'SMHM', 'STV', 'DIIC') AND pi_prod_instnc_stat_cd = 'A' THEN 1\n                WHEN pi_prod_instnc_resrc_typ_cd IN ('SING', 'HSIC', 'TTV', 'SMHM', 'STV', 'DIIC') THEN 2\n                WHEN pi_prod_instnc_stat_cd = 'A' THEN 3\n                ELSE 4\n                END AS prod_rank\n    FROM `cio-datahub-enterprise-pr-183a.ent_cust_cust.bq_prod_instnc_snpsht`\n    CROSS JOIN bq_snpsht_max_date\n    WHERE CAST(prod_instnc_ts AS DATE)=bq_snpsht_max_date.max_date\n    AND bus_prod_instnc_src_id = 1001\n    ),\n    --Rank Cust ID\n    rank_cust_id AS (\n    SELECT DISTINCT\n        bacct_bus_bacct_num,\n        cust_id,\n        RANK() OVER(PARTITION BY bacct_bus_bacct_num\n                        ORDER BY prod_rank,\n                                    cust_id) AS cust_id_rank               \n    FROM rank_prod_type\n    )\n    --Select best cust id\n    SELECT bacct_bus_bacct_num,\n        cust_id\n    FROM rank_cust_id\n    WHERE cust_id_rank = 1\n    \"\"\"\n\n    #### For wb\n    import google.oauth2.credentials\n    CREDENTIALS = google.oauth2.credentials.Credentials(token)\n\n    client = bigquery.Client(project=project_id, credentials=CREDENTIALS)\n\n    #     #### For prod \n    #     client = bigquery.Client(project=project_id)\n\n    df_cust = client.query(get_cust_id).to_dataframe()\n    df_final = df_orig.set_index('bus_bacct_num').join(df_cust.set_index('bacct_bus_bacct_num')).reset_index()\n    df_final = df_final.rename(columns={'index': 'bus_bacct_num', 'cust_bus_cust_id': 'cust_id'})\n    df_final = df_final.sort_values(by=['score_num'], ascending=False)\n    df_final.to_csv(file_name, index=False)\n    time.sleep(120)\n\n"
            ],
            "image": "northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-pycaret-slim:latest",
            "resources": {
              "cpuLimit": 4.0,
              "memoryLimit": 32.0
            }
          }
        },
        "exec-preprocess": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "preprocess"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.18' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef preprocess(pipeline_dataset: str\n               , save_data_path: str\n               , project_id: str\n               , dataset_id: str\n               , score_date_dash: str\n               ):\n\n    from google.cloud import bigquery\n    import pandas as pd\n    import numpy as np\n    import gc\n    import time\n\n    def to_categorical(df, cat_feature_names): \n\n        df_dummies = pd.get_dummies(df[cat_feature_names]) \n        df_dummies.columns = df_dummies.columns.str.replace('&', 'and')\n        df_dummies.columns = df_dummies.columns.str.replace(' ', '_')\n\n        df.drop(columns=cat_feature_names, axis=1, inplace=True)\n\n        df = df.join(df_dummies)\n\n        #column name clean-up\n        df.columns = df.columns.str.replace(' ', '_')\n        df.columns = df.columns.str.replace('-', '_')\n\n        return df\n\n    client = bigquery.Client(project=project_id)\n\n    # pipeline_dataset\n    pipeline_dataset_name = f\"{project_id}.{dataset_id}.{pipeline_dataset}\" \n    build_df_pipeline_dataset = f'SELECT * FROM `{pipeline_dataset_name}`'\n    df_pipeline_dataset = client.query(build_df_pipeline_dataset).to_dataframe()\n\n    # demo columns\n    df_pipeline_dataset['demo_urban_flag'] = df_pipeline_dataset.demo_sgname.fillna('').str.lower().apply(lambda x: 1 if 'urban' in x and 'suburban' not in x else 0).astype(int)\n    df_pipeline_dataset['demo_rural_flag'] = df_pipeline_dataset.demo_sgname.fillna('').str.lower().apply(lambda x: 1 if 'suburban' in x or 'rural' in x or 'town' in x else 0).astype(int)\n    df_pipeline_dataset['demo_family_flag'] = df_pipeline_dataset.demo_lsname.str.lower().str.contains('families').fillna(0).astype(int)\n\n    # categorical variables to dummy variables\n    cat_feature_names = ['revenue_band', 'payment_mthd', 'ebill_ind', 'dvc_non_telus_ind', 'credit_class', 'contract_type', 'bacct_delinq_ind', 'urbn_rur_ind',\n                     'dnc_sms_ind', 'dnc_em_ind', 'data_usg_trend', 'wls_data_plan_ind', 'wls_data_shr_plan_ind', 'demo_lsname']\n\n    df_pipeline_dataset = to_categorical(df_pipeline_dataset, cat_feature_names)\n\n    df_join = df_pipeline_dataset.copy()\n\n    # set up df_target \n    sql_target = ''' SELECT * FROM `{}.{}.bq_telus_postpaid_churn_targets` '''.format(project_id, dataset_id) \n    df_target = client.query(sql_target).to_dataframe()\n    df_target = df_target.loc[\n        df_target['YEAR_MONTH'] == '-'.join(score_date_dash.split('-')[:2])]  # score_date_dash = '2022-08-31'\n    df_target['ban'] = df_target['ban'].astype('int64')\n    df_target['subscriber_no'] = df_target['subscriber_no'].astype('str')\n    df_target = df_target.groupby(['ban', 'subscriber_no']).tail(1)\n\n    # set up df_final\n    df_final = df_join.merge(df_target[['ban', 'subscriber_no', 'target_ind']], on=['ban', 'subscriber_no'], how='left')\n    df_final.rename(columns={'target_ind': 'target'}, inplace=True) \n    df_final['target'].fillna(0, inplace=True) \n    df_final['target'] = df_final['target'].astype(int) \n    print(df_final.shape)\n\n    # delete df_join\n    del df_join\n    gc.collect()\n    print('......df_final done')\n\n    for f in df_final.columns:\n        df_final[f] = list(df_final[f])\n\n    df_final.to_csv(save_data_path, index=True) \n    del df_final\n    gc.collect()\n    print(f'......csv saved in {save_data_path}')\n    time.sleep(120)\n\n"
            ],
            "image": "northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-pycaret-slim:latest",
            "resources": {
              "cpuLimit": 4.0,
              "memoryLimit": 32.0
            }
          }
        }
      }
    },
    "pipelineInfo": {
      "name": "telus-postpaid-churn-serving-pipeline"
    },
    "root": {
      "dag": {
        "outputs": {
          "artifacts": {
            "batch-prediction-metrics": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "metrics",
                  "producerSubtask": "batch-prediction"
                }
              ]
            },
            "batch-prediction-metricsc": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "metricsc",
                  "producerSubtask": "batch-prediction"
                }
              ]
            }
          }
        },
        "tasks": {
          "batch-prediction": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-batch-prediction"
            },
            "dependentTasks": [
              "load-ml-model"
            ],
            "inputs": {
              "parameters": {
                "dataset_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "telus_postpaid_churn"
                    }
                  }
                },
                "file_bucket": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4-default"
                    }
                  }
                },
                "model_uri": {
                  "taskOutputParameter": {
                    "outputParameterKey": "model_uri",
                    "producerTask": "load-ml-model"
                  }
                },
                "project_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4"
                    }
                  }
                },
                "save_data_path": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "gs://divg-groovyhoon-pr-d2eab4-default/telus_postpaid_churn/telus_postpaid_churn_score.csv"
                    }
                  }
                },
                "score_date_dash": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "2023-12-06"
                    }
                  }
                },
                "score_table": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "bq_telus_postpaid_churn_scores"
                    }
                  }
                },
                "service_type": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "telus_postpaid_churn"
                    }
                  }
                },
                "table_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "bq_tpc_serving_dataset_preprocessed"
                    }
                  }
                },
                "temp_table": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "temp_telus_postpaid_churn_scores"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "batch-prediction"
            }
          },
          "bq-create-dataset": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-bq-create-dataset"
            },
            "inputs": {
              "parameters": {
                "dataset_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "telus_postpaid_churn"
                    }
                  }
                },
                "environment": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "serving"
                    }
                  }
                },
                "project_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4"
                    }
                  }
                },
                "region": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "northamerica-northeast1"
                    }
                  }
                },
                "score_date": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "2023-12-06"
                    }
                  }
                },
                "score_date_delta": {
                  "runtimeValue": {
                    "constantValue": {
                      "intValue": "0"
                    }
                  }
                },
                "token": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "ya29.a0AfB_byC73J5WDr3vBr0cAPNQKDyWkdOr14292fUsX54uIxgaBDpjXGWz9JwRg0NwbpfFw188lCQzGrDiNEe3GFwx5bNU6qNyz4jj4CZEOY-oR6LdniW19pl4X9eAFQyGyO6kO7dRoINmP9miaIO_Y7ZN52dsyeK8jbFolNK0frAaCgYKAbkSARISFQHGX2MiO6OIJ9e7Z2eEUbH-Gn9_Fg0178"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "bq-create-dataset"
            }
          },
          "load-ml-model": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-load-ml-model"
            },
            "dependentTasks": [
              "preprocess"
            ],
            "inputs": {
              "parameters": {
                "model_name": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "telus_postpaid_churn"
                    }
                  }
                },
                "project_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4"
                    }
                  }
                },
                "region": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "northamerica-northeast1"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "load-ml-model"
            }
          },
          "postprocess": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-postprocess"
            },
            "dependentTasks": [
              "batch-prediction"
            ],
            "inputs": {
              "parameters": {
                "dataset_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "telus_postpaid_churn"
                    }
                  }
                },
                "file_bucket": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4-default"
                    }
                  }
                },
                "project_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4"
                    }
                  }
                },
                "score_date_dash": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "2023-12-06"
                    }
                  }
                },
                "service_type": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "telus_postpaid_churn"
                    }
                  }
                },
                "temp_table": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "temp_telus_postpaid_churn_scores"
                    }
                  }
                },
                "token": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "ya29.a0AfB_byC73J5WDr3vBr0cAPNQKDyWkdOr14292fUsX54uIxgaBDpjXGWz9JwRg0NwbpfFw188lCQzGrDiNEe3GFwx5bNU6qNyz4jj4CZEOY-oR6LdniW19pl4X9eAFQyGyO6kO7dRoINmP9miaIO_Y7ZN52dsyeK8jbFolNK0frAaCgYKAbkSARISFQHGX2MiO6OIJ9e7Z2eEUbH-Gn9_Fg0178"
                    }
                  }
                },
                "ucar_score_table": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "bi-srv-mobilityds-pr-80a48d.ucar_ingestion.bq_product_instance_model_score"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "postprocess"
            }
          },
          "preprocess": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-preprocess"
            },
            "dependentTasks": [
              "bq-create-dataset"
            ],
            "inputs": {
              "parameters": {
                "dataset_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "telus_postpaid_churn"
                    }
                  }
                },
                "pipeline_dataset": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "bq_tpc_serving_dataset"
                    }
                  }
                },
                "project_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4"
                    }
                  }
                },
                "save_data_path": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "gs://divg-groovyhoon-pr-d2eab4-default/telus_postpaid_churn/telus_postpaid_churn_score.csv"
                    }
                  }
                },
                "score_date_dash": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "2023-12-06"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "preprocess"
            }
          }
        }
      },
      "inputDefinitions": {
        "parameters": {
          "file_bucket": {
            "type": "STRING"
          },
          "project_id": {
            "type": "STRING"
          },
          "region": {
            "type": "STRING"
          },
          "resource_bucket": {
            "type": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "batch-prediction-metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "batch-prediction-metricsc": {
            "artifactType": {
              "schemaTitle": "system.ClassificationMetrics",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "schemaVersion": "2.0.0",
    "sdkVersion": "kfp-1.8.18"
  },
  "runtimeConfig": {
    "parameters": {
      "file_bucket": {
        "stringValue": "divg-groovyhoon-pr-d2eab4-default"
      },
      "project_id": {
        "stringValue": "divg-groovyhoon-pr-d2eab4"
      },
      "region": {
        "stringValue": "northamerica-northeast1"
      },
      "resource_bucket": {
        "stringValue": "divg-groovyhoon-pr-d2eab4-default"
      }
    }
  }
}