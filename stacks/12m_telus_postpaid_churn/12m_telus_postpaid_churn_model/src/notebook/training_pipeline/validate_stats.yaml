name: Validate stats
description: 'Inputs:'
inputs:
- {name: project_id, type: String}
- {name: bucket_nm, type: String}
- {name: model_nm, type: String}
- {name: validation_type, type: String}
- {name: op_type, type: String}
- {name: statistics, type: Artifact}
- {name: base_stats_path, type: String}
- {name: update_ts, type: String}
- {name: src_schema_path, type: String}
- {name: src_anomaly_thresholds_path, type: String}
- {name: dest_anomalies_gcs_path, type: String}
- {name: dest_anomalies_bq_dataset, type: String, default: '', optional: true}
- name: in_bq_ind
  type: Boolean
  default: "True"
  optional: true
outputs:
- {name: anomalies, type: Artifact}
implementation:
  container:
    image: northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-tfdv-slim:1.0.0
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.18' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
      \ import *\n\ndef validate_stats( \n    project_id: str, \n    bucket_nm: str,\
      \ \n    model_nm: str, \n    validation_type: str, \n    op_type: str, \n  \
      \  statistics: Input[Artifact], \n    anomalies: Output[Artifact], \n    base_stats_path:\
      \ str, \n    update_ts: str, \n    src_schema_path: str, \n    src_anomaly_thresholds_path:\
      \ str, \n    dest_anomalies_gcs_path: str, \n    dest_anomalies_bq_dataset:\
      \ str = '', \n    in_bq_ind: bool = True \n    ): \n\n    ''' \n    Inputs:\
      \ \n        - project_id: project id \n        - bucket_nm: name of bucket where\
      \ anomaly thresholds are stored \n        - model_nm: name of model \n     \
      \   - validation_type: skew or drift \n        - op_type: serving or predictions\
      \ \n        - statistics: path to statistics imported from generate stats component\
      \ \n        - base_stats_path: path to statistics for comparison (training for\
      \ skew, old serving stats for drift) \n        - update_ts: time pipeline is\
      \ run. keep consistent across components \n        - src_schema_path: path to\
      \ where schema is in GCS (for either serving stats or prediction stats) \n \
      \       - src_anomaly_thresholds_path: path to json file where skew/drift anomaly\
      \ thresholds are specified \n        - dest_anomalies_gcs_path: path to where\
      \ anomalies should be stored in GCS \n        - dest_anomalies_bq_dataset: dataset\
      \ where anomalies will be stored in BQ \n        - in_bq_ind: indicate whether\
      \ you want to save anomalies in BQ \n\n    Outputs: \n        - anomalies: path\
      \ to anomalies file          \n    ''' \n\n    import logging \n    import tensorflow_data_validation\
      \ as tfdv \n    from google.cloud import storage \n    from google.cloud import\
      \ bigquery \n    import json \n    import pandas as pd \n    from datetime import\
      \ datetime \n\n    if (validation_type != \"skew\") and (validation_type !=\
      \ \"drift\"):\n        raise ValueError(\"Error: the validation_type can only\
      \ be one of skew or drift\")\n\n    if (op_type != \"serving\") and (op_type\
      \ != \"predictions\"):\n        raise ValueError(\"Error: the op_type can only\
      \ be one of serving or predictions\")\n\n    # convert timestamp to datetime\
      \ \n    update_ts = datetime.strptime(update_ts, '%Y-%m-%d %H:%M:%S') \n\n \
      \   # set uri of anomalies output \n    anomalies.uri = dest_anomalies_gcs_path\
      \ \n\n    def load_stats(path): \n        print(f'loading stats from: {path}')\
      \ \n        return tfdv.load_statistics(input_path=path) \n\n    base_stats\
      \ = load_stats(base_stats_path) \n    stats = load_stats(statistics.uri) \n\n\
      \    # get schema \n    schema = tfdv.load_schema_text(\n        input_path=src_schema_path\n\
      \        ) \n\n    if op_type == 'serving': \n        # set anomaly check to\
      \ features \n        anomaly_check = 'features' \n\n        # ensure that serving\
      \ set as env \n        if 'SERVING' not in schema.default_environment: \n  \
      \          schema.default_environment.append('SERVING') \n\n    if op_type ==\
      \ 'predictions': \n        # set anomaly check to predictions \n        anomaly_check\
      \ = 'predictions' \n\n        #ensure that predictions set as env \n       \
      \ if 'PREDICTIONS' not in schema.default_environment: \n            schema.default_environment.append('PREDICTIONS')\
      \ \n\n    # get serving anomaly thresholds \n    storage_client = storage.Client()\
      \ \n    bucket = storage_client.bucket(bucket_nm) \n    blob = bucket.blob(src_anomaly_thresholds_path)\
      \ \n    blob.download_to_filename('anomaly_thresholds.json') \n\n    f = open('anomaly_thresholds.json')\
      \ \n    anomaly_thresholds = json.load(f) \n\n    if validation_type == 'skew':\
      \ \n        # set serving thresholds \n        for feature, threshold in anomaly_thresholds[anomaly_check]['numerical'].items():\
      \ \n            tfdv.get_feature(schema, feature).skew_comparator.jensen_shannon_divergence.threshold\
      \ = threshold \n\n        for feature, threshold in anomaly_thresholds[anomaly_check]['categorical'].items():\
      \ \n            tfdv.get_feature(schema, feature).skew_comparator.infinity_norm.threshold\
      \ = threshold\n\n        # validating stats \n        detected_anomalies = tfdv.validate_statistics(\n\
      \            statistics=stats, \n            schema=schema, \n            environment=op_type.upper(),\
      \ \n            serving_statistics=base_stats\n            ) \n\n    elif validation_type\
      \ == 'drift': \n        # set serving thresholds \n        for feature, threshold\
      \ in anomaly_thresholds[anomaly_check]['numerical'].items(): \n            tfdv.get_feature(schema,\
      \ feature).drift_comparator.jensen_shannon_divergence.threshold = threshold\
      \ \n\n        for feature, threshold in anomaly_thresholds[anomaly_check]['categorical'].items():\
      \ \n            tfdv.get_feature(schema, feature).drift_comparator.infinity_norm.threshold\
      \ = threshold \n\n        # validating stats \n        detected_anomalies =\
      \ tfdv.validate_statistics(\n            statistics=stats, \n            schema=schema,\
      \ \n            environment=op_type.upper(), \n            previous_statistics=base_stats\
      \ \n            ) \n\n    else: \n        print(\"Please specify skew or drift\"\
      ) \n\n    # store updated schema in gcs\n    tfdv.write_schema_text(schema=schema,\
      \ output_path=src_schema_path) \n\n    logging.info(f'writing anomalies to:\
      \ {dest_anomalies_gcs_path}') \n    tfdv.write_anomalies_text(detected_anomalies,\
      \ dest_anomalies_gcs_path) \n\n    # OPTIONAL: save anomalies to BQ \n    if\
      \ in_bq_ind == True: \n        print(\"yes there are anomalies\")\n        anomalies_dict\
      \ = detected_anomalies.anomaly_info \n        skew_drift_dict = detected_anomalies.drift_skew_info\n\
      \n    df_anomalies = pd.DataFrame(columns=[\n                              \
      \  'model_nm', 'update_ts', 'feature_nm', 'short_description', 'long_description'])\
      \ \n\n    # check if there are anomalies (dict is not empty) \n    if bool(anomalies_dict):\
      \ \n        for key in anomalies_dict: \n            feature = key \n      \
      \      short_description = anomalies_dict[key].short_description \n        \
      \    long_description = anomalies_dict[key].description \n\n            df_anomalies.loc[len(df_anomalies.index)]\
      \ = pd.Series({ \n                'model_nm': model_nm, \n                'update_ts':\
      \ update_ts, \n                'feature_nm': feature, \n                'short_description':\
      \ short_description, \n                'long_description': long_description\n\
      \                }) \n\n    # check for skew-drift\n    df_sd = pd.DataFrame(columns=['feature_nm',\
      \ 'skew_drift'])\n\n    #check for skew-drift \n    if bool(skew_drift_dict):\
      \ \n        for sd in skew_drift_dict: \n\n            feature = sd.path.step[0]\n\
      \n            if validation_type == 'skew': \n                value = sd.skew_measurements[0].value\
      \ \n                threshold = sd.skew_measurements[0].threshold \n       \
      \         val_type = 'skew' \n                skew_drift_type_num = sd.skew_measurements[0].type\
      \ \n\n            elif validation_type == 'drift': \n                value =\
      \ sd.drift_measurements[0].value \n                threshold = sd.drift_measurements[0].threshold\
      \ \n                val_type = 'drift' \n                skew_drift_type_num\
      \ = sd.drift_measurements[0].type \n            else: \n                print(\"\
      Please specify skew or drift\") \n\n            if skew_drift_type_num == 1:\
      \ \n                skew_drift_type = 'L_INFTY' \n            elif skew_drift_type_num\
      \ == 2: \n                skew_drift_type = 'JENSEN_SHANNON_DIVERGENCE' \n \
      \           elif skew_drift_type_num == 3: \n                skew_drift_type\
      \ = 'NORMALIZED_ABSOLUTE_DIFFERENCE' \n            else: \n                skew_drift_type\
      \ = 'UNKNOWN' \n\n            df_sd.loc[len(df_sd.index)] = pd.Series({ \n \
      \               'feature_nm': feature, \n                'skew_drift': {'type':\
      \ skew_drift_type, 'validation_type': val_type, 'value': value, 'threshold':\
      \ threshold}\n                }) \n\n            print(feature) \n\n    df_anomalies\
      \ = pd.merge(df_anomalies, df_sd, on='feature_nm', how='left') \n\n    # load\
      \ data stats into BQ table \n    client = bigquery.Client(project=project_id)\
      \ \n\n    job_config = bigquery.LoadJobConfig(write_disposition='WRITE_APPEND',\
      \ \n                                        schema=[bigquery.SchemaField(\n\
      \                                                    'model_nm', 'STRING'),\
      \ \n                                                bigquery.SchemaField(\n\
      \                                                    'update_ts', 'TIMESTAMP'),\
      \ \n                                                bigquery.SchemaField(\n\
      \                                                    'feature_nm', 'STRING'),\
      \ \n                                                bigquery.SchemaField(\n\
      \                                                    'short_description', 'STRING'),\
      \ \n                                                bigquery.SchemaField(\n\
      \                                                    'long_description', 'STRING'),\n\
      \                                                bigquery.SchemaField('skew_drift',\
      \ 'RECORD', \n                                                    fields=[bigquery.SchemaField('type',\
      \ 'STRING'), \n                                                            bigquery.SchemaField('validation_type',\
      \ 'STRING'), \n                                                            bigquery.SchemaField('value',\
      \ 'FLOAT'), \n                                                            bigquery.SchemaField('threshold',\
      \ 'FLOAT')]), \n                                                ],) # create\
      \ new table or append if already exists \n\n    anomalies_table  = f'{project_id}.{dest_anomalies_bq_dataset}.bq_data_anomalies'\
      \ \n\n    job = client.load_table_from_dataframe(\n        df_anomalies, anomalies_table,\
      \ job_config=job_config \n        ) \n    job.result() \n    table = client.get_table(anomalies_table)\
      \ \n    print( \n        \"Loaded {} rows and {} columns to {}\".format(\n \
      \           table.num_rows, len(table.schema), anomalies_table\n        )\n\
      \    ) \n\n"
    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - validate_stats
