{
  "pipelineSpec": {
    "components": {
      "comp-postprocess": {
        "executorLabel": "exec-postprocess",
        "inputDefinitions": {
          "parameters": {
            "file_bucket": {
              "type": "STRING"
            },
            "hs_nba_utils_path": {
              "type": "STRING"
            },
            "model_type": {
              "type": "STRING"
            },
            "output_dataset_id": {
              "type": "STRING"
            },
            "pipeline_path": {
              "type": "STRING"
            },
            "pipeline_type": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "resource_bucket": {
              "type": "STRING"
            },
            "score_table_id": {
              "type": "STRING"
            },
            "stack_name": {
              "type": "STRING"
            },
            "token": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-predict": {
        "executorLabel": "exec-predict",
        "inputDefinitions": {
          "parameters": {
            "file_bucket": {
              "type": "STRING"
            },
            "hs_nba_utils_path": {
              "type": "STRING"
            },
            "model_type": {
              "type": "STRING"
            },
            "pipeline_type": {
              "type": "STRING"
            },
            "preprocess_output_csv": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "resource_bucket": {
              "type": "STRING"
            },
            "service_type": {
              "type": "STRING"
            },
            "serving_pipeline_path": {
              "type": "STRING"
            },
            "stack_name": {
              "type": "STRING"
            },
            "training_pipeline_path": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-preprocess": {
        "executorLabel": "exec-preprocess",
        "inputDefinitions": {
          "parameters": {
            "dataset_id": {
              "type": "STRING"
            },
            "file_bucket": {
              "type": "STRING"
            },
            "hs_nba_utils_path": {
              "type": "STRING"
            },
            "load_sql": {
              "type": "STRING"
            },
            "model_type": {
              "type": "STRING"
            },
            "pipeline_path": {
              "type": "STRING"
            },
            "pipeline_type": {
              "type": "STRING"
            },
            "preprocess_output_csv": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "resource_bucket": {
              "type": "STRING"
            },
            "stack_name": {
              "type": "STRING"
            },
            "table_id": {
              "type": "STRING"
            },
            "token": {
              "type": "STRING"
            }
          }
        }
      }
    },
    "deploymentSpec": {
      "executors": {
        "exec-postprocess": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "postprocess"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.18' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef postprocess(\n    project_id: str,\n    output_dataset_id: str,\n    score_table_id: str,\n    resource_bucket: str,\n    file_bucket: str,\n    stack_name: str,\n    model_type: str,\n    pipeline_type: str,\n    pipeline_path: str,\n    hs_nba_utils_path: str, \n    token: str\n    ):\n    \"\"\"\n    Postprocess data for a machine learning pipeline.\n    \"\"\"\n\n    # import global modules\n    from google.cloud import storage\n    from pathlib import Path\n    from yaml import safe_load\n    import sys\n    import os\n    import pandas as pd\n\n    # set global vars\n    pth_project = Path(os.getcwd())\n    pth_model_config = pth_project / 'model_config.yaml'\n    pth_queries = pth_project / 'queries'\n    sys.path.insert(0, pth_project.as_posix())\n\n    # init gcp clients\n    storage_client = storage.Client()\n\n    def extract_dir_from_bucket(\n        bucket: Any, local_path: Path, prefix: str, split_prefix: str = 'serving_pipeline' \n    ):    \n        \"\"\"\n        Download files from a specified bucket to a local path, excluding a specified prefix.\n\n        Parameters:\n        - bucket: The bucket object from which to download files.\n        - local_path: The local path where the files will be downloaded to.\n        - prefix: The prefix to filter the files in the bucket. Only files with this prefix will be downloaded.\n        - split_prefix: The prefix to exclude from the downloaded file paths. Default is 'serving_pipeline'.\n        \"\"\"\n        for blob in bucket.list_blobs(prefix=prefix):\n            if not blob.name.endswith(\"/\"):\n                path = local_path / blob.name.split(f'{split_prefix}/')[-1]\n                str_path = path.as_posix()\n                Path(str_path[:str_path.rindex('/')]).mkdir(parents=True, exist_ok=True)\n                blob.download_to_filename(str_path)\n\n    # download utils and model config locally\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(resource_bucket)\n    extract_dir_from_bucket(\n        bucket, pth_project, f'{stack_name}/{hs_nba_utils_path}', split_prefix='notebook'\n    )\n    extract_dir_from_bucket(\n        bucket, pth_project, f'{stack_name}/{pipeline_path}/queries'\n    )\n    blob = bucket.blob(f'{stack_name}/{pipeline_path}/model_config.yaml')\n    blob.download_to_filename(pth_model_config)\n\n    # import local modules\n    from hs_nba_utils.etl.load import create_temp_table, insert_from_temp_table\n\n    # load model config\n    d_model_config = safe_load(pth_model_config.open())\n\n   # load data from bucket\n    df_scores = pd.read_csv(f'gs://{file_bucket}/{pipeline_type}/df_score.csv')\n\n    print(f'Scores df.shape {df_scores.shape}')\n\n    # insert model id and and set unavailable targets to None\n    df_scores['model_id'] = d_model_config['model_id']\n    l_unavailable_targets = d_model_config['unavailable_target_variables']\n    if l_unavailable_targets:\n        df_scores[l_unavailable_targets] = [None] * len(l_unavailable_targets)\n\n    # create temp table in bq\n    temp_table_name = create_temp_table(\n        project_id, output_dataset_id, score_table_id, df_scores\n    )\n\n    print(f'created a temp table {temp_table_name}')\n\n    # insert data from temp into main table\n    current_part_dt = str(df_scores['part_dt'].max())\n    insert_from_temp_table(\n        project_id, output_dataset_id, score_table_id, temp_table_name, current_part_dt,\n        pth_queries / 'drop_current_part_dt.sql', pth_queries / 'insert_from_temp_table.sql', token\n    )\n\n"
            ],
            "image": "northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-pycaret-slim:latest",
            "resources": {
              "cpuLimit": 4.0,
              "memoryLimit": 64.0
            }
          }
        },
        "exec-predict": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "predict"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.18' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef predict(project_id: str,\n            resource_bucket: str,\n            file_bucket: str, \n            stack_name: str,\n            service_type: str, \n            model_type: str,\n            pipeline_type: str,\n            training_pipeline_path: str,\n            serving_pipeline_path: str,\n            preprocess_output_csv: str, \n            hs_nba_utils_path: str\n            ):\n\n    \"\"\"\n    Machine learning predict pipeline.\n    \"\"\"\n\n    # Import global modules\n    from google.cloud import storage\n    from pathlib import Path\n    from yaml import safe_load\n    import sys\n    import os\n    import pandas as pd\n    import pickle\n\n    # set global vars\n    pth_project = Path(os.getcwd())\n    pth_model_config = pth_project / 'model_config.yaml'\n    sys.path.insert(0, pth_project.as_posix())\n\n    # init gcp clients\n    storage_client = storage.Client()\n\n    def extract_dir_from_bucket(\n        bucket: Any, local_path: Path, prefix: str, split_prefix: str = 'serving_pipeline' \n    ):            \n        \"\"\"\n        Download files from a specified bucket to a local path, excluding a specified prefix.\n\n        Parameters:\n        - bucket: The bucket object from which to download files.\n        - local_path: The local path where the files will be downloaded to.\n        - prefix: The prefix to filter the files in the bucket. Only files with this prefix will be downloaded.\n        - split_prefix: The prefix to exclude from the downloaded file paths. Default is 'serving_pipeline'.\n        \"\"\"\n        for blob in bucket.list_blobs(prefix=prefix):\n            if not blob.name.endswith(\"/\"):\n                path = local_path / blob.name.split(f'{split_prefix}/')[-1]\n                str_path = path.as_posix()\n                Path(str_path[:str_path.rindex('/')]).mkdir(parents=True, exist_ok=True)\n                blob.download_to_filename(str_path)\n\n    # download utils and model config locally\n    bucket = storage_client.bucket(resource_bucket)\n    extract_dir_from_bucket(\n        bucket, pth_project, f'{stack_name}/{hs_nba_utils_path}', split_prefix='notebook'\n    )  \n    blob = bucket.blob(f\"{stack_name}/{serving_pipeline_path}/model_config.yaml\" )\n    blob.download_to_filename(pth_model_config)\n\n    # download model pickle file\n    model_name = f'{service_type}_xgb_models_latest.pkl'    \n    bucket = storage_client.bucket(file_bucket)\n    blob = bucket.blob(f'models/{model_name}')\n    blob.download_to_filename(pth_project / model_name)\n\n    # load model\n    d_model_config = safe_load(pth_model_config.open())\n    with open(pth_project / model_name, \"rb\") as f:\n        models_dict = pickle.load(f)\n    model = models_dict['model']\n\n    # load data from bucket\n    df_features = pd.read_csv(f'gs://{file_bucket}/{pipeline_type}/{preprocess_output_csv}', index_col=None)\n\n    print(f'Features df.shape {df_features.shape}')\n\n    # select features and model predict  \n    l_customer_ids = [f['name'] for f in d_model_config['customer_ids']]\n    l_feature_names = [f['name'] for f in d_model_config['features']]\n\n    # make predictions on df_features\n    np_preds = model.predict_proba(df_features[l_feature_names])\n\n    # map target index with target names\n    d_rename_mapping = {\n        d_target_info['class_index']: d_target_info['name']\n        for d_target_info in d_model_config['target_variables'][model_type]\n    }\n\n    # build result dataframe\n    df_preds = pd.DataFrame(np_preds)\n    df_preds = df_preds.rename(columns=d_rename_mapping)\n\n    # add customer ids and partition date\n    df_preds[l_customer_ids] = df_features[l_customer_ids]    \n    df_preds['part_dt'] = df_features['part_dt']\n\n    # save data to pipeline bucket\n    location_to_save = f'gs://{file_bucket}/{pipeline_type}/df_score.csv'\n    df_preds.to_csv(location_to_save, index=False)\n    print(f'Scores saved into {location_to_save}')\n\n"
            ],
            "image": "northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-pycaret-slim:latest",
            "resources": {
              "cpuLimit": 8.0,
              "memoryLimit": 64.0
            }
          }
        },
        "exec-preprocess": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "preprocess"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.18' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef preprocess(\n    project_id: str,\n    dataset_id: str,\n    table_id: str,\n    file_bucket: str,\n    resource_bucket: str,\n    stack_name: str,\n    pipeline_path: str,\n    hs_nba_utils_path: str, \n    model_type: str,\n    load_sql: str, \n    preprocess_output_csv: str,\n    pipeline_type: str, \n    token: str\n):\n    \"\"\"\n    Preprocess data for a machine learning training pipeline.\n    \"\"\"\n\n    # import global modules\n    from google.cloud import storage\n    from google.cloud import bigquery\n    from pathlib import Path\n    from yaml import safe_load\n    import sys\n    import os\n\n    # for prod\n    pth_project = Path(os.getcwd())\n    pth_model_config = pth_project / 'model_config.yaml'\n    pth_queries = pth_project / 'queries'\n    sys.path.insert(0, pth_project.as_posix())\n\n    #### For wb\n    import google.oauth2.credentials\n    CREDENTIALS = google.oauth2.credentials.Credentials(token)\n\n    client = bigquery.Client(project=project_id, credentials=CREDENTIALS)\n    job_config = bigquery.QueryJobConfig()\n\n#     #### For prod \n#     client = bigquery.Client(project=project_id)\n#     job_config = bigquery.QueryJobConfig()\n\n    def extract_dir_from_bucket(\n        bucket: Any, local_path: Path, prefix: str, split_prefix: str = 'serving_pipeline' \n    ):\n        \"\"\"\n        Download files from a specified bucket to a local path, excluding a specified prefix.\n\n        Parameters:\n        - bucket: The bucket object from which to download files.\n        - local_path: The local path where the files will be downloaded to.\n        - prefix: The prefix to filter the files in the bucket. Only files with this prefix will be downloaded.\n        - split_prefix: The prefix to exclude from the downloaded file paths. Default is 'serving_pipeline'.\n        \"\"\"\n        for blob in bucket.list_blobs(prefix=prefix):\n            if not blob.name.endswith(\"/\"):\n                path = local_path / blob.name.split(f'{split_prefix}/')[-1]\n                str_path = path.as_posix()\n                Path(str_path[:str_path.rindex('/')]).mkdir(parents=True, exist_ok=True)\n                blob.download_to_filename(str_path)\n\n    # download utils and model config locally\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(resource_bucket)\n    extract_dir_from_bucket(\n        bucket, pth_project, f'{stack_name}/{hs_nba_utils_path}', split_prefix='notebook'\n    ) \n    extract_dir_from_bucket(\n        bucket, pth_project, f'{stack_name}/{pipeline_path}/queries', split_prefix=pipeline_type\n    )\n\n    blob = bucket.blob(f'{stack_name}/{pipeline_path}/model_config.yaml')\n    blob.download_to_filename(pth_model_config)\n\n    # import local modules\n    from hs_nba_utils.etl.extract import extract_bq_data\n    from hs_nba_utils.modeling.prospects_features_preprocessing import process_prospects_features\n\n    # load model config\n    d_model_config = safe_load(pth_model_config.open())\n\n    # select columns to query\n    target_column = d_model_config['target_column']\n    str_feature_names = ','.join([f\"cast({f['name']} as {f['type']}) as {f['name']}\" for f in d_model_config['features']])\n    str_customer_ids = ','.join([f\"cast({f['name']} as {f['type']}) as {f['name']}\" for f in d_model_config['customer_ids']])\n    str_target_labels = ','.join([f\"\\\"{f['name']}\\\"\" for f in d_model_config['target_variables']['acquisition']])\n\n    # extract training data\n    sql = (pth_queries / load_sql).read_text().format(\n        project_id=project_id\n        , dataset_id=dataset_id\n        , table_id=table_id\n        , target_column=target_column\n        , customer_ids=str_customer_ids\n        , feature_names=str_feature_names\n        , target_labels=str_target_labels\n    )\n\n    # save sql to gcs bucket\n    file_name = f'{pipeline_type}_queries/load_train_data_formatted.sql'\n\n    # Convert the string to bytes\n    content_bytes = sql.encode('utf-8')\n\n    # Upload the file to GCS\n    bucket = storage_client.bucket(file_bucket)\n    blob = bucket.blob(file_name)\n    blob.upload_from_string(content_bytes)\n\n    df = extract_bq_data(client, sql)\n    print(f\"Training dataset df.shape {df.shape}\")\n\n    # process features\n    df_processed = process_prospects_features(\n        df, d_model_config, training_mode=True, model_type=model_type, target_name=target_column\n    )\n    print(f\"Training dataset processed df.shape {df_processed.shape}\")\n\n    # save data to pipeline bucket\n    df_processed.to_csv(\n        f'gs://{file_bucket}/{pipeline_type}/{preprocess_output_csv}', index=False\n    )\n    print(f'Training data saved into {file_bucket}')\n\n"
            ],
            "image": "northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-pycaret-slim:latest",
            "resources": {
              "cpuLimit": 8.0,
              "memoryLimit": 64.0
            }
          }
        }
      }
    },
    "pipelineInfo": {
      "name": "hs-nba-prospects-serving-pipeline"
    },
    "root": {
      "dag": {
        "tasks": {
          "postprocess": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-postprocess"
            },
            "dependentTasks": [
              "predict"
            ],
            "inputs": {
              "parameters": {
                "file_bucket": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4"
                    }
                  }
                },
                "hs_nba_utils_path": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "vertex_pipelines/hs_nba_utils/notebook"
                    }
                  }
                },
                "model_type": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "acquisition"
                    }
                  }
                },
                "output_dataset_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "telus_ffh_nba"
                    }
                  }
                },
                "pipeline_path": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "vertex_pipelines/hs_nba_prospects/serving_pipeline"
                    }
                  }
                },
                "pipeline_type": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "serving_pipeline"
                    }
                  }
                },
                "project_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4"
                    }
                  }
                },
                "resource_bucket": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4-default"
                    }
                  }
                },
                "score_table_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "bq_hs_nba_prospects_scores"
                    }
                  }
                },
                "stack_name": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "ffh_nba"
                    }
                  }
                },
                "token": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "ya29.a0AXooCgtawt5rd34VdGzU018YwqI_ve0P8JFcJUqx4mLEhvoGIhUFKGcg2xwnscqyY3BWWLeHfmVcLYbRpgwwG9QCfD627sAvR6ocikaDwDa28xXhShgQxFekUoyMN0g6xSb_W2HxtNMYhg4e9UP3ABLqMka3zpPcw27IWxyn_QaCgYKAVISARISFQHGX2MilNArChFWfgBkWOoeWS8NtQ0177"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "postprocess"
            }
          },
          "predict": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-predict"
            },
            "dependentTasks": [
              "preprocess"
            ],
            "inputs": {
              "parameters": {
                "file_bucket": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4"
                    }
                  }
                },
                "hs_nba_utils_path": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "vertex_pipelines/hs_nba_utils/notebook"
                    }
                  }
                },
                "model_type": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "acquisition"
                    }
                  }
                },
                "pipeline_type": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "serving_pipeline"
                    }
                  }
                },
                "preprocess_output_csv": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "df_serving.csv"
                    }
                  }
                },
                "project_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4"
                    }
                  }
                },
                "resource_bucket": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4-default"
                    }
                  }
                },
                "service_type": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "hs_nba_prospects"
                    }
                  }
                },
                "serving_pipeline_path": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "vertex_pipelines/hs_nba_prospects/serving_pipeline"
                    }
                  }
                },
                "stack_name": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "ffh_nba"
                    }
                  }
                },
                "training_pipeline_path": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "vertex_pipelines/hs_nba_prospects/training_pipeline"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "predict"
            }
          },
          "preprocess": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-preprocess"
            },
            "inputs": {
              "parameters": {
                "dataset_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "nba_features_prospect"
                    }
                  }
                },
                "file_bucket": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4"
                    }
                  }
                },
                "hs_nba_utils_path": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "vertex_pipelines/hs_nba_utils/notebook"
                    }
                  }
                },
                "load_sql": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "load_serving_data.sql"
                    }
                  }
                },
                "model_type": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "acquisition"
                    }
                  }
                },
                "pipeline_path": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "vertex_pipelines/hs_nba_prospects/serving_pipeline"
                    }
                  }
                },
                "pipeline_type": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "serving_pipeline"
                    }
                  }
                },
                "preprocess_output_csv": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "df_serving.csv"
                    }
                  }
                },
                "project_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4"
                    }
                  }
                },
                "resource_bucket": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4-default"
                    }
                  }
                },
                "stack_name": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "ffh_nba"
                    }
                  }
                },
                "table_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "master_features_set_prospect_predict_vw"
                    }
                  }
                },
                "token": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "ya29.a0AXooCgtawt5rd34VdGzU018YwqI_ve0P8JFcJUqx4mLEhvoGIhUFKGcg2xwnscqyY3BWWLeHfmVcLYbRpgwwG9QCfD627sAvR6ocikaDwDa28xXhShgQxFekUoyMN0g6xSb_W2HxtNMYhg4e9UP3ABLqMka3zpPcw27IWxyn_QaCgYKAVISARISFQHGX2MilNArChFWfgBkWOoeWS8NtQ0177"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "preprocess"
            }
          }
        }
      },
      "inputDefinitions": {
        "parameters": {
          "file_bucket": {
            "type": "STRING"
          },
          "project_id": {
            "type": "STRING"
          },
          "region": {
            "type": "STRING"
          },
          "resource_bucket": {
            "type": "STRING"
          }
        }
      }
    },
    "schemaVersion": "2.0.0",
    "sdkVersion": "kfp-1.8.18"
  },
  "runtimeConfig": {
    "parameters": {
      "file_bucket": {
        "stringValue": "divg-groovyhoon-pr-d2eab4"
      },
      "project_id": {
        "stringValue": "divg-groovyhoon-pr-d2eab4"
      },
      "region": {
        "stringValue": "northamerica-northeast1"
      },
      "resource_bucket": {
        "stringValue": "divg-groovyhoon-pr-d2eab4-default"
      }
    }
  }
}