{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c150da3-6e1e-4f02-a778-bb3b12af9054",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b559b7e6-fe51-49cc-8e9c-832380843832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, ClassificationMetrics,\n",
    "                        Metrics, component)\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import google\n",
    "from google.oauth2 import credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2.service_account import Credentials\n",
    "from google.cloud import storage\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components.v1.batch_predict_job import \\\n",
    "    ModelBatchPredictOp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558ff533-70c8-4421-813e-c567d6bc496b",
   "metadata": {},
   "source": [
    "### YAML Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f70d721-76e7-4c2e-8153-88b5bbe8ee47",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID =  ''\n",
    "DATASET_ID = ''\n",
    "RESOURCE_BUCKET = ''\n",
    "FILE_BUCKET = ''\n",
    "REGION = ''\n",
    "UCAR_SCORE_TABLE= 'bi-srv-mobilityds-pr-80a48d.ucar_ingestion.bq_product_instance_model_score'\n",
    "MODEL_ID = '6535'\n",
    "MODEL_NAME = 'telus_postpaid_churn'\n",
    "PREDICTION_IMAGE = \"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-pycaret-slim:latest\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24de5c39-21e0-425c-80a1-7d944234d897",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID =  'divg-groovyhoon-pr-d2eab4'\n",
    "DATASET_ID = 'telus_postpaid_churn'\n",
    "RESOURCE_BUCKET = 'divg-groovyhoon-pr-d2eab4-default'\n",
    "FILE_BUCKET = 'divg-groovyhoon-pr-d2eab4-default'\n",
    "MODEL_ID = '6535'\n",
    "MODEL_NAME = 'telus_postpaid_churn'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eeed21-e12b-45ee-b1e4-d0c8f3843533",
   "metadata": {},
   "source": [
    "### Service Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65d1668-e1d8-4f58-958f-edc44d06a6b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SERVICE_TYPE = 'telus_postpaid_churn'\n",
    "SERVICE_TYPE_NAME = 'telus-postpaid-churn'\n",
    "TABLE_ID = 'bq_telus_postpaid_churn_targets'\n",
    "REGION = \"northamerica-northeast1\"\n",
    "PROC_SERVING_DATASET_TABLE_NAME = 'bq_tpc_serving_dataset_preprocessed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2229e5d1-1362-40ed-aa7d-cb22e41e4960",
   "metadata": {},
   "source": [
    "### Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d628b-e759-4c9c-aab5-568c54721aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STACK_NAME = 'telus_postpaid_churn'\n",
    "SERVING_PIPELINE_NAME_PATH = 'telus_postpaid_churn_model/serving_pipeline'\n",
    "SERVING_PIPELINE_NAME = 'telus-postpaid-churn-serving-pipeline' # Same name as pulumi.yaml\n",
    "SERVING_PIPELINE_DESCRIPTION = 'telus-postpaid-churn-serving-pipeline'\n",
    "PIPELINE_ROOT = f\"gs://{FILE_BUCKET}\"\n",
    "REGION = \"northamerica-northeast1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe6a625-854f-4437-87c1-2b5aa38a6562",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_DATASET_TABLE_NAME = 'bq_tpc_serving_dataset'\n",
    "SERVING_DATASET_SP_NAME = 'bq_sp_tpc_serving_dataset'\n",
    "SCORE_TABLE_NAME = 'bq_telus_postpaid_churn_scores'\n",
    "TEMP_TABLE='temp_telus_postpaid_churn_scores'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9caff85-6365-4002-8864-356b649aadd5",
   "metadata": {},
   "source": [
    "### Save Data Path Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dda022-7462-4464-83f7-0995fd03f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SAVE_DATA_PATH='gs://{}/{}/{}_train.csv'.format(FILE_BUCKET, SERVICE_TYPE, SERVICE_TYPE)\n",
    "VALIDATION_SAVE_DATA_PATH='gs://{}/{}/{}_validation.csv'.format(FILE_BUCKET, SERVICE_TYPE, SERVICE_TYPE)\n",
    "SERVING_SAVE_DATA_PATH='gs://{}/{}/{}_score.csv'.format(FILE_BUCKET, SERVICE_TYPE, SERVICE_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc0bd98-1d58-491e-bc5e-bbac378c7bbf",
   "metadata": {},
   "source": [
    "### Import Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442904b-a40a-4ba8-b237-e9894ffd4f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download required component files to local\n",
    "prefix = f'{STACK_NAME}/{SERVING_PIPELINE_NAME_PATH}/components/'\n",
    "dl_dir = 'components/'\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(RESOURCE_BUCKET)\n",
    "blobs = bucket.list_blobs(prefix=prefix)  # Get list of files\n",
    "for blob in blobs: # download each file that starts with \"prefix\" into \"dl_dir\"\n",
    "    if blob.name.endswith(\"/\"):\n",
    "        continue\n",
    "    file_split = blob.name.split(prefix)\n",
    "    file_path = f\"{dl_dir}{file_split[-1]}\"\n",
    "    directory = \"/\".join(file_path.split(\"/\")[0:-1])\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    blob.download_to_filename(file_path) \n",
    "\n",
    "# import main pipeline components\n",
    "from components.bq_create_dataset import bq_create_dataset\n",
    "from components.preprocess import preprocess\n",
    "from components.batch_prediction import batch_prediction\n",
    "from components.postprocess import postprocess\n",
    "from components.load_ml_model import load_ml_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff235c32-f418-408a-a571-6e1fadaa4972",
   "metadata": {},
   "source": [
    "### Import Pipeline Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79af8761-d160-49a9-8746-1631f9691e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download required component files to local\n",
    "prefix = f'{STACK_NAME}/{SERVING_PIPELINE_NAME_PATH}/utils/'\n",
    "dl_dir = 'utils/'\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(RESOURCE_BUCKET)\n",
    "blobs = bucket.list_blobs(prefix=prefix)  # Get list of files\n",
    "for blob in blobs: # download each file that starts with \"prefix\" into \"dl_dir\"\n",
    "    if blob.name.endswith(\"/\"):\n",
    "        continue\n",
    "    file_split = blob.name.split(prefix)\n",
    "    file_path = f\"{dl_dir}{file_split[-1]}\"\n",
    "    directory = \"/\".join(file_path.split(\"/\")[0:-1])\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    blob.download_to_filename(file_path) \n",
    "\n",
    "from utils.monitoring import generate_data_stats\n",
    "from utils.monitoring import validate_stats \n",
    "from utils.monitoring import visualize_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315d454-416f-4eed-b12d-1acd5aed1207",
   "metadata": {},
   "source": [
    "### Date Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfad310-a482-4285-aa31-6d9113008be6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scoringDate = date(2023, 11, 1)  \n",
    "scoringDate = date.today() - relativedelta(days=3)\n",
    "\n",
    "# training dates\n",
    "SCORE_DATE = scoringDate.strftime('%Y%m%d')  # date.today().strftime('%Y%m%d')\n",
    "SCORE_DATE_DASH = scoringDate.strftime('%Y-%m-%d')\n",
    "SCORE_DATE_MINUS_6_MOS_DASH = ((scoringDate - relativedelta(months=6)).replace(day=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_START_DASH = (scoringDate.replace(day=1) - timedelta(days=1)).replace(day=1).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_END_DASH = ((scoringDate.replace(day=1)) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "#revert these changes after 2023-05-30\n",
    "PROMO_EXPIRY_START = (scoringDate.replace(day=1) + relativedelta(months=3)).replace(day=1).strftime('%Y-%m-%d')\n",
    "PROMO_EXPIRY_END = (scoringDate.replace(day=1) + relativedelta(months=4)).replace(day=1).strftime('%Y-%m-%d')\n",
    "\n",
    "SCORE_DATE_DELTA = 0\n",
    "SCORE_DATE_VAL_DELTA = 0\n",
    "TICKET_DATE_WINDOW = 30  # Days of ticket data to be queried\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81ea34e-5958-4841-809e-e29d00f0b154",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scoringDate) \n",
    "print(SCORE_DATE)\n",
    "print(PROMO_EXPIRY_START)\n",
    "print(PROMO_EXPIRY_END)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e57c86-786e-4b85-af8c-f1a4c46110e3",
   "metadata": {},
   "source": [
    "### Model Monitoring Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880d8484-2ba7-444a-9dae-75d79a60c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_MONITORING_STACK_NAME = 'util'\n",
    "MODEL_MONITORING_PATH = 'pipeline_utils'\n",
    "TRAINING_PIPELINE_NAME_PATH = 'telus_postpaid_churn_model/training_pipeline'\n",
    "SERVING_PIPELINE_NAME_PATH = 'telus_postpaid_churn_model/serving_pipeline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca81df-5764-4b74-9a2e-da87fe7a4be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = date.today()\n",
    "\n",
    "# BQ table where training data is stored\n",
    "INPUT_SERVING_DATA_CSV_PATH = 'gs://{}/{}/{}_score.csv'.format(FILE_BUCKET, SERVICE_TYPE, SERVICE_TYPE)\n",
    "\n",
    "# BQ dataset where monitoring stats are stored\n",
    "MODEL_MONITORING_DATASET = \"telus_postpaid_churn\"\n",
    "\n",
    "# Paths to statistics artifacts in GCS\n",
    "SERVING_STATISTICS_OUTPUT_PATH = f\"gs://{FILE_BUCKET}/{STACK_NAME}/statistics/serving_statistics_{today}\" \n",
    "ANOMALIES_PATH = f\"gs://{FILE_BUCKET}/{STACK_NAME}/anomalies/anomalies_{today}\"\n",
    "\n",
    "# stats prefix\n",
    "TRAINING_STATS_PREFIX = f\"{STACK_NAME}/statistics/training_statistics\"\n",
    "SERVING_STATS_PREFIX = f\"{STACK_NAME}/statistics/serving_statistics\"\n",
    "SCHEMA_PREFIX = f\"{MODEL_NAME}/schemas/training_stats_schema\"\n",
    "\n",
    "# Paths to schemas in GCS\n",
    "# SCHEMA_PATH = f'gs://{FILE_BUCKET}/{MODEL_NAME}/schemas/training_stats_schema_{today}'\n",
    "# SATISTICS_PATH = f'gs://{FILE_BUCKET}/{MODEL_NAME}/schemas/training_statistics_{today}'\n",
    "\n",
    "# Thresholds for anomalies\n",
    "ANOMALY_THRESHOLDS_PATH = f\"{STACK_NAME}/training_statistics/anomaly_thresholds.json\" #same path structure as utils reading from bucket\n",
    "\n",
    "# Filters for predictions monitoring\n",
    "DATE_COL = 'partition_date'\n",
    "DATE_FILTER = str(today)\n",
    "TABLE_BLOCK_SAMPLE = 1 # no sampling\n",
    "ROW_SAMPLE = 1 # no sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5598dd35-8160-4c7b-a1f1-f7135cf5474a",
   "metadata": {},
   "source": [
    "### Check if existing stats files for validation of serving data and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d2ea6e-453c-4b50-b9e0-58d7c372b270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_file_by_date(storage_client,\n",
    "                        bucket_name,\n",
    "                        prefix):\n",
    "    \n",
    "    blob_updated_arr = []\n",
    "    blob_name_arr = []\n",
    "    blobs = storage_client.list_blobs(bucket_or_name=bucket_name, \n",
    "                                      prefix=prefix)\n",
    "    # Get all files in bucket that match the prefix and append to list\n",
    "    for blob in blobs:\n",
    "        blob_updated_arr.append(blob.updated)\n",
    "        blob_name_arr.append(blob.name)\n",
    "    \n",
    "    # if list is greater than 0 then files with the prefix in the bucket exists\n",
    "    # retrieve the filename of the latest updated file\n",
    "    if len(blob_name_arr) > 0:\n",
    "        max_date_index = np.argmax(blob_updated_arr) # blob_updated_arr is a list of datetime.datetime object \n",
    "        max_name = blob_name_arr[max_date_index] # retrieves the name including the path defined by prefix based on index\n",
    "        latest_file_path = f\"gs://{FILE_BUCKET}/{max_name}\"\n",
    "        \n",
    "        return latest_file_path, len(blob_name_arr), max(blob_updated_arr).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    else:\n",
    "        return '', 0, ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badadeae-1cbc-4d3f-bc0d-b5f07bd7bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging \n",
    "\n",
    "# check if serving and/or prediction stats available to compare to\n",
    "previous_serving_stats_ind = False\n",
    "previous_pred_stats_ind = False\n",
    "\n",
    "storage_client = storage.Client()\n",
    "\n",
    "# TRAINING_STATS_PREFIX = f\"{STACK_NAME}/statistics/training_statistics\"\n",
    "training_stats_path, num_training_stats_files, training_max_date = get_latest_file_by_date(storage_client=storage_client,\n",
    "                                                                                bucket_name=FILE_BUCKET,\n",
    "                                                                                prefix=TRAINING_STATS_PREFIX)\n",
    "\n",
    "# SERVING_STATS_PREFIX = f\"{STACK_NAME}/statistics/serving_statistics\"\n",
    "previous_pred_stats_path, num_pred_stats_files, pred_max_date = get_latest_file_by_date(storage_client=storage_client,\n",
    "                                                                                    bucket_name=FILE_BUCKET,\n",
    "                                                                                    prefix=SERVING_STATS_PREFIX)\n",
    "\n",
    "# Get latest SCHEMA_PATH file\n",
    "# Generated from training pipline\n",
    "SCHEMA_PATH, num_schema_files, schema_max_date = get_latest_file_by_date(storage_client=storage_client,\n",
    "                                                        bucket_name=FILE_BUCKET,\n",
    "                                                        prefix=SCHEMA_PREFIX)\n",
    "\n",
    "# Determine if previous prediction                                                                                                                                   \n",
    "if num_pred_stats_files > 0:\n",
    "    previous_pred_stats_ind = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdcc135-0580-4e4d-bebe-a72da778dc9f",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb70ef8-0fad-4590-ba5b-8090eeeb43c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "from kfp.v2 import compiler\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "@dsl.pipeline(\n",
    "    name=SERVING_PIPELINE_NAME, \n",
    "    description=SERVING_PIPELINE_DESCRIPTION\n",
    "    )\n",
    "def pipeline(\n",
    "        project_id: str = PROJECT_ID,\n",
    "        region: str = REGION,\n",
    "        resource_bucket: str = RESOURCE_BUCKET,\n",
    "        file_bucket: str = FILE_BUCKET\n",
    "    ):\n",
    "\n",
    "    #### this code block is only for a personal workbench \n",
    "    \n",
    "    import google.oauth2.credentials\n",
    "    token = !gcloud auth print-access-token\n",
    "    token_str = token[0]\n",
    "    \n",
    "    #### the end\n",
    "    \n",
    "    from datetime import datetime\n",
    "    update_ts = datetime.now()\n",
    "    update_ts_str = update_ts.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # ----- create training set --------\n",
    "    bq_create_scoring_dataset_op = bq_create_dataset(score_date=SCORE_DATE_DASH\n",
    "                                  , score_date_delta=SCORE_DATE_DELTA\n",
    "                                  , project_id=PROJECT_ID\n",
    "                                  , dataset_id=DATASET_ID\n",
    "                                  , region=REGION\n",
    "                                  , environment='serving'\n",
    "                                  , token=token_str\n",
    "                                  )\n",
    "    \n",
    "    # ----- preprocessing train data --------\n",
    "    preprocess_scoring_op = preprocess(pipeline_dataset=SERVING_DATASET_TABLE_NAME\n",
    "                                    , save_data_path=SERVING_SAVE_DATA_PATH\n",
    "                                    , project_id=PROJECT_ID\n",
    "                                    , dataset_id=DATASET_ID\n",
    "                                    , score_date_dash=SCORE_DATE_DASH\n",
    "                                    )\n",
    "\n",
    "    preprocess_scoring_op.set_memory_limit('32G')\n",
    "    preprocess_scoring_op.set_cpu_limit('4')\n",
    "    \n",
    "    load_ml_model_op = load_ml_model(project_id = PROJECT_ID\n",
    "                                    , region = REGION\n",
    "                                    , model_name = MODEL_NAME\n",
    "                                    )\n",
    "\n",
    "    load_ml_model_op.set_memory_limit('32G')\n",
    "    load_ml_model_op.set_cpu_limit('4')\n",
    "\n",
    "    batch_prediction_op = batch_prediction(project_id=PROJECT_ID\n",
    "                                        , dataset_id=DATASET_ID\n",
    "                                        , table_id=PROC_SERVING_DATASET_TABLE_NAME\n",
    "                                        , file_bucket=FILE_BUCKET\n",
    "                                        , save_data_path=SERVING_SAVE_DATA_PATH\n",
    "                                        , service_type=SERVICE_TYPE\n",
    "                                        , score_date_dash=SCORE_DATE_DASH\n",
    "                                        , score_table=SCORE_TABLE_NAME\n",
    "                                        , temp_table=TEMP_TABLE\n",
    "                                        , model_uri=load_ml_model_op.outputs['model_uri']\n",
    "                                        )\n",
    "    \n",
    "    batch_prediction_op.set_memory_limit('32G')\n",
    "    batch_prediction_op.set_cpu_limit('4')\n",
    "    \n",
    "    postprocessing_op = postprocess(project_id=PROJECT_ID\n",
    "                                    , file_bucket=FILE_BUCKET\n",
    "                                    , dataset_id=DATASET_ID\n",
    "                                    , service_type=SERVICE_TYPE\n",
    "                                    , score_date_dash=SCORE_DATE_DASH\n",
    "                                    , temp_table=TEMP_TABLE\n",
    "                                    , ucar_score_table=UCAR_SCORE_TABLE\n",
    "                                    , token=token_str\n",
    "                                    ) \n",
    "    \n",
    "    postprocessing_op.set_memory_limit('32G')\n",
    "    postprocessing_op.set_cpu_limit('4')\n",
    "\n",
    "#     # generate statistics\n",
    "#     generate_serving_data_stats_op = generate_data_stats(project_id=PROJECT_ID\n",
    "#                                                         , bucket_nm=FILE_BUCKET\n",
    "#                                                         , data_type = 'csv'\n",
    "#                                                         , op_type = 'serving'\n",
    "#                                                         , model_nm = MODEL_NAME\n",
    "#                                                         , update_ts = update_ts_str\n",
    "#                                                         , token = token_str\n",
    "#                                                         , dest_stats_gcs_path = SERVING_STATISTICS_OUTPUT_PATH\n",
    "#                                                         , src_csv_path = INPUT_SERVING_DATA_CSV_PATH\n",
    "#                                                         , table_block_sample = TABLE_BLOCK_SAMPLE\n",
    "#                                                         , row_sample = ROW_SAMPLE\n",
    "#                                                         , in_bq_ind = True\n",
    "#                                                         , dest_stats_bq_dataset = MODEL_MONITORING_DATASET\n",
    "#                                                         , pass_through_features = ['ban']\n",
    "#                                                         ).set_display_name(\"generate-serving-data-statistics\")\n",
    "\n",
    "#     generate_serving_data_stats_op.set_memory_limit('32G')\n",
    "#     generate_serving_data_stats_op.set_cpu_limit('4')\n",
    "    \n",
    "#     # compare to training data\n",
    "#     # visualize serving statistics\n",
    "#     visualize_serving_stats_op = visualize_stats(statistics = generate_serving_data_stats_op.outputs[\"statistics\"]\n",
    "#                                                 , stats_nm=f\"Serving Statistics {update_ts_str}\"\n",
    "#                                                 , base_stats_path = training_stats_path # This should be previous_serving_stats_path\n",
    "#                                                 , base_stats_nm=f\"Training Statistics {training_max_date}\"\n",
    "#                                                 , op_type = 'serving'\n",
    "#                                                 ).set_display_name(\"visualize-serving-data-statistics\")\n",
    "    \n",
    "#     # validate stats + find anomalies by comparing with training stats\n",
    "#     validate_serving_stats_op = validate_stats(project_id = PROJECT_ID\n",
    "#                                             , bucket_nm = FILE_BUCKET # anomaly_thresholds.json is stored in resources bucket via github\n",
    "#                                             , model_nm = MODEL_NAME\n",
    "#                                             , update_ts = update_ts_str\n",
    "#                                             , op_type = 'serving'\n",
    "#                                             , validation_type = 'skew'\n",
    "#                                             , dest_anomalies_bq_dataset = MODEL_MONITORING_DATASET #Change this to dataset of model monitoring\n",
    "#                                             , statistics = generate_serving_data_stats_op.outputs[\"statistics\"]\n",
    "#                                             , base_stats_path = training_stats_path\n",
    "#                                             , src_schema_path = SCHEMA_PATH # ok\n",
    "#                                             , dest_anomalies_gcs_path = ANOMALIES_PATH # ok \n",
    "#                                             , src_anomaly_thresholds_path = ANOMALY_THRESHOLDS_PATH\n",
    "#                                             , in_bq_ind = True\n",
    "#                                             )\n",
    "    \n",
    "    preprocess_scoring_op.after(bq_create_scoring_dataset_op)\n",
    "    \n",
    "    load_ml_model_op.after(preprocess_scoring_op) \n",
    "    batch_prediction_op.after(load_ml_model_op)\n",
    "    postprocessing_op.after(batch_prediction_op) \n",
    "    \n",
    "#     generate_serving_data_stats_op.after(preprocess_scoring_op)\n",
    "#     visualize_serving_stats_op.after(generate_serving_data_stats_op)\n",
    "#     validate_serving_stats_op.after(visualize_serving_stats_op)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a51c8-4c37-4392-a21a-c4c3279d36d4",
   "metadata": {},
   "source": [
    "### Run the Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27465a63-5c3b-4e96-9c08-f9cc5dafde90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kfp.v2 import compiler\n",
    "# from google.cloud.aiplatform import pipeline_jobs\n",
    "# import json\n",
    "\n",
    "# compiler.Compiler().compile(\n",
    "#    pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    "# )\n",
    "\n",
    "# job = pipeline_jobs.PipelineJob(\n",
    "#                                    display_name=SERVING_PIPELINE_NAME,\n",
    "#                                    template_path=\"pipeline.json\",\n",
    "#                                    location=REGION,\n",
    "#                                    enable_caching=False,\n",
    "#                                    pipeline_root = PIPELINE_ROOT\n",
    "#                                 )\n",
    "# job.run(service_account = f\"bilayer-sa@{PROJECT_ID}.iam.gserviceaccount.com\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2fb80d-72f1-4cbc-a2f4-03601e1699a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.oauth2.credentials\n",
    "import json\n",
    "\n",
    "token = !gcloud auth print-access-token\n",
    "CREDENTIALS = google.oauth2.credentials.Credentials(token[0])\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "   pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    ")\n",
    "\n",
    "job = pipeline_jobs.PipelineJob(\n",
    "   display_name=SERVING_PIPELINE_NAME,\n",
    "   template_path=\"pipeline.json\",\n",
    "   credentials = CREDENTIALS,\n",
    "   pipeline_root = PIPELINE_ROOT,\n",
    "   location=REGION,\n",
    "   enable_caching=False # I encourage you to enable caching when testing as it will reduce resource use\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c98ed-3ebc-459d-b641-feef2e01b5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e2b5a5-9674-424b-b1bf-6e986239c082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2a6305-6afc-4a4e-9a4c-9ca2cf23095e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55a1299-0319-44a5-b84c-12116b36fdab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
