name: Generate data stats
description: 'Inputs:'
inputs:
- {name: project_id, type: String}
- {name: data_type, type: String}
- {name: op_type, type: String}
- {name: model_nm, type: String}
- {name: update_ts, type: String}
- {name: token, type: String}
- {name: bucket_nm, type: String, default: '', optional: true}
- {name: model_type, type: String, default: supervised, optional: true}
- {name: date_col, type: String, default: '', optional: true}
- {name: date_filter, type: String, default: '', optional: true}
- {name: table_block_sample, type: Float, default: '1', optional: true}
- {name: row_sample, type: Float, default: '1', optional: true}
- name: in_bq_ind
  type: Boolean
  default: "True"
  optional: true
- {name: src_bq_path, type: String, default: '', optional: true}
- {name: src_csv_path, type: String, default: '', optional: true}
- {name: dest_stats_bq_dataset, type: String, default: '', optional: true}
- {name: dest_schema_path, type: String, default: '', optional: true}
- {name: dest_stats_gcs_path, type: String, default: '', optional: true}
- {name: pass_through_features, type: JsonArray, default: '[]', optional: true}
- {name: training_target_col, type: String, default: '', optional: true}
- {name: pred_cols, type: JsonArray, default: '[]', optional: true}
outputs:
- {name: statistics, type: Artifact}
implementation:
  container:
    image: northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-tfdv-slim:1.0.0
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.18' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
      \ import *\n\ndef generate_data_stats(\n    project_id: str,\n    data_type:\
      \ str,\n    op_type: str,\n    model_nm: str,\n    update_ts: str,\n    token:\
      \ str, \n    statistics: Output[Artifact],\n    bucket_nm: str = '',\n    model_type:\
      \ str = 'supervised',\n    date_col: str = '',\n    date_filter: str = '',\n\
      \    table_block_sample: float = 1,\n    row_sample: float = 1,\n    in_bq_ind:\
      \ bool = True,\n    src_bq_path: str = '',\n    src_csv_path: str = '',\n  \
      \  dest_stats_bq_dataset: str = '',\n    dest_schema_path: str = '',\n    dest_stats_gcs_path:\
      \ str = '',\n    pass_through_features: list = [],\n    training_target_col:\
      \ str = \"\",\n    pred_cols: list = []\n):\n    '''\n    Inputs:\n        -\
      \ project_id: project id\n        - data_type: bigquery or csv\n        - op_type:\
      \ training or serving or predictions\n        - model_nm: name of model\n  \
      \      - update_ts: time when pipeline was run\n        - bucket_nm: name of\
      \ bucket where pred schema is or will be stored (Optional: for predictions)\n\
      \        - model_type: supervised or unsupervised. unsupervised models will\
      \ create new schema as required.\n        - date_col: (Optional: name of column\
      \ for filtering data by date)\n        - date_filter: YYYY-MM-DD (Optional:\
      \ query only specific date for stats)\n        - table_block_sample: sample\
      \ of data blocks to be loaded (only if bq). Reduces query size for large datasets.\
      \ 0.1 for 10% (default is 1)\n        - row_sample: sample of rows to be loaded\
      \ (only if bq). If using table_block_sample as well, this will be the % of rows\
      \ from the selected blocks.\n          0.1 for 10% (default is 1)\n        -\
      \ in_bq_ind: True or False (Optional: store stats in bigquery)\n        - src_bq_path:\
      \ bigquery path to data that will be used to generate stats (if data_type is\
      \ bigquery)\n        - src_csv_path: path to csv file that will be used to generate\
      \ stats (if data_type is csv)\n        - dest_stats_bq_dataset: bq dataset where\
      \ monitoring stats will be stored (only if in_bq_path set to True)\n       \
      \ - dest_schema_path: gcs path to where schema will be stored (optional: only\
      \ for training stats)\n        - dest_stats_gcs_path: gcs path to where stats\
      \ should be stored\n        - pass_through_features: list of feature columns\
      \ not used for training e.g. keys and ids \n        - training_target_col: target\
      \ column name from training data (Optional: set to be excluded from serving\
      \ data)\n        - pred_cols: column names where predictions are stored\n\n\
      \    Outputs:\n        - statistics\n    '''\n\n    import tensorflow_data_validation\
      \ as tfdv\n    from apache_beam.options.pipeline_options import (\n        PipelineOptions\n\
      \    )\n    from google.cloud import storage\n    from google.cloud import bigquery\n\
      \    from datetime import datetime\n    import json\n    import pandas as pd\n\
      \    import numpy as np\n    import google.oauth2.credentials\n\n    print('msg1:\
      \ all libraries imported')\n\n    # convert timestamp to datetime\n    update_ts\
      \ = datetime.strptime(update_ts, '%Y-%m-%d %H:%M:%S')\n\n    statistics.uri\
      \ = dest_stats_gcs_path\n\n    pipeline_options = PipelineOptions()\n    stats_options\
      \ = tfdv.StatsOptions()\n\n    # import from csv in GCS\n    if data_type ==\
      \ 'csv':\n        df = pd.read_csv(src_csv_path)\n\n        if op_type == 'predictions':\n\
      \            df = df[pred_cols]\n\n    # import from Big Query\n    elif data_type\
      \ == 'bigquery':\n\n        percent_table_sample = table_block_sample * 100\n\
      \n        if op_type == 'predictions':\n            # query data stored in BQ\
      \ and load into pandas dataframe\n            build_df = '''SELECT '''\n   \
      \         for pred_col in pred_cols:\n                build_df = build_df +\
      \ f\"{pred_col}, \"\n            build_df = build_df + '''FROM `{bq_table}`\
      \ TABLESAMPLE SYSTEM ({percent_table_sample} PERCENT)\n                    \
      \    WHERE rand() < {row_sample} \n                    '''.format(bq_table =\
      \ src_bq_path,\n                               percent_table_sample = percent_table_sample,\
      \ \n                               row_sample = row_sample)\n        else:\n\
      \            # query data stored in BQ and load into pandas dataframe\n    \
      \        build_df = '''\n            SELECT * FROM `{bq_table}` TABLESAMPLE\
      \ SYSTEM ({percent_table_sample} PERCENT)\n                        WHERE rand()\
      \ < {row_sample} \n                    '''.format(bq_table = src_bq_path,\n\
      \                               percent_table_sample = percent_table_sample,\
      \ \n                               row_sample = row_sample)\n\n        if len(date_filter)\
      \ > 0:\n            build_df = build_df + ''' AND {date_col}=\"{date_filter}\"\
      \n            '''.format(date_col=date_col, date_filter=date_filter)\n\n   \
      \     job_config = bigquery.QueryJobConfig()\n        df = client.query(build_df,\
      \ job_config=job_config).to_dataframe()\n\n        # check this: if dataframe\
      \ is empty, return error\n        if (df.size == 0):\n            raise TypeError(\"\
      Dataframe is empty, cannot generate statistics.\")\n\n    else:\n        print(\"\
      This data type is not supported. Please use a csv or Big Query table, otherwise\
      \ create your own custom component\")\n\n    print('msg2: df created')\n\n \
      \   # drop pass-through features\n    if len(pass_through_features) > 0:\n \
      \       df = df.drop(columns=pass_through_features)\n\n    stats = tfdv.generate_statistics_from_dataframe(\n\
      \        dataframe=df,\n        stats_options=stats_options,\n        n_jobs=1\n\
      \    )\n    tfdv.write_stats_text(\n        stats=stats, \n        output_path=dest_stats_gcs_path\n\
      \    )\n\n    # generate schema for training data\n    if op_type == 'training':\n\
      \        schema = tfdv.infer_schema(stats)\n\n        if 'TRAINING' not in schema.default_environment:\n\
      \                schema.default_environment.append('TRAINING')\n\n        #\
      \ set training target column for supervised learning models (will not be in\
      \ serving data)\n        if model_type == 'supervised':\n            if 'SERVING'\
      \ not in schema.default_environment:\n                schema.default_environment.append('SERVING')\n\
      \n            # check if training_target_col specified\n            if len(training_target_col)\
      \ > 0:\n                # specify that target/label is not in SERVING environment\n\
      \                if 'SERVING' not in tfdv.get_feature(schema, training_target_col).not_in_environment:\n\
      \                    tfdv.get_feature(\n                        schema, training_target_col).not_in_environment.append('SERVING')\
      \ \n            else:\n                print(\"No training target column specified\"\
      )\n\n        tfdv.write_schema_text(\n            schema=schema, output_path=dest_schema_path\n\
      \        )\n\n    print('msg3: stats generated')\n\n    if (op_type == 'predictions')\
      \ | (model_type == 'unsupervised'):\n        # if schema does not exist create\
      \ new one for predictions or unsupervised model\n        storage_client = storage.Client()\n\
      \        bucket = storage_client.bucket(bucket_nm)\n        blob = bucket.blob(dest_schema_path.split(f'gs://{bucket_nm}/')[1])\n\
      \n        if not blob.exists():\n            # generate schema for predictions\
      \ data or unsupervised learning model\n            schema = tfdv.infer_schema(stats)\n\
      \            tfdv.write_schema_text(\n                schema=schema, output_path=dest_schema_path\n\
      \            )\n\n    df_stats = pd.DataFrame(columns=['model_nm',\n       \
      \                              'update_ts',\n                              \
      \       'op_type',\n                                     'feature_nm',\n   \
      \                                  'feature_type',\n                       \
      \              'num_non_missing',\n                                     'min_num_values',\n\
      \                                     'max_num_values',\n                  \
      \                   'avg_num_values',\n                                    \
      \ 'tot_num_values',\n                                     'mean',\n        \
      \                             'std_dev',\n                                 \
      \    'num_zeros',\n                                     'min_val',\n       \
      \                              'median',\n                                 \
      \    'max_val',\n                                     'unique_values',\n   \
      \                                  'top_value_freq',\n                     \
      \                'avg_length'])\n\n    print('msg4: save stats in data monitoring\
      \ table')\n\n    # OPTIONAL: save stats in data monitoring table\n    if in_bq_ind\
      \ == True:\n\n        for feature in stats.datasets[0].features:\n         \
      \   feature_nm = feature.path.step[0]\n            if (feature.type == 0):\n\
      \                feature_type = 'INT'\n            elif (feature.type == 1):\n\
      \                feature_type = 'FLOAT'\n            elif (feature.type == 2):\n\
      \                feature_type = 'STRING'\n            else:\n              \
      \  featue_type = 'UNKNOWN'\n\n            if (feature_type == 'INT') | (feature_type\
      \ == 'FLOAT'):\n                num_non_missing = feature.num_stats.common_stats.num_non_missing\n\
      \                min_num_values = feature.num_stats.common_stats.min_num_values\n\
      \                max_num_values = feature.num_stats.common_stats.max_num_values\n\
      \                avg_num_values = feature.num_stats.common_stats.avg_num_values\n\
      \                tot_num_values = feature.num_stats.common_stats.tot_num_values\n\
      \n                mean = feature.num_stats.mean\n                std_dev = feature.num_stats.std_dev\n\
      \                num_zeros = feature.num_stats.num_zeros\n                min_val\
      \ = feature.num_stats.min\n                median = feature.num_stats.median\n\
      \                max_val = feature.num_stats.max\n\n                df_stats.loc[len(df_stats.index)]\
      \ = pd.Series({\n                    'model_nm': model_nm,\n               \
      \     'update_ts': update_ts,\n                    'op_type': op_type,\n   \
      \                 'feature_nm': feature_nm,\n                    'feature_type':\
      \ feature_type,\n                    'num_non_missing': num_non_missing,\n \
      \                   'min_num_values': min_num_values,\n                    'max_num_values':\
      \ max_num_values,\n                    'avg_num_values': avg_num_values,\n \
      \                   'tot_num_values': tot_num_values,\n                    'mean':\
      \ mean,\n                    'std_dev': std_dev,\n                    'num_zeros':\
      \ num_zeros,\n                    'min_val': min_val,\n                    'median':\
      \ median,\n                    'max_val': max_val\n                })\n\n  \
      \          elif feature_type == 'STRING':\n                num_non_missing =\
      \ feature.string_stats.common_stats.num_non_missing\n                min_num_values\
      \ = feature.string_stats.common_stats.min_num_values\n                max_num_values\
      \ = feature.string_stats.common_stats.max_num_values\n                avg_num_values\
      \ = feature.string_stats.common_stats.avg_num_values\n                tot_num_values\
      \ = feature.string_stats.common_stats.tot_num_values\n\n                unique_values\
      \ = feature.string_stats.unique\n\n                # create dict of top values\
      \ to be stored in BQ as record\n                top_values = feature.string_stats.top_values\n\
      \                top_value_freq_arr = []\n                for i in range(len(top_values)):\n\
      \                    top_value = top_values[i]\n                    value =\
      \ top_value.value\n                    freq = top_value.frequency\n        \
      \            top_value_dict = {'value': value, 'frequency': freq}\n        \
      \            top_value_freq_arr.append(top_value_dict)\n\n                avg_length\
      \ = feature.string_stats.avg_length\n\n                df_stats.loc[len(df_stats.index)]\
      \ = pd.Series({\n                    'model_nm': model_nm,\n               \
      \     'update_ts': update_ts,\n                    'op_type': op_type,\n   \
      \                 'feature_nm': feature_nm,\n                    'feature_type':\
      \ feature_type,\n                    'num_non_missing': num_non_missing,\n \
      \                   'min_num_values': min_num_values,\n                    'max_num_values':\
      \ max_num_values,\n                    'avg_num_values': avg_num_values,\n \
      \                   'tot_num_values': tot_num_values,\n                    'unique_values':\
      \ unique_values,\n                    'top_value_freq': top_value_freq_arr,\n\
      \                    'avg_length': avg_length\n                })\n\n      \
      \  # set null records to None value\n        df_stats['top_value_freq'] = df_stats['top_value_freq'].fillna(np.nan).replace([\n\
      \            np.nan], [None])\n\n#         #### For wb\n#         CREDENTIALS\
      \ = google.oauth2.credentials.Credentials(token)\n\n#         client = bigquery.Client(project=project_id,\
      \ credentials=CREDENTIALS)\n\n        #### For prod \n        client = bigquery.Client(project=project_id)\n\
      \n        job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_APPEND\"\
      ,\n                                            schema=[\n                  \
      \                              bigquery.SchemaField(\n                     \
      \                               \"model_nm\", \"STRING\"),\n               \
      \                                 bigquery.SchemaField(\n                  \
      \                                  \"update_ts\", \"TIMESTAMP\"),\n        \
      \                                        bigquery.SchemaField(\n           \
      \                                         \"op_type\", \"STRING\"),\n      \
      \                                          bigquery.SchemaField(\n         \
      \                                           \"feature_nm\", \"STRING\"),\n \
      \                                               bigquery.SchemaField(\n    \
      \                                                \"feature_type\", \"STRING\"\
      ),\n                                                bigquery.SchemaField(\n\
      \                                                    \"num_non_missing\", \"\
      INTEGER\"),\n                                                bigquery.SchemaField(\n\
      \                                                    \"min_num_values\", \"\
      INTEGER\"),\n                                                bigquery.SchemaField(\n\
      \                                                    \"max_num_values\", \"\
      INTEGER\"),\n                                                bigquery.SchemaField(\n\
      \                                                    \"avg_num_values\", \"\
      FLOAT\"),\n                                                bigquery.SchemaField(\n\
      \                                                    \"tot_num_values\", \"\
      INTEGER\"),\n                                                bigquery.SchemaField(\n\
      \                                                    \"mean\", \"FLOAT\"),\n\
      \                                                bigquery.SchemaField(\n   \
      \                                                 \"std_dev\", \"FLOAT\"),\n\
      \                                                bigquery.SchemaField(\n   \
      \                                                 \"num_zeros\", \"INTEGER\"\
      ),\n                                                bigquery.SchemaField(\n\
      \                                                    \"min_val\", \"FLOAT\"\
      ),\n                                                bigquery.SchemaField(\n\
      \                                                    \"median\", \"FLOAT\"),\n\
      \                                                bigquery.SchemaField(\n   \
      \                                                 \"max_val\", \"FLOAT\"),\n\
      \                                                bigquery.SchemaField(\n   \
      \                                                 \"unique_values\", \"FLOAT\"\
      ),\n                                                bigquery.SchemaField(\"\
      top_value_freq\", \"RECORD\", mode=\"REPEATED\", fields=[\n                \
      \                                    bigquery.SchemaField(\"frequency\", \"\
      FLOAT\"), bigquery.SchemaField(\"value\", \"STRING\")]),\n                 \
      \                               bigquery.SchemaField(\n                    \
      \                                \"avg_length\", \"FLOAT\")\n              \
      \                              ],)  # create new table or append if already\
      \ exists\n\n        data_stats_table = f\"{project_id}.{dest_stats_bq_dataset}.bq_data_monitoring\"\
      \n\n        print(f'msg5: {data_stats_table}')\n\n        job = client.load_table_from_dataframe(#\
      \ Make an API request.\n            df_stats, data_stats_table, job_config=job_config\n\
      \        )\n        job.result()\n        table = client.get_table(data_stats_table)\
      \  # Make an API request.\n        print(\n            \"Loaded {} rows and\
      \ {} columns to {}\".format(\n                table.num_rows, len(table.schema),\
      \ data_stats_table\n            )\n        )\n\n"
    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - generate_data_stats
