{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c150da3-6e1e-4f02-a778-bb3b12af9054",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b559b7e6-fe51-49cc-8e9c-832380843832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, ClassificationMetrics,\n",
    "                        Metrics, component)\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import google\n",
    "from google.oauth2 import credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2.service_account import Credentials\n",
    "from google.cloud import storage\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components.v1.batch_predict_job import \\\n",
    "    ModelBatchPredictOp as batch_prediction_op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558ff533-70c8-4421-813e-c567d6bc496b",
   "metadata": {},
   "source": [
    "### YAML Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f70d721-76e7-4c2e-8153-88b5bbe8ee47",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID =  ''\n",
    "DATASET_ID = ''\n",
    "RESOURCE_BUCKET = ''\n",
    "FILE_BUCKET = ''\n",
    "REGION = ''\n",
    "MODEL_ID = ''\n",
    "MODEL_NAME = ''\n",
    "PIPELINE_PATH = 'vertex_pipelines/hs_nba_prospects/training_pipeline'\n",
    "HS_NBA_UTILS_PATH =  'vertex_pipelines/hs_nba_utils/notebook'\n",
    "MODEL_TYPE='acquisition'\n",
    "LOAD_SQL=''\n",
    "TRAIN_CSV='' \n",
    "SAVE_FILE_NAME=''\n",
    "STATS_FILE_NAME=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb9c0d1-697a-4e34-92d5-125c106d6c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID =  'divg-groovyhoon-pr-d2eab4'\n",
    "DATASET_ID = 'nba_product_reco_prospects'\n",
    "RESOURCE_BUCKET = 'divg-groovyhoon-pr-d2eab4-default'\n",
    "FILE_BUCKET = 'divg-groovyhoon-pr-d2eab4-default'\n",
    "REGION = ''\n",
    "MODEL_ID = ''\n",
    "MODEL_NAME = 'nba_product_reco_prospects'\n",
    "PIPELINE_PATH = 'vertex_pipelines/hs_nba_prospects/training_pipeline'\n",
    "HS_NBA_UTILS_PATH =  'vertex_pipelines/hs_nba_utils/notebook'\n",
    "MODEL_TYPE='acquisition'\n",
    "LOAD_SQL='load_train_data.sql'\n",
    "TRAIN_CSV='training_dataset.csv' \n",
    "SAVE_FILE_NAME='df_val_exp.csv'\n",
    "STATS_FILE_NAME='df_stats.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eeed21-e12b-45ee-b1e4-d0c8f3843533",
   "metadata": {},
   "source": [
    "### Service Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65d1668-e1d8-4f58-958f-edc44d06a6b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SERVICE_TYPE = 'nba_product_reco_prospects'\n",
    "SERVICE_TYPE_NAME = 'nba-product-reco-prospects'\n",
    "TABLE_ID = 'master_features_set_prospect'\n",
    "REGION = \"northamerica-northeast1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2229e5d1-1362-40ed-aa7d-cb22e41e4960",
   "metadata": {},
   "source": [
    "### Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d628b-e759-4c9c-aab5-568c54721aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STACK_NAME = 'nba_product_reco_prospects'\n",
    "TRAINING_PIPELINE_NAME_PATH = f'{STACK_NAME}_model/training_pipeline'\n",
    "SERVING_PIPELINE_NAME_PATH = f'{STACK_NAME}_model/serving_pipeline'\n",
    "TRAINING_PIPELINE_NAME = f'{SERVICE_TYPE_NAME}-train-pipeline' # Same name as pulumi.yaml\n",
    "SERVING_PIPELINE_NAME = f'{SERVICE_TYPE_NAME}-serving-pipeline' # Same name as pulumi.yaml\n",
    "TRAINING_PIPELINE_DESCRIPTION = f'{SERVICE_TYPE_NAME}-train-pipeline'\n",
    "SERVING_PIPELINE_DESCRIPTION = f'{SERVICE_TYPE_NAME}-serving-pipeline'\n",
    "PIPELINE_ROOT = f\"gs://{FILE_BUCKET}\"\n",
    "REGION = \"northamerica-northeast1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc0bd98-1d58-491e-bc5e-bbac378c7bbf",
   "metadata": {},
   "source": [
    "### Import Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442904b-a40a-4ba8-b237-e9894ffd4f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download required component files to local\n",
    "prefix = f'{STACK_NAME}/{TRAINING_PIPELINE_NAME_PATH}/components/'\n",
    "dl_dir = 'components/'\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(RESOURCE_BUCKET)\n",
    "blobs = bucket.list_blobs(prefix=prefix)  # Get list of files\n",
    "for blob in blobs: # download each file that starts with \"prefix\" into \"dl_dir\"\n",
    "    if blob.name.endswith(\"/\"):\n",
    "        continue\n",
    "    file_split = blob.name.split(prefix)\n",
    "    file_path = f\"{dl_dir}{file_split[-1]}\"\n",
    "    directory = \"/\".join(file_path.split(\"/\")[0:-1])\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    blob.download_to_filename(file_path)\n",
    "\n",
    "# import main pipeline components\n",
    "from components.bq_create_dataset import bq_create_dataset\n",
    "from components.preprocess import preprocess\n",
    "from components.train_and_save_model import train_and_save_model\n",
    "from components.upload_model import upload_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb0c622-d24e-49f3-8b6b-6b9a60c147b6",
   "metadata": {},
   "source": [
    "### Import Pipeline Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c696f267-f8ab-43cf-ab7b-1b6f5e31c927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download required component files to local\n",
    "prefix = f'{STACK_NAME}/{TRAINING_PIPELINE_NAME_PATH}/utils/'\n",
    "dl_dir = 'utils/'\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(RESOURCE_BUCKET)\n",
    "blobs = bucket.list_blobs(prefix=prefix)  # Get list of files\n",
    "for blob in blobs: # download each file that starts with \"prefix\" into \"dl_dir\"\n",
    "    if blob.name.endswith(\"/\"):\n",
    "        continue\n",
    "    file_split = blob.name.split(prefix)\n",
    "    file_path = f\"{dl_dir}{file_split[-1]}\"\n",
    "    directory = \"/\".join(file_path.split(\"/\")[0:-1])\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    blob.download_to_filename(file_path)\n",
    "\n",
    "from utils.monitoring import generate_data_stats\n",
    "from utils.monitoring import validate_stats\n",
    "from utils.monitoring import visualize_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315d454-416f-4eed-b12d-1acd5aed1207",
   "metadata": {},
   "source": [
    "### Date Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfad310-a482-4285-aa31-6d9113008be6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training pipeline is to run on the 3rd of every month\n",
    "# change the training time window every 3rd of month to R12 months as of {last day of 2 months ago} e.g. on May 1st, training window to be 2023-04-01 to 2024-03-31\n",
    "\n",
    "# set training dates\n",
    "trainingDate = (date.today() - relativedelta(days=2))\n",
    "\n",
    "# training dates\n",
    "TRAIN_DATE = trainingDate.strftime('%Y%m%d')  # date.today().strftime('%Y%m%d')\n",
    "TRAIN_DATE_DASH = trainingDate.strftime('%Y-%m-%d')\n",
    "\n",
    "FROM_DATE = trainingDate.replace(day=1) + relativedelta(months=-13)\n",
    "TO_DATE = trainingDate.replace(day=1) + relativedelta(months=-1, days=-1)\n",
    "\n",
    "FROM_DATE = FROM_DATE.strftime('%Y-%m-%d')\n",
    "TO_DATE = TO_DATE.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09097ee1-13cd-409b-af0f-5c78f73aaa1c",
   "metadata": {},
   "source": [
    "### Model Monitoring Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f70c28-5070-45fc-bdf7-89112c5d2ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_MONITORING_STACK_NAME = 'util'\n",
    "# MODEL_MONITORING_PATH = 'pipeline_utils'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32fad49-8603-4190-9a66-aaf2dcdcf845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# today = date.today()\n",
    "\n",
    "# # BQ table where training data is stored\n",
    "# INPUT_TRAINING_DATA_TABLE_PATH = f\"{PROJECT_ID}.{DATASET_ID}.{TRAINING_DATASET_TABLE_NAME}\"\n",
    "# INPUT_TRAINING_DATA_CSV_PATH = 'gs://{}/{}/{}_train.csv'.format(FILE_BUCKET, SERVICE_TYPE, SERVICE_TYPE)                               \n",
    "\n",
    "# # BQ dataset where monitoring stats are stored\n",
    "# MODEL_MONITORING_DATASET = \"telus_postpaid_churn_model\"\n",
    "\n",
    "# # Paths to statistics artifacts in GCS\n",
    "# TRAINING_STATISTICS_PATH = f\"gs://{FILE_BUCKET}/{STACK_NAME}/{TRAINING_PIPELINE_NAME_PATH}/training_statistics/training_statistics_{today}\"\n",
    "# TRAINING_STATS_PREFIX = f\"{STACK_NAME}/statistics/training_statistics\"\n",
    "# TRAINING_STATISTICS_OUTPUT_PATH = f\"gs://{FILE_BUCKET}/{STACK_NAME}/statistics/training_statistics_{today}\" \n",
    "\n",
    "# ANOMALIES_PATH = f\"gs://{FILE_BUCKET}/{STACK_NAME}/anomalies/anomalies_{today}\"\n",
    "# PREDICTION_ANOMALIES_PATH = f\"gs://{FILE_BUCKET}/{STACK_NAME}/anomalies/prediction_anomalies_{today}\"\n",
    "# PREDICTION_STATS_PATH = f\"gs://{FILE_BUCKET}/{STACK_NAME}/statistics/prediction_statistics_{today}\"\n",
    "# PREDICTION_STATS_PREFIX = f\"{FILE_BUCKET}/statistics/prediction_statistics\"\n",
    "\n",
    "# # Paths to schemas in GCS\n",
    "# SCHEMA_PATH = f'gs://{FILE_BUCKET}/{STACK_NAME}/schemas/training_stats_schema_{today}'\n",
    "# # SATISTICS_PATH = f'gs://{FILE_BUCKET}/{STACK_NAME}/schemas/training_statistics_{today}'\n",
    "# # Thresholds for anomalies\n",
    "# ANOMALY_THRESHOLDS_PATH = f\"{STACK_NAME}/{TRAINING_PIPELINE_NAME_PATH}/training_statistics/anomaly_thresholds.json\" #same path structure as utils reading from bucket\n",
    "\n",
    "# # Filters for predictions monitoring\n",
    "# DATE_COL = 'partition_date'\n",
    "# DATE_FILTER = str(today)\n",
    "# TABLE_BLOCK_SAMPLE = 1 # no sampling\n",
    "# ROW_SAMPLE = 1 # no sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2eea99-a3f0-48ea-8962-115edcaf26d9",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5e82db-20c8-4480-bb48-b44d2ce005ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "from kfp.v2 import compiler\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "@dsl.pipeline(\n",
    "    name=TRAINING_PIPELINE_NAME, \n",
    "    description=TRAINING_PIPELINE_DESCRIPTION\n",
    "    )\n",
    "def pipeline(\n",
    "        project_id: str = PROJECT_ID,\n",
    "        region: str = REGION,\n",
    "        resource_bucket: str = RESOURCE_BUCKET, \n",
    "        file_bucket: str = FILE_BUCKET\n",
    "    ):\n",
    "    \n",
    "    #### this code block is only for a personal workbench \n",
    "    \n",
    "    import google.oauth2.credentials\n",
    "    token = !gcloud auth print-access-token\n",
    "    token_str = token[0]\n",
    "    \n",
    "    #### the end\n",
    "    \n",
    "    from datetime import datetime\n",
    "    update_ts = datetime.now()\n",
    "    update_ts_str = update_ts.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    from pathlib import Path\n",
    "    cwd = os.getcwd() \n",
    " \n",
    "#     # ----- create training set --------\n",
    "#     bq_create_training_dataset_op = bq_create_dataset(from_date=FROM_DATE, \n",
    "#                                 to_date=TO_DATE,\n",
    "#                                 project_id=PROJECT_ID, \n",
    "#                                 dataset_id=DATASET_ID,\n",
    "#                                 table_id=TABLE_ID,\n",
    "#                                 region=REGION,\n",
    "#                                 token=token_str\n",
    "#                                 )\n",
    "\n",
    "#     bq_create_training_dataset_op.set_memory_limit('32G')\n",
    "#     bq_create_training_dataset_op.set_cpu_limit('4')\n",
    "    \n",
    "#     # ----- preprocessing train data --------\n",
    "#     preprocess_train_op = preprocess(project_id=PROJECT_ID,\n",
    "#                                     dataset_id=DATASET_ID, \n",
    "#                                     table_id=TABLE_ID, \n",
    "#                                     file_bucket=FILE_BUCKET, \n",
    "#                                     resource_bucket=RESOURCE_BUCKET, \n",
    "#                                     stack_name=STACK_NAME, \n",
    "#                                     pipeline_path=PIPELINE_PATH,\n",
    "#                                     hs_nba_utils_path=HS_NBA_UTILS_PATH, \n",
    "#                                     model_type=MODEL_TYPE,\n",
    "#                                     load_sql=LOAD_SQL, \n",
    "#                                     train_csv=TRAIN_CSV,\n",
    "#                                     token=token_str\n",
    "#                                     )\n",
    "\n",
    "#     preprocess_train_op.set_memory_limit('32G')\n",
    "#     preprocess_train_op.set_cpu_limit('4')\n",
    "    \n",
    "    train_and_save_model_op = train_and_save_model(file_bucket=FILE_BUCKET, \n",
    "                                                    resource_bucket=RESOURCE_BUCKET,\n",
    "                                                    service_type=SERVICE_TYPE, \n",
    "                                                    project_id=PROJECT_ID, \n",
    "                                                    dataset_id=DATASET_ID, \n",
    "                                                    model_type=MODEL_TYPE, \n",
    "                                                    train_csv=TRAIN_CSV, \n",
    "                                                    save_file_name=SAVE_FILE_NAME,\n",
    "                                                    stats_file_name=STATS_FILE_NAME, \n",
    "                                                    pipeline_path=PIPELINE_PATH,\n",
    "                                                    hs_nba_utils_path=HS_NBA_UTILS_PATH, \n",
    "                                                    token=token_str\n",
    "                                                   )\n",
    "\n",
    "    train_and_save_model_op.set_memory_limit('32G')\n",
    "    train_and_save_model_op.set_cpu_limit('4')\n",
    "    \n",
    "#     # col_input_op = col_list = bq_create_dataset_op.outputs[\"col_list\"]\n",
    "#     upload_model_op = upload_model(project_id = PROJECT_ID\n",
    "#                                 , region = REGION\n",
    "#                                 , model = train_and_save_model_op.outputs[\"model\"]\n",
    "#                                 , model_name = MODEL_NAME\n",
    "#                                 , prediction_image = PREDICTION_IMAGE\n",
    "#                                 , col_list = train_and_save_model_op.outputs[\"col_list\"]\n",
    "#                                 , model_uri = train_and_save_model_op.outputs[\"model_uri\"]\n",
    "#                                 )\n",
    "    \n",
    "#     upload_model_op.set_memory_limit('32G')\n",
    "#     upload_model_op.set_cpu_limit('4')\n",
    "        \n",
    "    # preprocess_train_op.after(bq_create_training_dataset_op)\n",
    "    # train_and_save_model_op.after(preprocess_train_op)\n",
    "#     upload_model_op.after(train_and_save_model_op)\n",
    "\n",
    "    train_and_save_model_op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a51c8-4c37-4392-a21a-c4c3279d36d4",
   "metadata": {},
   "source": [
    "### Run the Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27465a63-5c3b-4e96-9c08-f9cc5dafde90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from kfp.v2 import compiler\n",
    "# from google.cloud.aiplatform import pipeline_jobs\n",
    "# import json\n",
    "\n",
    "# compiler.Compiler().compile(\n",
    "#    pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    "# )\n",
    "\n",
    "# job = pipeline_jobs.PipelineJob(\n",
    "#                                    display_name=TRAINING_PIPELINE_NAME,\n",
    "#                                    template_path=\"pipeline.json\",\n",
    "#                                    location=REGION,\n",
    "#                                    enable_caching=False,\n",
    "#                                    pipeline_root = PIPELINE_ROOT\n",
    "#                                 )\n",
    "# job.run(service_account = f\"bilayer-sa@{PROJECT_ID}.iam.gserviceaccount.com\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f0f361-e6ce-4d31-b50e-b490d906a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.oauth2.credentials\n",
    "import json\n",
    "\n",
    "token = !gcloud auth print-access-token\n",
    "CREDENTIALS = google.oauth2.credentials.Credentials(token[0])\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "   pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    ")\n",
    "\n",
    "job = pipeline_jobs.PipelineJob(\n",
    "   display_name=TRAINING_PIPELINE_NAME,\n",
    "   template_path=\"pipeline.json\",\n",
    "   credentials = CREDENTIALS,\n",
    "   pipeline_root = PIPELINE_ROOT,\n",
    "   location=REGION,\n",
    "   enable_caching=False # I encourage you to enable caching when testing as it will reduce resource use\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aabfdf-e94c-4716-96c5-32bb12e2e6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75dd69a-5553-4e4d-a802-290af3836f46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde51c8a-a3ba-40cd-9016-9976b5c750ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e86720d-9dd9-45bd-bad4-657c77c0413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import global modules\n",
    "# from google.cloud import storage\n",
    "# from google.cloud import bigquery\n",
    "# from pathlib import Path\n",
    "# from yaml import safe_load\n",
    "# import sys\n",
    "# import os\n",
    "\n",
    "# #tag cell with parameters\n",
    "# PROJECT_ID =  'divg-groovyhoon-pr-d2eab4'\n",
    "# DATASET_ID = 'nba_product_reco_prospects'\n",
    "# RESOURCE_BUCKET = 'divg-groovyhoon-pr-d2eab4-default'\n",
    "# FILE_BUCKET = 'divg-groovyhoon-pr-d2eab4'\n",
    "# MODEL_ID = ''\n",
    "# MODEL_NAME = 'nba_product_reco_prospects'\n",
    "\n",
    "# project_id = PROJECT_ID\n",
    "# dataset_id = DATASET_ID\n",
    "# resource_bucket = RESOURCE_BUCKET\n",
    "# file_bucket = FILE_BUCKET\n",
    "# table_id = 'master_features_set_prospect'\n",
    "\n",
    "# # set global vars\n",
    "# pth_project = Path(os.getcwd())\n",
    "# pth_model_config = pth_project / 'model_config.yaml'\n",
    "# pth_queries = pth_project / 'queries'\n",
    "# sys.path.insert(0, pth_project.as_posix())\n",
    "\n",
    "# # import local modules\n",
    "# from hs_nba_utils.etl.extract import extract_bq_data\n",
    "# from hs_nba_utils.modeling.hs_features_preprocessing import process_hs_features\n",
    "\n",
    "# # load model config\n",
    "# d_model_config = safe_load(pth_model_config.open())\n",
    "\n",
    "# # select columns to query\n",
    "# target_column = d_model_config['target_column']\n",
    "# str_feature_names = ','.join([f\"cast({f['name']} as {f['type']}) as {f['name']}\" for f in d_model_config['features']])\n",
    "# str_customer_ids = ','.join([f\"cast({f['name']} as {f['type']}) as {f['name']}\" for f in d_model_config['customer_ids']])\n",
    "\n",
    "# # extract training data\n",
    "# sql = (pth_queries / 'load_train_data.sql').read_text().format(\n",
    "#     project_id=project_id\n",
    "#     , dataset_id=dataset_id\n",
    "#     , table_id=table_id\n",
    "#     , target_column=target_column\n",
    "#     , customer_ids=str_customer_ids\n",
    "#     , feature_names=str_feature_names\n",
    "#     )\n",
    "\n",
    "# print(sql)\n",
    "\n",
    "# df = extract_bq_data(client, sql)\n",
    "# print(f\"Training dataset df.shape {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb26deb-932e-4055-a745-374158e89b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import global modules\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from pathlib import Path\n",
    "from yaml import safe_load\n",
    "import sys\n",
    "import os\n",
    "\n",
    "pth = Path(os.getcwd()) \n",
    "\n",
    "pth_model_config = pth / 'model_config.yaml'\n",
    "\n",
    "d_model_config = safe_load(pth_model_config.open())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cbb88e-59d7-4530-a5af-e071e37918ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "[f['name'] for f in d_model_config['features']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937a9f08-386a-436e-8989-94d0586b1ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_model_config = pth / 'model_config.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34edae4c-dcf5-4cb1-bbba-b21cd3a4ba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model_config = safe_load(pth_model_config.open())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8717f7c-e63c-4a04-8214-6ddb02210fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model_config['target_variables']['tier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718c8a7a-6b0c-4684-88a8-e319ce4505b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "','.join([f['name'] for f in d_model_config['target_variables']['acquisition']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad0e930-8355-44f1-8e7d-c9e66a2a7455",
   "metadata": {},
   "outputs": [],
   "source": [
    "','.join([f\"\\\"{f['name']}\\\"\" for f in d_model_config['target_variables']['acquisition']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c098a3-ece8-42ab-b270-8133cad75aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "','.join([f\"cast({f['name']} as {f['type']}) as {f['name']}\" for f in d_model_config['customer_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4767ef6-6378-48da-9f67-4ec7b4d79777",
   "metadata": {},
   "outputs": [],
   "source": [
    "','.join([f\"\\\"{f['name']}\\\"\" for f in d_model_config['target_variables']['acquisition']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db784eb-4996-40cf-95c6-f76634dc5df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model_config['customer_id_fields']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67899961-afff-4da1-8b3b-19baedbb37a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "[f['name'] for f in d_model_config['customer_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a8be8d-f2b6-4e01-b344-a4d8322cec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d_model_config['target_variables']['acquisition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fde61a-db24-4915-9c7f-ff95d4e8c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10): \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a873667-06bc-4d89-9b7a-0e291ab9b48e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
