{
  "pipelineSpec": {
    "components": {
      "comp-preprocess": {
        "executorLabel": "exec-preprocess",
        "inputDefinitions": {
          "parameters": {
            "dataset_id": {
              "type": "STRING"
            },
            "file_bucket": {
              "type": "STRING"
            },
            "hs_nba_utils_path": {
              "type": "STRING"
            },
            "load_sql": {
              "type": "STRING"
            },
            "model_type": {
              "type": "STRING"
            },
            "pipeline_path": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "resource_bucket": {
              "type": "STRING"
            },
            "stack_name": {
              "type": "STRING"
            },
            "table_id": {
              "type": "STRING"
            },
            "token": {
              "type": "STRING"
            },
            "train_csv": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-train-and-save-model": {
        "executorLabel": "exec-train-and-save-model",
        "inputDefinitions": {
          "parameters": {
            "dataset_id": {
              "type": "STRING"
            },
            "file_bucket": {
              "type": "STRING"
            },
            "hs_nba_utils_path": {
              "type": "STRING"
            },
            "model_type": {
              "type": "STRING"
            },
            "pipeline_path": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "resource_bucket": {
              "type": "STRING"
            },
            "save_file_name": {
              "type": "STRING"
            },
            "service_type": {
              "type": "STRING"
            },
            "stack_name": {
              "type": "STRING"
            },
            "stats_file_name": {
              "type": "STRING"
            },
            "token": {
              "type": "STRING"
            },
            "train_csv": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "metricsc": {
              "artifactType": {
                "schemaTitle": "system.ClassificationMetrics",
                "schemaVersion": "0.0.1"
              }
            },
            "model": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "col_list": {
              "type": "STRING"
            },
            "model_uri": {
              "type": "STRING"
            }
          }
        }
      }
    },
    "deploymentSpec": {
      "executors": {
        "exec-preprocess": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "preprocess"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.18' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef preprocess(\n    project_id: str,\n    dataset_id: str,\n    table_id: str,\n    file_bucket: str,\n    resource_bucket: str,\n    stack_name: str,\n    pipeline_path: str,\n    hs_nba_utils_path: str, \n    model_type: str,\n    load_sql: str, \n    train_csv: str,\n    token: str\n):\n    \"\"\"\n    Preprocess data for a machine learning training pipeline.\n    \"\"\"\n\n    # import global modules\n    from google.cloud import storage\n    from google.cloud import bigquery\n    from pathlib import Path\n    from yaml import safe_load\n    import sys\n    import os\n\n    # set global vars\n\n    # for prod\n    pth_project = Path(os.getcwd())\n    pth_model_config = pth_project / 'model_config.yaml'\n    pth_queries = pth_project / 'queries'\n    sys.path.insert(0, pth_project.as_posix())\n\n    #### For wb\n    import google.oauth2.credentials\n    CREDENTIALS = google.oauth2.credentials.Credentials(token)\n\n    client = bigquery.Client(project=project_id, credentials=CREDENTIALS)\n    job_config = bigquery.QueryJobConfig()\n\n#     #### For prod \n#     client = bigquery.Client(project=project_id)\n#     job_config = bigquery.QueryJobConfig()\n\n    # import query \n    # process features\n    # save data to bucket\n\n    def extract_dir_from_bucket(\n        bucket: Any, local_path: Path, prefix: str, split_prefix: str = 'serving_pipeline' \n    ):\n        \"\"\"\n        Download files from a specified bucket to a local path, excluding a specified prefix.\n\n        Parameters:\n        - bucket: The bucket object from which to download files.\n        - local_path: The local path where the files will be downloaded to.\n        - prefix: The prefix to filter the files in the bucket. Only files with this prefix will be downloaded.\n        - split_prefix: The prefix to exclude from the downloaded file paths. Default is 'serving_pipeline'.\n        \"\"\"\n        for blob in bucket.list_blobs(prefix=prefix):\n            if not blob.name.endswith(\"/\"):\n                path = local_path / blob.name.split(f'{split_prefix}/')[-1]\n                str_path = path.as_posix()\n                Path(str_path[:str_path.rindex('/')]).mkdir(parents=True, exist_ok=True)\n                blob.download_to_filename(str_path)\n\n    # download utils and model config locally\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(resource_bucket)\n    extract_dir_from_bucket(\n        bucket, pth_project, f'{stack_name}/{hs_nba_utils_path}', split_prefix='notebook'\n    ) \n    extract_dir_from_bucket(\n        bucket, pth_project, f'{stack_name}/{pipeline_path}/queries', split_prefix='training_pipeline'\n    )\n\n    blob = bucket.blob(f'{pipeline_path}/model_config.yaml')\n    blob.download_to_filename(pth_model_config)\n\n    # import local modules\n    from hs_nba_utils.etl.extract import extract_bq_data\n    from hs_nba_utils.modeling.prospects_features_preprocessing import process_prospects_features\n\n    # load model config\n    d_model_config = safe_load(pth_model_config.open())\n\n    # select columns to query\n    target_column = d_model_config['target_column']\n    str_feature_names = ','.join([f\"cast({f['name']} as {f['type']}) as {f['name']}\" for f in d_model_config['features']])\n    str_customer_ids = ','.join([f\"cast({f['name']} as {f['type']}) as {f['name']}\" for f in d_model_config['customer_ids']])\n    str_target_labels = ','.join([f\"\\\"{f['name']}\\\"\" for f in d_model_config['target_variables']['acquisition']])\n\n    # extract training data\n    sql = (pth_queries / load_sql).read_text().format(\n        project_id=project_id\n        , dataset_id=dataset_id\n        , table_id=table_id\n        , target_column=target_column\n        , customer_ids=str_customer_ids\n        , feature_names=str_feature_names\n        , target_labels=str_target_labels\n    )\n\n    # save sql to gcs bucket\n    file_name = f'{stack_name}/{pipeline_path}/queries/{load_sql}'\n\n    # Convert the string to bytes\n    content_bytes = sql.encode('utf-8')\n\n    # Upload the file to GCS\n    bucket = storage_client.bucket(file_bucket)\n    blob = bucket.blob(file_name)\n    blob.upload_from_string(content_bytes)\n\n    df = extract_bq_data(client, sql)\n    print(f\"Training dataset df.shape {df.shape}\")\n\n    # process features\n    df_processed = process_prospects_features(\n        df, d_model_config, training_mode=True, model_type=model_type, target_name=target_column\n    )\n    print(f\"Training dataset processed df.shape {df_processed.shape}\")\n\n    # save data to pipeline bucket\n    df_processed.to_csv(\n        f'gs://{file_bucket}/{stack_name}/{pipeline_path}/{train_csv}', index=False\n    )\n    print(f'Training data saved into {file_bucket}')\n\n"
            ],
            "image": "northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-pycaret-slim:latest",
            "resources": {
              "cpuLimit": 8.0,
              "memoryLimit": 32.0
            }
          }
        },
        "exec-train-and-save-model": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "train_and_save_model"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.18' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef train_and_save_model(file_bucket: str\n                        , resource_bucket: str\n                        , stack_name: str\n                        , service_type: str\n                        , project_id: str\n                        , dataset_id: str\n                        , model_type: str\n                        , train_csv: str\n                        , save_file_name: str\n                        , stats_file_name: str\n                        , pipeline_path: str\n                        , hs_nba_utils_path: str\n                        , metrics: Output[Metrics]\n                        , metricsc: Output[ClassificationMetrics]\n                        , model: Output[Model]\n                        , token: str\n                        )-> NamedTuple(\"output\", [(\"col_list\", list), (\"model_uri\", str)]):\n\n    #### Import Libraries ####\n    import os \n    import gc\n    import sys\n    import time\n    import pickle\n    import pandas as pd\n    import numpy as np\n    import xgboost as xgb\n\n    from pathlib import Path\n    from yaml import safe_load\n\n    from datetime import datetime\n    from google.cloud import storage\n    from google.cloud import bigquery\n\n    telus_purple = '#4B286D'\n    telus_green = '#66CC00'\n    telus_grey = '#F4F4F7'\n\n    # for prod\n    pth_project = Path(os.getcwd())\n    pth_model_config = pth_project / 'model_config.yaml'\n    sys.path.insert(0, pth_project.as_posix())\n\n    #### For wb\n    import google.oauth2.credentials\n    CREDENTIALS = google.oauth2.credentials.Credentials(token)\n    client = bigquery.Client(project=project_id, credentials=CREDENTIALS)\n    job_config = bigquery.QueryJobConfig()\n\n#     #### For prod\n#     client = bigquery.Client(project=project_id)\n#     job_config = bigquery.QueryJobConfig()\n\n    def extract_dir_from_bucket(\n        bucket: Any, local_path: Path, prefix: str, split_prefix: str = 'serving_pipeline' \n    ):\n        \"\"\"\n        Download files from a specified bucket to a local path, excluding a specified prefix.\n\n        Parameters:\n        - bucket: The bucket object from which to download files.\n        - local_path: The local path where the files will be downloaded to.\n        - prefix: The prefix to filter the files in the bucket. Only files with this prefix will be downloaded.\n        - split_prefix: The prefix to exclude from the downloaded file paths. Default is 'serving_pipeline'.\n        \"\"\"\n        for blob in bucket.list_blobs(prefix=prefix):\n            if not blob.name.endswith(\"/\"):\n                path = local_path / blob.name.split(f'{split_prefix}/')[-1]\n                str_path = path.as_posix()\n                Path(str_path[:str_path.rindex('/')]).mkdir(parents=True, exist_ok=True)\n                blob.download_to_filename(str_path)\n\n    # download utils and model config locally\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(resource_bucket)\n    extract_dir_from_bucket(\n        bucket, pth_project, f'{stack_name}/{hs_nba_utils_path}', split_prefix='notebook'\n    ) \n    extract_dir_from_bucket(\n        bucket, pth_project, f'{stack_name}/{pipeline_path}/queries', split_prefix='training_pipeline'\n    )\n\n    blob = bucket.blob(f'{pipeline_path}/model_config.yaml')\n    blob.download_to_filename(pth_model_config)\n\n    # import local modules\n    from hs_nba_utils.modeling.train import train\n    from hs_nba_utils.modeling.evaluate import evaluate\n    from hs_nba_utils.modeling.save_model import save_model\n\n    # load model config\n    d_model_config = safe_load(pth_model_config.open())\n\n    # train    \n    df_result, xgb_model = train(file_bucket=file_bucket, \n                stack_name=stack_name, \n                pipeline_path=pipeline_path, \n                service_type=service_type, \n                model_type=model_type, \n                d_model_config=d_model_config, \n                train_csv=train_csv, \n                save_file_name=save_file_name\n                ) \n\n    print('training step successfully completed')\n\n    # evaluate\n    df_stats = evaluate(df_result=df_result, \n                file_bucket=file_bucket, \n                stack_name=stack_name, \n                pipeline_path=pipeline_path, \n                service_type=service_type, \n                model_type=model_type, \n                d_model_config=d_model_config, \n                stats_file_name=stats_file_name\n                )\n\n    print('evaluate step successfully completed')\n\n    # save model \n    col_list, model_uri = save_model(model=xgb_model, \n                file_bucket=file_bucket, \n                stack_name=stack_name, \n                pipeline_path=pipeline_path, \n                service_type=service_type, \n                d_model_config=d_model_config\n                )\n\n    print('save model step successfully completed')\n\n    print(model_uri)\n\n    return (col_list, model_uri)\n\n"
            ],
            "image": "northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-pycaret-slim:latest",
            "resources": {
              "cpuLimit": 8.0,
              "memoryLimit": 32.0
            }
          }
        }
      }
    },
    "pipelineInfo": {
      "name": "hs-nba-prospects-train-pipeline"
    },
    "root": {
      "dag": {
        "outputs": {
          "artifacts": {
            "train-and-save-model-metrics": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "metrics",
                  "producerSubtask": "train-and-save-model"
                }
              ]
            },
            "train-and-save-model-metricsc": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "metricsc",
                  "producerSubtask": "train-and-save-model"
                }
              ]
            }
          }
        },
        "tasks": {
          "preprocess": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-preprocess"
            },
            "inputs": {
              "parameters": {
                "dataset_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "hs_nba_prospects"
                    }
                  }
                },
                "file_bucket": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4-default"
                    }
                  }
                },
                "hs_nba_utils_path": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "vertex_pipelines/hs_nba_utils/notebook"
                    }
                  }
                },
                "load_sql": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "load_train_data.sql"
                    }
                  }
                },
                "model_type": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "acquisition"
                    }
                  }
                },
                "pipeline_path": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "vertex_pipelines/hs_nba_prospects/training_pipeline"
                    }
                  }
                },
                "project_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4"
                    }
                  }
                },
                "resource_bucket": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4-default"
                    }
                  }
                },
                "stack_name": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "ffh_nba"
                    }
                  }
                },
                "table_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "master_features_set_prospect"
                    }
                  }
                },
                "token": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "ya29.c.c0AY_VpZhUstRQ0xMqq6fQPgPPdpIeyEZPTQxInhV9-DDyHvNNbJ-HW7mQ800vQfIuqMkFbzDvOCgjmiNthCMe9FvPO1k6_tVbFWg94Z8zPsPkXvib0tCH1ZvpEHl8DCJ5UCOWPxIxKckbdCq3XanpWGsAZW2G1vuvGLJMNx4Fd0JPFcRUgcr23B9qVWxtNcfK4ey9SiJGprIKmGjtNWJuZuvQyl2KKY-fLDRqrXlIMjsBDWMl7uYXcrso3ksOsYI5IUWHMYB430I_XzOinwlMyLt1K7JhhFRAYob5HNTDAO4pd1Y63zE_zJFnuDBwsx0p3QDpKdZr1XSp-Fn1H-NwDkxeeYrvQXImVuThwqHo5bQO_xz29BN8YVUCWeWewj7cFQH395DnyStV6tc9dkMghe7O72qjWUhpV8x8eqg3ZxsWqtucu-m6_xbs92eJzgYF9FihsUQX8iURSzwnrexVMaFw4wqyn72O2aMruRx3alj_St8UZaFkItba--mvJnF4Xy-4SZZlXuo-us4iXfvdwMda8pYsstm2ySOtg93n1XVOcbd1oII6gSZ3s-wUfhV5dYkMU-rS_1r-q-s32XOSqF9gyOXq5r3ctIizZYjwvJ79miOpdJR5fmjY8joUIBSmO2YqxVQizUIkYiiX46zqJegv0mBr6UqMgs9ie1BQ8Zyyb9RbVoFmif9xnyh7XlXfeen9Q108Xdodl0s_jVQpOyiWtwcVj6eFo33-Y2v95JZ5qwUvxM3BBg993h0-7F47FM5-7-j-MzpU_euO7r44tFxtxfFIem3_c9kc_gxbYu6njiqcvI4kVrknzsypgoSFMVt7UJJYejzXcrs4pwh_RzMysk2yzOtp564-QcxFbQr4_rqjxIxkFslvFO_0S6fbUrgJ7BYiq_5Xbe02MYZgaF10nYId7yRVxQ5qz_4M713y1urXntlMRnukqs8sRYg56FkiSqnzr3Vq24kt4d3zMw5fnIMWthpIZkkF-qBYe1r8flvevrmQ8Rb"
                    }
                  }
                },
                "train_csv": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "df_train.csv"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "preprocess"
            }
          },
          "train-and-save-model": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-train-and-save-model"
            },
            "dependentTasks": [
              "preprocess"
            ],
            "inputs": {
              "parameters": {
                "dataset_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "hs_nba_prospects"
                    }
                  }
                },
                "file_bucket": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4-default"
                    }
                  }
                },
                "hs_nba_utils_path": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "vertex_pipelines/hs_nba_utils/notebook"
                    }
                  }
                },
                "model_type": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "acquisition"
                    }
                  }
                },
                "pipeline_path": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "vertex_pipelines/hs_nba_prospects/training_pipeline"
                    }
                  }
                },
                "project_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4"
                    }
                  }
                },
                "resource_bucket": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4-default"
                    }
                  }
                },
                "save_file_name": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "df_test_exp.csv"
                    }
                  }
                },
                "service_type": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "hs_nba_prospects"
                    }
                  }
                },
                "stack_name": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "ffh_nba"
                    }
                  }
                },
                "stats_file_name": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "df_stats.csv"
                    }
                  }
                },
                "token": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "ya29.c.c0AY_VpZhUstRQ0xMqq6fQPgPPdpIeyEZPTQxInhV9-DDyHvNNbJ-HW7mQ800vQfIuqMkFbzDvOCgjmiNthCMe9FvPO1k6_tVbFWg94Z8zPsPkXvib0tCH1ZvpEHl8DCJ5UCOWPxIxKckbdCq3XanpWGsAZW2G1vuvGLJMNx4Fd0JPFcRUgcr23B9qVWxtNcfK4ey9SiJGprIKmGjtNWJuZuvQyl2KKY-fLDRqrXlIMjsBDWMl7uYXcrso3ksOsYI5IUWHMYB430I_XzOinwlMyLt1K7JhhFRAYob5HNTDAO4pd1Y63zE_zJFnuDBwsx0p3QDpKdZr1XSp-Fn1H-NwDkxeeYrvQXImVuThwqHo5bQO_xz29BN8YVUCWeWewj7cFQH395DnyStV6tc9dkMghe7O72qjWUhpV8x8eqg3ZxsWqtucu-m6_xbs92eJzgYF9FihsUQX8iURSzwnrexVMaFw4wqyn72O2aMruRx3alj_St8UZaFkItba--mvJnF4Xy-4SZZlXuo-us4iXfvdwMda8pYsstm2ySOtg93n1XVOcbd1oII6gSZ3s-wUfhV5dYkMU-rS_1r-q-s32XOSqF9gyOXq5r3ctIizZYjwvJ79miOpdJR5fmjY8joUIBSmO2YqxVQizUIkYiiX46zqJegv0mBr6UqMgs9ie1BQ8Zyyb9RbVoFmif9xnyh7XlXfeen9Q108Xdodl0s_jVQpOyiWtwcVj6eFo33-Y2v95JZ5qwUvxM3BBg993h0-7F47FM5-7-j-MzpU_euO7r44tFxtxfFIem3_c9kc_gxbYu6njiqcvI4kVrknzsypgoSFMVt7UJJYejzXcrs4pwh_RzMysk2yzOtp564-QcxFbQr4_rqjxIxkFslvFO_0S6fbUrgJ7BYiq_5Xbe02MYZgaF10nYId7yRVxQ5qz_4M713y1urXntlMRnukqs8sRYg56FkiSqnzr3Vq24kt4d3zMw5fnIMWthpIZkkF-qBYe1r8flvevrmQ8Rb"
                    }
                  }
                },
                "train_csv": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "df_train.csv"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "train-and-save-model"
            }
          }
        }
      },
      "inputDefinitions": {
        "parameters": {
          "file_bucket": {
            "type": "STRING"
          },
          "project_id": {
            "type": "STRING"
          },
          "region": {
            "type": "STRING"
          },
          "resource_bucket": {
            "type": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "train-and-save-model-metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "train-and-save-model-metricsc": {
            "artifactType": {
              "schemaTitle": "system.ClassificationMetrics",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "schemaVersion": "2.0.0",
    "sdkVersion": "kfp-1.8.18"
  },
  "runtimeConfig": {
    "parameters": {
      "file_bucket": {
        "stringValue": "divg-groovyhoon-pr-d2eab4-default"
      },
      "project_id": {
        "stringValue": "divg-groovyhoon-pr-d2eab4"
      },
      "region": {
        "stringValue": "northamerica-northeast1"
      },
      "resource_bucket": {
        "stringValue": "divg-groovyhoon-pr-d2eab4-default"
      }
    }
  }
}