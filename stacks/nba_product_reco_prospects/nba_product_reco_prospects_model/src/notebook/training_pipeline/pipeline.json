{
  "pipelineSpec": {
    "components": {
      "comp-bq-create-dataset": {
        "executorLabel": "exec-bq-create-dataset",
        "inputDefinitions": {
          "parameters": {
            "dataset_id": {
              "type": "STRING"
            },
            "from_date": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "region": {
              "type": "STRING"
            },
            "table_id": {
              "type": "STRING"
            },
            "to_date": {
              "type": "STRING"
            },
            "token": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "parameters": {
            "col_list": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-preprocess": {
        "executorLabel": "exec-preprocess",
        "inputDefinitions": {
          "parameters": {
            "cwd": {
              "type": "STRING"
            },
            "dataset_id": {
              "type": "STRING"
            },
            "file_bucket": {
              "type": "STRING"
            },
            "hs_nba_utils_path": {
              "type": "STRING"
            },
            "model_type": {
              "type": "STRING"
            },
            "pipeline_path": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "resource_bucket": {
              "type": "STRING"
            },
            "stack_name": {
              "type": "STRING"
            },
            "table_id": {
              "type": "STRING"
            },
            "token": {
              "type": "STRING"
            }
          }
        }
      }
    },
    "deploymentSpec": {
      "executors": {
        "exec-bq-create-dataset": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "bq_create_dataset"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.18' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef bq_create_dataset(from_date: str\n                      , to_date: str \n                      , project_id: str\n                      , dataset_id: str\n                      , table_id: str\n                      , region: str\n                      , token: str\n                      ) -> NamedTuple(\"output\", [(\"col_list\", list)]):\n\n    from google.cloud import bigquery\n    import logging \n    from datetime import datetime\n\n    #### For wb\n    import google.oauth2.credentials\n    CREDENTIALS = google.oauth2.credentials.Credentials(token)\n\n    client = bigquery.Client(project=project_id, credentials=CREDENTIALS)\n    job_config = bigquery.QueryJobConfig()\n\n#     #### For prod \n#     client = bigquery.Client(project=project_id)\n#     job_config = bigquery.QueryJobConfig()\n\n    # Change dataset / table + sp table name to version in bi-layer\n    query =\\\n        f\"\"\"\n            DECLARE _from_dt DATE DEFAULT '{from_date}';\n            DECLARE _to_dt DATE DEFAULT '{to_date}';\n\n            -- Run this once the training pipeline deploys in BI Layer\n            -- CALL `{project_id}.{dataset_id}.{table_id}`(_from_dt, _to_dt);\n\n            SELECT *\n            FROM {dataset_id}.INFORMATION_SCHEMA.PARTITIONS\n            WHERE table_name='{table_id}'\n        \"\"\"\n\n    df = client.query(query, job_config=job_config).to_dataframe()\n    logging.info(df.to_string())\n\n    ######################### Save column list_##########################\n    query =\\\n        f\"\"\"\n           SELECT\n                *\n            FROM {dataset_id}.{table_id}\n            LIMIT 1\n        \"\"\"\n\n    df = client.query(query, job_config=job_config).to_dataframe()\n\n    col_list = list([col for col in df.columns])\n\n    return (col_list,)\n\n"
            ],
            "image": "northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-pycaret-slim:latest",
            "resources": {
              "cpuLimit": 4.0,
              "memoryLimit": 32.0
            }
          }
        },
        "exec-preprocess": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "preprocess"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.18' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef preprocess(\n    project_id: str,\n    dataset_id: str,\n    table_id: str,\n    file_bucket: str,\n    resource_bucket: str,\n    stack_name: str,\n    pipeline_path: str,\n    hs_nba_utils_path: str, \n    cwd:str,\n    model_type:str,\n    token:str\n):\n    \"\"\"\n    Preprocess data for a machine learning training pipeline.\n    \"\"\"\n\n    # import global modules\n    from google.cloud import storage\n    from google.cloud import bigquery\n    from pathlib import Path\n    from yaml import safe_load\n    import sys\n    import os\n\n    # set global vars\n\n    # # for wb \n    # pth_project = Path(cwd)\n\n    # for prod\n    pth_project = Path(os.getcwd())\n\n    print(f'pth_project: {pth_project}')\n\n    pth_model_config = pth_project / 'model_config.yaml'\n    pth_queries = pth_project / 'queries'\n    sys.path.insert(0, pth_project.as_posix())\n\n    # storage client gcp clients\n    storage_client = storage.Client()\n\n    #### For wb\n    import google.oauth2.credentials\n    CREDENTIALS = google.oauth2.credentials.Credentials(token)\n\n    client = bigquery.Client(project=project_id, credentials=CREDENTIALS)\n    job_config = bigquery.QueryJobConfig()\n\n#     #### For prod \n#     client = bigquery.Client(project=project_id)\n#     job_config = bigquery.QueryJobConfig()\n\n    def extract_dir_from_bucket(\n        bucket: Any, local_path: Path, prefix: str, split_prefix: str = 'serving_pipeline' \n    ):\n        \"\"\"\n        Download files from a specified bucket to a local path, excluding a specified prefix.\n\n        Parameters:\n        - bucket: The bucket object from which to download files.\n        - local_path: The local path where the files will be downloaded to.\n        - prefix: The prefix to filter the files in the bucket. Only files with this prefix will be downloaded.\n        - split_prefix: The prefix to exclude from the downloaded file paths. Default is 'serving_pipeline'.\n        \"\"\"\n        for blob in bucket.list_blobs(prefix=prefix):\n            if not blob.name.endswith(\"/\"):\n                path = local_path / blob.name.split(f'{split_prefix}/')[-1]\n                str_path = path.as_posix()\n                Path(str_path[:str_path.rindex('/')]).mkdir(parents=True, exist_ok=True)\n                blob.download_to_filename(str_path)\n\n    # download utils and model config locally\n    bucket = storage_client.bucket(resource_bucket)\n    # extract_dir_from_bucket(\n    #     bucket, pth_project, f'{stack_name}/{hs_nba_utils_path}', split_prefix='notebook'\n    # ) \n    # extract_dir_from_bucket(\n    #     bucket, pth_project, f'{stack_name}/{pipeline_path}/queries', split_prefix='training_pipeline'\n    # )\n\n    extract_dir_from_bucket(\n        bucket, pth_project, f'{hs_nba_utils_path}', split_prefix='notebook'\n    ) \n    extract_dir_from_bucket(\n        bucket, pth_project, f'{pipeline_path}/queries', split_prefix='training_pipeline'\n    )\n\n    blob = bucket.blob(f'{pipeline_path}/model_config.yaml')\n    blob.download_to_filename(pth_model_config)\n\n    # import local modules\n    from hs_nba_utils.etl.extract import extract_bq_data\n    from hs_nba_utils.modeling.prospects_features_preprocessing import process_prospects_features\n\n    print('modules loaded successfully')\n\n    # load model config\n    d_model_config = safe_load(pth_model_config.open())\n\n    print(f'pth_model_config: {pth_model_config}')\n\n    # select columns to query\n    target_column = d_model_config['target_column']\n    str_feature_names = ','.join([f\"cast({f['name']} as {f['type']}) as {f['name']}\" for f in d_model_config['features']])\n    str_customer_ids = ','.join([f\"cast({f['name']} as {f['type']}) as {f['name']}\" for f in d_model_config['customer_ids']])\n    str_target_labels = ','.join([f\"\\\"{f['name']}\\\"\" for f in d_model_config['target_variables']['acquisition']])\n\n    # extract training data\n    sql = (pth_queries / 'load_train_data.sql').read_text().format(\n        project_id=project_id\n        , dataset_id=dataset_id\n        , table_id=table_id\n        , target_column=target_column\n        , customer_ids=str_customer_ids\n        , feature_names=str_feature_names\n        , target_labels=str_target_labels\n    )\n\n    # save sql to gcs bucket\n    file_name = f'{stack_name}/load_train_data.sql'\n\n    # Initialize a client\n    storage_client = storage.Client()\n\n    # Convert the string to bytes\n    content_bytes = sql.encode('utf-8')\n\n    # Upload the file to GCS\n    bucket = storage_client.bucket(file_bucket)\n    blob = bucket.blob(file_name)\n    blob.upload_from_string(content_bytes)\n\n    df = extract_bq_data(client, sql)\n    print(f\"Training dataset df.shape {df.shape}\")\n\n    # process features\n    df_processed = process_prospects_features(\n        df, d_model_config, training_mode=True, model_type=model_type, target_name=target_column\n    )\n    print(f\"Training dataset processed df.shape {df_processed.shape}\")\n\n    # save data to pipeline bucket\n    df_processed.to_csv(\n        f'gs://{file_bucket}/nba_product_reco_prospects/training_dataset.csv', index=False\n    )\n    print(f'Training data saved into {file_bucket}')\n\n"
            ],
            "image": "northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-pycaret-slim:latest",
            "resources": {
              "cpuLimit": 4.0,
              "memoryLimit": 32.0
            }
          }
        }
      }
    },
    "pipelineInfo": {
      "name": "nba-product-reco-prospects-train-pipeline"
    },
    "root": {
      "dag": {
        "tasks": {
          "bq-create-dataset": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-bq-create-dataset"
            },
            "inputs": {
              "parameters": {
                "dataset_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "nba_product_reco_prospects"
                    }
                  }
                },
                "from_date": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "2023-04-01"
                    }
                  }
                },
                "project_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4"
                    }
                  }
                },
                "region": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "northamerica-northeast1"
                    }
                  }
                },
                "table_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "master_features_set_prospect"
                    }
                  }
                },
                "to_date": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "2024-03-31"
                    }
                  }
                },
                "token": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "ya29.a0AXooCgsC0YNOASgqMaWX57_aeV4n9KovifVMUhlfdnorDVH2p-A_P17fiv9eBQwOSdrozXE2qEcJyFpkIvFDpKiKV3wiN8fz4OJDLyINSh_KHN7uzO-_Cb1c-1PYkaxHsUmP0mSpOjTEvCTOEIE06k5HQP6ZPMGB7lk1CPOkdKuXaCgYKAbASARISFQHGX2Mi8koFzYGYUTg3Of5Mi49LPA0179"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "bq-create-dataset"
            }
          },
          "preprocess": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-preprocess"
            },
            "dependentTasks": [
              "bq-create-dataset"
            ],
            "inputs": {
              "parameters": {
                "cwd": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "/home/jupyter/josh_dev_env/stacks/nba_product_reco_prospects/nba_product_reco_prospects_model/src/notebook/training_pipeline"
                    }
                  }
                },
                "dataset_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "nba_product_reco_prospects"
                    }
                  }
                },
                "file_bucket": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4-default"
                    }
                  }
                },
                "hs_nba_utils_path": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "vertex_pipelines/hs_nba_utils/notebook"
                    }
                  }
                },
                "model_type": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "acquisition"
                    }
                  }
                },
                "pipeline_path": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "vertex_pipelines/hs_nba_existing_customers/training_pipeline"
                    }
                  }
                },
                "project_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4"
                    }
                  }
                },
                "resource_bucket": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4-default"
                    }
                  }
                },
                "stack_name": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "nba_product_reco_prospects"
                    }
                  }
                },
                "table_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "master_features_set_prospect"
                    }
                  }
                },
                "token": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "ya29.a0AXooCgsC0YNOASgqMaWX57_aeV4n9KovifVMUhlfdnorDVH2p-A_P17fiv9eBQwOSdrozXE2qEcJyFpkIvFDpKiKV3wiN8fz4OJDLyINSh_KHN7uzO-_Cb1c-1PYkaxHsUmP0mSpOjTEvCTOEIE06k5HQP6ZPMGB7lk1CPOkdKuXaCgYKAbASARISFQHGX2Mi8koFzYGYUTg3Of5Mi49LPA0179"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "preprocess"
            }
          }
        }
      },
      "inputDefinitions": {
        "parameters": {
          "file_bucket": {
            "type": "STRING"
          },
          "project_id": {
            "type": "STRING"
          },
          "region": {
            "type": "STRING"
          },
          "resource_bucket": {
            "type": "STRING"
          }
        }
      }
    },
    "schemaVersion": "2.0.0",
    "sdkVersion": "kfp-1.8.18"
  },
  "runtimeConfig": {
    "parameters": {
      "file_bucket": {
        "stringValue": "divg-groovyhoon-pr-d2eab4-default"
      },
      "project_id": {
        "stringValue": "divg-groovyhoon-pr-d2eab4"
      },
      "region": {
        "stringValue": "northamerica-northeast1"
      },
      "resource_bucket": {
        "stringValue": "divg-groovyhoon-pr-d2eab4-default"
      }
    }
  }
}