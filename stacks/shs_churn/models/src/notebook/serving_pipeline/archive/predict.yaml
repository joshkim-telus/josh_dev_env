name: Predict
description: Machine learning predict pipeline.
inputs:
- {name: project_id, type: String}
- {name: dataset_id, type: String}
- {name: score_table_id, type: String}
- {name: resource_bucket, type: String}
- {name: file_bucket, type: String}
- {name: stack_name, type: String}
- {name: service_type, type: String}
- {name: model_id, type: String}
- {name: pipeline_type, type: String}
- {name: training_pipeline_path, type: String}
- {name: serving_pipeline_path, type: String}
- {name: preprocess_output_csv, type: String}
- {name: utils_path, type: String}
- {name: score_file_name, type: String}
- {name: token, type: String}
implementation:
  container:
    image: northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-pycaret-slim:latest
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.18' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
      \ import *\n\ndef predict(project_id: str,\n            dataset_id: str,\n \
      \           score_table_id: str,\n            resource_bucket: str,\n      \
      \      file_bucket: str, \n            stack_name: str,\n            service_type:\
      \ str, \n            model_id: str, \n            pipeline_type: str,\n    \
      \        training_pipeline_path: str,\n            serving_pipeline_path: str,\n\
      \            preprocess_output_csv: str, \n            utils_path: str, \n \
      \           score_file_name: str, \n            token: str\n            ):\n\
      \n    \"\"\"\n    Machine learning predict pipeline.\n    \"\"\"\n\n    # Import\
      \ global modules\n    from google.cloud import storage\n    from pathlib import\
      \ Path\n    from yaml import safe_load\n    import sys\n    import os\n    import\
      \ pandas as pd\n    import pickle\n    from google.cloud import bigquery\n\n\
      \    # for prod\n    pth_project = Path(os.getcwd())\n    pth_model_config =\
      \ pth_project / 'model_config.yaml'\n    pth_queries = pth_project / 'queries'\n\
      \    sys.path.insert(0, pth_project.as_posix())\n\n    # init gcp clients\n\
      \    storage_client = storage.Client()\n\n    #### For wb\n    import google.oauth2.credentials\n\
      \    CREDENTIALS = google.oauth2.credentials.Credentials(token)\n\n    client\
      \ = bigquery.Client(project=project_id, credentials=CREDENTIALS)\n    job_config\
      \ = bigquery.QueryJobConfig()\n\n    # #### For prod \n    # client = bigquery.Client(project=project_id)\n\
      \    # job_config = bigquery.QueryJobConfig()\n\n    def extract_dir_from_bucket(\n\
      \        bucket: Any, local_path: Path, prefix: str, split_prefix: str = 'serving_pipeline'\
      \ \n    ):\n        \"\"\"\n        Download files from a specified bucket to\
      \ a local path, excluding a specified prefix.\n\n        Parameters:\n     \
      \   - bucket: The bucket object from which to download files.\n        - local_path:\
      \ The local path where the files will be downloaded to.\n        - prefix: The\
      \ prefix to filter the files in the bucket. Only files with this prefix will\
      \ be downloaded.\n        - split_prefix: The prefix to exclude from the downloaded\
      \ file paths. Default is 'serving_pipeline'.\n        \"\"\"\n        for blob\
      \ in bucket.list_blobs(prefix=prefix):\n            if not blob.name.endswith(\"\
      /\"):\n                path = local_path / blob.name.split(f'{split_prefix}/')[-1]\n\
      \                str_path = path.as_posix()\n                Path(str_path[:str_path.rindex('/')]).mkdir(parents=True,\
      \ exist_ok=True)\n                blob.download_to_filename(str_path)\n\n  \
      \  # download utils and model config locally\n    storage_client = storage.Client()\n\
      \    bucket = storage_client.bucket(resource_bucket)\n    extract_dir_from_bucket(\n\
      \        bucket, pth_project, f'{stack_name}/{utils_path}', split_prefix='resources'\n\
      \    ) \n    extract_dir_from_bucket(\n        bucket, pth_project, f'{stack_name}/{serving_pipeline_path}/queries',\
      \ split_prefix=pipeline_type\n    )\n\n    blob = bucket.blob(f\"{stack_name}/{serving_pipeline_path}/model_config.yaml\"\
      \ )\n    blob.download_to_filename(pth_model_config)\n\n    # download model\
      \ pickle file\n    model_name = f'{service_type}_xgb_models_latest.pkl'\n  \
      \  bucket = storage_client.bucket(file_bucket)\n    blob = bucket.blob(f'models/{model_name}')\n\
      \    blob.download_to_filename(pth_project / model_name)\n\n    # load model\n\
      \    d_model_config = safe_load(pth_model_config.open())\n    with open(pth_project\
      \ / model_name, \"rb\") as f:\n        models_dict = pickle.load(f)\n    model\
      \ = models_dict['model']\n\n    # load data from bucket\n    df_features = pd.read_csv(f'gs://{file_bucket}/{pipeline_type}/{preprocess_output_csv}',\
      \ index_col=None)\n\n    print(f'Features df.shape {df_features.shape}')\n\n\
      \    # select features and model predict  \n    l_feature_names = [f['name']\
      \ for f in d_model_config['features']]\n\n    # make predictions on df_features\n\
      \    pred_prob = model.predict_proba(df_features[l_feature_names], ntree_limit=model.best_iteration)[:,\
      \ 1]\n\n    # import local modules\n    from etl.load import create_temp_table,\
      \ insert_from_temp_table\n\n    # build result dataframe\n    result = pd.DataFrame(columns=['ban',\
      \ 'score_date', 'model_id', 'score'])\n    result['score'] = list(pred_prob)\n\
      \    result['score'] = result['score'].fillna(0.0).astype('float64')\n    result['ban']\
      \ = list(df_features['ban'])\n    result['ban'] = result['ban'].astype('str')\n\
      \    result['score_date'] = df_features['part_dt']\n    result['model_id'] =\
      \ model_id\n\n    # save data to pipeline bucket\n    location_to_save = f'gs://{file_bucket}/{pipeline_type}/{score_file_name}'\n\
      \    result.to_csv(location_to_save, index=False)\n    print(f'Scores saved\
      \ into {location_to_save}')\n\n    # create temp table in bq\n    temp_table_name\
      \ = create_temp_table(\n        bq_client=client, project_id=project_id, dataset_id=dataset_id,\
      \ table_id=score_table_id, df_to_load=result\n    )\n\n    print(f'created a\
      \ temp table {temp_table_name}')\n\n    # insert data from temp into main table\n\
      \    current_part_dt = str(result['score_date'].max())\n    insert_from_temp_table(\n\
      \        bq_client=client, project_id=project_id, dataset_id=dataset_id, table_id=score_table_id,\
      \ temp_table_id=temp_table_name, current_part_dt=current_part_dt,\n        pth_drop_query=pth_queries\
      \ / 'drop_current_part_dt.sql', pth_insert_query=pth_queries / 'insert_from_temp_score_table.sql'\n\
      \    )\n\n"
    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - predict
