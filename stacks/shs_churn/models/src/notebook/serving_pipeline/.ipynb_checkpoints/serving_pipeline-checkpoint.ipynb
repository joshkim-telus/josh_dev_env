{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import kfp\n",
    "from typing import Any\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, ClassificationMetrics,\n",
    "                        Metrics, component)\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import google\n",
    "from google.oauth2 import credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2.service_account import Credentials\n",
    "from google.cloud import storage\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components.v1.batch_predict_job import \\\n",
    "    ModelBatchPredictOp as batch_prediction_op\n",
    "\n",
    "# Set global vars\n",
    "pth_project = Path(os.getcwd())\n",
    "sys.path.insert(0, pth_project.as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YAML Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID=''\n",
    "DATASET_ID=''\n",
    "TABLE_ID=''\n",
    "SCORE_TABLE_ID=''\n",
    "UCAR_SCORE_TABLE=''\n",
    "RESOURCE_BUCKET=''\n",
    "FILE_BUCKET=''\n",
    "REGION=''\n",
    "MODEL_ID=''\n",
    "STACK_NAME=''\n",
    "MODEL_NAME=''\n",
    "SERVICE_TYPE=''\n",
    "SERVICE_TYPE_NAME=''\n",
    "PIPELINE_TYPE=''\n",
    "TRAINING_PIPELINE_PATH=''\n",
    "SERVING_PIPELINE_PATH=''\n",
    "UTILS_PATH=''\n",
    "MODEL_TYPE=''\n",
    "LOAD_SQL=''\n",
    "PREPROCESS_OUTPUT_CSV=''\n",
    "SAVE_FILE_NAME=''\n",
    "STATS_FILE_NAME=''\n",
    "TRAINING_MODE=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "STACK_NAME = 'shs_churn'\n",
    "PROJECT_ID =  'divg-groovyhoon-pr-d2eab4'\n",
    "DATASET_ID = 'shs_churn'\n",
    "TABLE_ID = 'master_features_set_predict_vw'\n",
    "SCORE_TABLE_ID = 'bq_shs_churn_scores' # change\n",
    "RESOURCE_BUCKET = 'divg-groovyhoon-pr-d2eab4-default'\n",
    "FILE_BUCKET = 'divg-groovyhoon-pr-d2eab4_shs_churn' # change\n",
    "REGION = 'northamerica-northeast1'\n",
    "MODEL_ID = '5023' # change\n",
    "MODEL_NAME = 'shs_churn' # change\n",
    "SERVICE_TYPE = 'shs_churn' # change\n",
    "SERVICE_TYPE_NAME = 'shs-churn' # change\n",
    "PIPELINE_TYPE='serving_pipeline'\n",
    "TRAINING_PIPELINE_PATH = 'models/training_pipeline' # change\n",
    "SERVING_PIPELINE_PATH = 'models/serving_pipeline' # change\n",
    "UTILS_PATH =  'utils/resources' \n",
    "MODEL_TYPE='churn' # change \n",
    "LOAD_SQL='load_serving_data.sql'\n",
    "PREPROCESS_OUTPUT_CSV='df_serving.csv' \n",
    "SAVE_FILE_NAME='df_test_exp.csv'\n",
    "STATS_FILE_NAME='df_stats.csv'\n",
    "SCORE_FILE_NAME='df_score.csv'\n",
    "TRAINING_MODE=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORE_FILE_NAME = 'df_score.csv'\n",
    "file_name_without_extension = SCORE_FILE_NAME.rsplit('.', 1)[0]\n",
    "print(file_name_without_extension)  # Output: df_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_PIPELINE_NAME = f'{SERVICE_TYPE_NAME}-train-pipeline' # Same name as pulumi.yaml\n",
    "SERVING_PIPELINE_NAME = f'{SERVICE_TYPE_NAME}-serving-pipeline' # Same name as pulumi.yaml\n",
    "TRAINING_PIPELINE_DESCRIPTION = f'{SERVICE_TYPE_NAME}-train-pipeline'\n",
    "SERVING_PIPELINE_DESCRIPTION = f'{SERVICE_TYPE_NAME}-serving-pipeline'\n",
    "PIPELINE_ROOT = f\"gs://{FILE_BUCKET}\"\n",
    "REGION = \"northamerica-northeast1\"\n",
    "UCAR_SCORE_TABLE= f'{PROJECT_ID}.ucar_ingestion.bq_product_instance_model_score'\n",
    "TEMP_TABLE='bq_product_instance_model_score_temp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import kfp components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dir_from_bucket(\n",
    "    bucket: Any, local_path: Path, prefix: str, split_prefix: str = PIPELINE_TYPE\n",
    "):        \n",
    "    \"\"\"\n",
    "    Download files from a specified bucket to a local path, excluding a specified prefix.\n",
    "\n",
    "    Parameters:\n",
    "      - bucket: The bucket object from which to download files.\n",
    "      - local_path: The local path where the files will be downloaded to.\n",
    "      - prefix: The prefix to filter the files in the bucket. Only files with this prefix will be downloaded.\n",
    "      - split_prefix: The prefix to exclude from the downloaded file paths. Default is 'serving_pipeline'.\n",
    "    \"\"\"\n",
    "    for blob in bucket.list_blobs(prefix=prefix):\n",
    "        if not blob.name.endswith(\"/\"):\n",
    "            path = local_path / blob.name.split(f'{split_prefix}/')[-1]\n",
    "            str_path = path.as_posix()\n",
    "            Path(str_path[:str_path.rindex('/')]).mkdir(parents=True, exist_ok=True)\n",
    "            blob.download_to_filename(str_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pipeline kfp components locally\n",
    "storage_client = storage.Client(PROJECT_ID)\n",
    "bucket = storage_client.bucket(RESOURCE_BUCKET)\n",
    "prefix_core = f'{STACK_NAME}/{SERVING_PIPELINE_PATH}/kfp_components'\n",
    "extract_dir_from_bucket(bucket, pth_project, prefix_core)\n",
    "\n",
    "# import kfp components\n",
    "from kfp_components.run_sp import run_sp\n",
    "from kfp_components.preprocess import preprocess\n",
    "from kfp_components.predict import predict\n",
    "from kfp_components.postprocess import postprocess\n",
    "from kfp_components.aggregate_results import aggregate_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training pipeline is to run on the 3rd of every month\n",
    "# change the training time window every 3rd of month to R12 months as of {last day of 2 months ago} e.g. on May 1st, training window to be 2023-04-01 to 2024-03-31\n",
    "\n",
    "# set training dates\n",
    "scoringDate = (date.today() - relativedelta(days=2))\n",
    "\n",
    "# training dates\n",
    "SCORING_DATE = scoringDate.strftime('%Y%m%d')  # date.today().strftime('%Y%m%d')\n",
    "SCORING_DATE_DASH = scoringDate.strftime('%Y-%m-%d')\n",
    "\n",
    "SCORING_DATE_DASH = \"2024-01-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scoringDate)\n",
    "print(SCORING_DATE)\n",
    "print(SCORING_DATE_DASH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "from kfp.v2 import compiler\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "@dsl.pipeline(\n",
    "    name=SERVING_PIPELINE_NAME, \n",
    "    description=SERVING_PIPELINE_DESCRIPTION\n",
    "    )\n",
    "def pipeline(\n",
    "        project_id: str = PROJECT_ID,\n",
    "        region: str = REGION,\n",
    "        resource_bucket: str = RESOURCE_BUCKET, \n",
    "        file_bucket: str = FILE_BUCKET\n",
    "    ):\n",
    "    \n",
    "    #### this code block is only for a personal workbench \n",
    "    \n",
    "    import google.oauth2.credentials\n",
    "    token = !gcloud auth print-access-token\n",
    "    token_str = token[0]\n",
    "    \n",
    "    #### the end\n",
    "    \n",
    "#     from datetime import datetime\n",
    "#     update_ts = datetime.now()\n",
    "#     update_ts_str = update_ts.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    from pathlib import Path\n",
    "    cwd = os.getcwd() \n",
    "\n",
    "#     # ----- create training set --------\n",
    "#     run_sp_op = run_sp(from_date=SCORING_DATE_DASH, \n",
    "#                         to_date=SCORING_DATE_DASH, \n",
    "#                         stack_name=STACK_NAME,\n",
    "#                         project_id=PROJECT_ID, \n",
    "#                         dataset_id=DATASET_ID, \n",
    "#                         token=token_str\n",
    "#                         )\n",
    "\n",
    "#     run_sp_op.set_memory_limit('32G')\n",
    "#     run_sp_op.set_cpu_limit('4')\n",
    "     \n",
    "    # ----- preprocessing train data --------\n",
    "    preprocess_op = preprocess(project_id=PROJECT_ID,\n",
    "                        dataset_id=DATASET_ID,\n",
    "                        table_id=TABLE_ID,\n",
    "                        file_bucket=FILE_BUCKET, \n",
    "                        resource_bucket=RESOURCE_BUCKET, \n",
    "                        stack_name=STACK_NAME,\n",
    "                        pipeline_path=SERVING_PIPELINE_PATH,\n",
    "                        utils_path=UTILS_PATH,\n",
    "                        model_type=MODEL_TYPE,\n",
    "                        pipeline_type=PIPELINE_TYPE, \n",
    "                        load_sql=LOAD_SQL,\n",
    "                        preprocess_output_csv=PREPROCESS_OUTPUT_CSV,\n",
    "                        training_mode=TRAINING_MODE, \n",
    "                        token=token_str\n",
    "                        )\n",
    "\n",
    "    preprocess_op.set_memory_limit('64G')\n",
    "    preprocess_op.set_cpu_limit('8')\n",
    "\n",
    "    predict_op = predict(project_id=PROJECT_ID,\n",
    "                        dataset_id=DATASET_ID,\n",
    "                        score_table_id=SCORE_TABLE_ID,\n",
    "                        resource_bucket=RESOURCE_BUCKET,\n",
    "                        file_bucket=FILE_BUCKET,\n",
    "                        stack_name=STACK_NAME,\n",
    "                        model_id=MODEL_ID,\n",
    "                        service_type=SERVICE_TYPE, \n",
    "                        pipeline_type=PIPELINE_TYPE, \n",
    "                        training_pipeline_path=TRAINING_PIPELINE_PATH,\n",
    "                        serving_pipeline_path=SERVING_PIPELINE_PATH,\n",
    "                        preprocess_output_csv=PREPROCESS_OUTPUT_CSV ,\n",
    "                        utils_path=UTILS_PATH, \n",
    "                        score_file_name=SCORE_FILE_NAME, \n",
    "                        token=token_str\n",
    "                        )\n",
    "    \n",
    "    predict_op.set_memory_limit('64G')\n",
    "    predict_op.set_cpu_limit('8')\n",
    "\n",
    "    postprocess_op = postprocess(project_id=PROJECT_ID,\n",
    "                        dataset_id=DATASET_ID,\n",
    "                        score_table_id=SCORE_TABLE_ID,\n",
    "                        ucar_score_table=UCAR_SCORE_TABLE,\n",
    "                        temp_table=TEMP_TABLE,\n",
    "                        resource_bucket=RESOURCE_BUCKET,\n",
    "                        file_bucket=FILE_BUCKET,\n",
    "                        stack_name=STACK_NAME,\n",
    "                        model_type=MODEL_TYPE,\n",
    "                        model_id=MODEL_ID,\n",
    "                        score_date_dash=SCORING_DATE_DASH, \n",
    "                        pipeline_type=PIPELINE_TYPE, \n",
    "                        pipeline_path=SERVING_PIPELINE_PATH,\n",
    "                        utils_path=UTILS_PATH, \n",
    "                        score_file_name=SCORE_FILE_NAME, \n",
    "                        token=token_str\n",
    "                        )\n",
    "    \n",
    "    postprocess_op.set_memory_limit('64G')\n",
    "    postprocess_op.set_cpu_limit('4')\n",
    "\n",
    "    # preprocess_op.after(run_sp_op)\n",
    "    predict_op.after(preprocess_op)\n",
    "    postprocess_op.after(predict_op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kfp.v2 import compiler\n",
    "# from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "# compiler.Compiler().compile(\n",
    "#    pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    "# )\n",
    "\n",
    "# job = pipeline_jobs.PipelineJob(\n",
    "#    display_name='hs-nba-existing-customers-pipeline',\n",
    "#    template_path=\"pipeline.json\",\n",
    "#    location=REGION,\n",
    "#    enable_caching=False,\n",
    "#    pipeline_root = f\"gs://{RESOURCE_BUCKET}\"\n",
    "# )\n",
    "# job.run(service_account = f\"bilayer-sa@{PROJECT_ID}.iam.gserviceaccount.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.oauth2.credentials\n",
    "import json\n",
    "\n",
    "token = !gcloud auth print-access-token\n",
    "CREDENTIALS = google.oauth2.credentials.Credentials(token[0])\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "   pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    ")\n",
    "\n",
    "job = pipeline_jobs.PipelineJob(\n",
    "   display_name=TRAINING_PIPELINE_NAME,\n",
    "   template_path=\"pipeline.json\",\n",
    "   credentials = CREDENTIALS,\n",
    "   pipeline_root = PIPELINE_ROOT,\n",
    "   location=REGION,\n",
    "   enable_caching=False # I encourage you to enable caching when testing as it will reduce resource use\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
