{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c150da3-6e1e-4f02-a778-bb3b12af9054",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b559b7e6-fe51-49cc-8e9c-832380843832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import kfp\n",
    "from typing import Any\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, ClassificationMetrics,\n",
    "                        Metrics, component)\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import google\n",
    "from google.oauth2 import credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2.service_account import Credentials\n",
    "from google.cloud import storage\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components.v1.batch_predict_job import \\\n",
    "    ModelBatchPredictOp as batch_prediction_op\n",
    "\n",
    "# Set global vars\n",
    "pth_project = Path(os.getcwd())\n",
    "sys.path.insert(0, pth_project.as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558ff533-70c8-4421-813e-c567d6bc496b",
   "metadata": {},
   "source": [
    "### YAML Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f70d721-76e7-4c2e-8153-88b5bbe8ee47",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID=''\n",
    "DATASET_ID=''\n",
    "TABLE_ID=''\n",
    "RESOURCE_BUCKET=''\n",
    "FILE_BUCKET=''\n",
    "REGION=''\n",
    "MODEL_ID=''\n",
    "STACK_NAME=''\n",
    "MODEL_NAME=''\n",
    "SERVICE_TYPE=''\n",
    "SERVICE_TYPE_NAME=''\n",
    "PIPELINE_TYPE=''\n",
    "PIPELINE_PATH=''\n",
    "UTILS_PATH=''\n",
    "MODEL_TYPE=''\n",
    "LOAD_SQL=''\n",
    "PREPROCESS_OUTPUT_CSV=''\n",
    "SAVE_FILE_NAME=''\n",
    "STATS_FILE_NAME=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9364f939-b050-4f3b-a85b-7d3c86fe6c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID =  'divg-groovyhoon-pr-d2eab4'\n",
    "DATASET_ID = 'shs_churn'\n",
    "TABLE_ID = 'master_features_set_prospect_train_vw'\n",
    "RESOURCE_BUCKET = 'divg-groovyhoon-pr-d2eab4-default'\n",
    "FILE_BUCKET = 'divg-groovyhoon-pr-d2eab4_shs_churn' # change\n",
    "REGION = 'northamerica-northeast1'\n",
    "MODEL_ID = '5023' # change\n",
    "STACK_NAME = 'shs_churn'\n",
    "MODEL_NAME = 'shs_churn' # change \n",
    "SERVICE_TYPE = 'shs_churn' # change\n",
    "SERVICE_TYPE_NAME = 'shs-churn' # change\n",
    "PIPELINE_TYPE='training_pipeline'\n",
    "PIPELINE_PATH = 'models/training_pipeline' # change\n",
    "UTILS_PATH =  'utils/resources' \n",
    "MODEL_TYPE='churn' # change \n",
    "LOAD_SQL='load_train_data.sql'\n",
    "PREPROCESS_OUTPUT_CSV='df_train.csv' \n",
    "SAVE_FILE_NAME='df_test_exp.csv'\n",
    "STATS_FILE_NAME='df_stats.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2229e5d1-1362-40ed-aa7d-cb22e41e4960",
   "metadata": {},
   "source": [
    "### Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d628b-e759-4c9c-aab5-568c54721aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAINING_PIPELINE_NAME = f'{SERVICE_TYPE_NAME}-training-pipeline' # Same name as pulumi.yaml\n",
    "SERVING_PIPELINE_NAME = f'{SERVICE_TYPE_NAME}-serving-pipeline' # Same name as pulumi.yaml\n",
    "TRAINING_PIPELINE_DESCRIPTION = f'{SERVICE_TYPE_NAME}-training-pipeline'\n",
    "SERVING_PIPELINE_DESCRIPTION = f'{SERVICE_TYPE_NAME}-serving-pipeline'\n",
    "PIPELINE_ROOT = f\"gs://{FILE_BUCKET}\"\n",
    "REGION = \"northamerica-northeast1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2c13f6-71e5-46e2-afb4-3b6aa7ec260f",
   "metadata": {},
   "source": [
    "### Import kfp Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d40ca47-32df-4e03-a35f-e9d21941ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dir_from_bucket(\n",
    "    bucket: Any, local_path: Path, prefix: str, split_prefix: str = 'training_pipeline' \n",
    "):        \n",
    "    \"\"\"\n",
    "    Download files from a specified bucket to a local path, excluding a specified prefix.\n",
    "\n",
    "    Parameters:\n",
    "      - bucket: The bucket object from which to download files.\n",
    "      - local_path: The local path where the files will be downloaded to.\n",
    "      - prefix: The prefix to filter the files in the bucket. Only files with this prefix will be downloaded.\n",
    "      - split_prefix: The prefix to exclude from the downloaded file paths. Default is 'serving_pipeline'.\n",
    "    \"\"\"\n",
    "    for blob in bucket.list_blobs(prefix=prefix):\n",
    "        if not blob.name.endswith(\"/\"):\n",
    "            path = local_path / blob.name.split(f'{split_prefix}/')[-1]\n",
    "            str_path = path.as_posix()\n",
    "            Path(str_path[:str_path.rindex('/')]).mkdir(parents=True, exist_ok=True)\n",
    "            blob.download_to_filename(str_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776c260e-7b37-46e6-b513-26c42429991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pipeline kfp components locally\n",
    "storage_client = storage.Client(PROJECT_ID)\n",
    "bucket = storage_client.bucket(RESOURCE_BUCKET)\n",
    "prefix_core = f'{STACK_NAME}/{PIPELINE_PATH}/kfp_components'\n",
    "extract_dir_from_bucket(bucket, pth_project, prefix_core)\n",
    "\n",
    "# import kfp components\n",
    "from kfp_components.run_sp import run_sp\n",
    "from kfp_components.preprocess import preprocess\n",
    "from kfp_components.train_and_save_model import train_and_save_model\n",
    "from kfp_components.upload_model import upload_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315d454-416f-4eed-b12d-1acd5aed1207",
   "metadata": {},
   "source": [
    "### Date Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfad310-a482-4285-aa31-6d9113008be6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training pipeline is to run on the 3rd of every month\n",
    "# change the training time window every 3rd of month to R12 months as of {last day of 2 months ago} e.g. on May 1st, training window to be 2023-04-01 to 2024-03-31\n",
    "\n",
    "# set training date (2 days ago from today)\n",
    "trainingDate = (date.today() - relativedelta(days=2))\n",
    "\n",
    "# training dates\n",
    "TRAIN_DATE = trainingDate.strftime('%Y%m%d')  # date.today().strftime('%Y%m%d')\n",
    "TRAIN_DATE_DASH = trainingDate.strftime('%Y-%m-%d')\n",
    "\n",
    "# original \n",
    "FROM_DATE = trainingDate.replace(day=1) + relativedelta(months=-13)\n",
    "TO_DATE = trainingDate.replace(day=1) + relativedelta(months=-6, days=-1)\n",
    "\n",
    "FROM_DATE = FROM_DATE.strftime('%Y-%m-%d')\n",
    "TO_DATE = TO_DATE.strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1ee26c-f07b-4eab-992f-c2ad79e0b786",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FROM_DATE)\n",
    "print(TO_DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2eea99-a3f0-48ea-8962-115edcaf26d9",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5e82db-20c8-4480-bb48-b44d2ce005ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "from kfp.v2 import compiler\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "@dsl.pipeline(\n",
    "    name=TRAINING_PIPELINE_NAME, \n",
    "    description=TRAINING_PIPELINE_DESCRIPTION\n",
    "    )\n",
    "def pipeline(\n",
    "        project_id: str = PROJECT_ID,\n",
    "        region: str = REGION,\n",
    "        resource_bucket: str = RESOURCE_BUCKET, \n",
    "        file_bucket: str = FILE_BUCKET\n",
    "    ):\n",
    "    \n",
    "    #### this code block is only for a personal workbench \n",
    "    \n",
    "    import google.oauth2.credentials\n",
    "    token = !gcloud auth print-access-token\n",
    "    token_str = token[0]\n",
    "    \n",
    "    #### the end\n",
    "    \n",
    "    from datetime import datetime\n",
    "    update_ts = datetime.now()\n",
    "    update_ts_str = update_ts.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    from pathlib import Path\n",
    "    cwd = os.getcwd() \n",
    "\n",
    "    # ----- create training set --------\n",
    "    run_sp_op = run_sp(from_date=FROM_DATE, \n",
    "                                to_date=TO_DATE, \n",
    "                                project_id=PROJECT_ID, \n",
    "                                dataset_id=DATASET_ID, \n",
    "                                token=token_str\n",
    "                                )\n",
    "\n",
    "    run_sp_op.set_memory_limit('32G')\n",
    "    run_sp_op.set_cpu_limit('4')\n",
    "    \n",
    "    # ----- preprocessing train data --------\n",
    "    preprocess_op = preprocess(project_id=PROJECT_ID,\n",
    "                                dataset_id=DATASET_ID, \n",
    "                                table_id=TABLE_ID, \n",
    "                                file_bucket=FILE_BUCKET, \n",
    "                                resource_bucket=RESOURCE_BUCKET, \n",
    "                                stack_name=STACK_NAME, \n",
    "                                pipeline_path=PIPELINE_PATH,\n",
    "                                utils_path=UTILS_PATH, \n",
    "                                model_type=MODEL_TYPE,\n",
    "                                pipeline_type=PIPELINE_TYPE, \n",
    "                                load_sql=LOAD_SQL, \n",
    "                                preprocess_output_csv=PREPROCESS_OUTPUT_CSV, \n",
    "                                token=token_str\n",
    "                                )\n",
    "\n",
    "    preprocess_op.set_memory_limit('32G')\n",
    "    preprocess_op.set_cpu_limit('8')\n",
    "    \n",
    "    train_and_save_model_op = train_and_save_model(file_bucket=FILE_BUCKET, \n",
    "                                resource_bucket=RESOURCE_BUCKET,\n",
    "                                stack_name=STACK_NAME,\n",
    "                                service_type=SERVICE_TYPE, \n",
    "                                project_id=PROJECT_ID, \n",
    "                                dataset_id=DATASET_ID, \n",
    "                                model_type=MODEL_TYPE, \n",
    "                                preprocess_output_csv=PREPROCESS_OUTPUT_CSV , \n",
    "                                save_file_name=SAVE_FILE_NAME,\n",
    "                                stats_file_name=STATS_FILE_NAME, \n",
    "                                pipeline_path=PIPELINE_PATH,\n",
    "                                utils_path=UTILS_PATH, \n",
    "                                pipeline_type=PIPELINE_TYPE,\n",
    "                                # token=token_str\n",
    "                               )\n",
    "\n",
    "    train_and_save_model_op.set_memory_limit('32G')\n",
    "    train_and_save_model_op.set_cpu_limit('8')\n",
    "    \n",
    "    preprocess_op.after(run_sp_op)\n",
    "    train_and_save_model_op.after(preprocess_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a51c8-4c37-4392-a21a-c4c3279d36d4",
   "metadata": {},
   "source": [
    "### Run the Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27465a63-5c3b-4e96-9c08-f9cc5dafde90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from kfp.v2 import compiler\n",
    "# from google.cloud.aiplatform import pipeline_jobs\n",
    "# import json\n",
    "\n",
    "# compiler.Compiler().compile(\n",
    "#    pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    "# )\n",
    "\n",
    "# job = pipeline_jobs.PipelineJob(\n",
    "#                                    display_name=TRAINING_PIPELINE_NAME,\n",
    "#                                    template_path=\"pipeline.json\",\n",
    "#                                    location=REGION,\n",
    "#                                    enable_caching=False,\n",
    "#                                    pipeline_root = PIPELINE_ROOT\n",
    "#                                 )\n",
    "# job.run(service_account = f\"bilayer-sa@{PROJECT_ID}.iam.gserviceaccount.com\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f0f361-e6ce-4d31-b50e-b490d906a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.oauth2.credentials\n",
    "import json\n",
    "\n",
    "token = !gcloud auth print-access-token\n",
    "CREDENTIALS = google.oauth2.credentials.Credentials(token[0])\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "   pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    ")\n",
    "\n",
    "job = pipeline_jobs.PipelineJob(\n",
    "   display_name=TRAINING_PIPELINE_NAME,\n",
    "   template_path=\"pipeline.json\",\n",
    "   credentials = CREDENTIALS,\n",
    "   pipeline_root = PIPELINE_ROOT,\n",
    "   location=REGION,\n",
    "   enable_caching=False # I encourage you to enable caching when testing as it will reduce resource use\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
