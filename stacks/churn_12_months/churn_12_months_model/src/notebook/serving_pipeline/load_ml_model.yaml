name: Load ml model
inputs:
- {name: project_id, type: String}
- {name: region, type: String}
- {name: model_name, type: String}
outputs:
- {name: model, type: Artifact}
- {name: model_uri, type: String}
implementation:
  container:
    image: northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-load-model-slim:1.0.0
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.18' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
      \ import *\n\ndef load_ml_model(\n    project_id: str, \n    region: str, \n\
      \    model_name: str, \n    model: Output[Artifact]\n) -> NamedTuple(\"output\"\
      , [(\"model_uri\", str)]):\n\n    from google.cloud import aiplatform\n\n  \
      \  # List models with the given display name, order by update time\n    models\
      \ = aiplatform.Model.list(\n        filter=f'display_name={model_name}', \n\
      \        order_by=\"update_time\",\n        location=region)\n\n    if not models:\n\
      \        raise ValueError(f\"No model found with display name: {model_name}\"\
      )\n\n    # Get the latest model's resource name\n    latest_model = models[-1]\n\
      \    model_uri = latest_model.resource_name\n\n    # Update the model URI and\
      \ metadata\n    model.uri = model_uri\n    model.metadata['resourceName'] =\
      \ model_uri\n    env_var = latest_model.to_dict()['containerSpec']['env'][1]['value']\n\
      \n    # Return the model URI as part of the output\n    return (env_var,)\n\n"
    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - load_ml_model
