name: Batch prediction
inputs:
- {name: project_id, type: String}
- {name: dataset_id, type: String}
- {name: table_id, type: String}
- {name: file_bucket, type: String}
- {name: save_data_path, type: String}
- {name: service_type, type: String}
- {name: score_table, type: String}
- {name: score_date_dash, type: String}
- {name: temp_table, type: String}
- {name: model_uri, type: String}
outputs:
- {name: metrics, type: Metrics}
- {name: metricsc, type: ClassificationMetrics}
implementation:
  container:
    image: northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-pycaret-slim:latest
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.18' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
      \ import *\n\ndef batch_prediction(\n        project_id: str,\n        dataset_id:\
      \ str,\n        table_id: str, \n        file_bucket: str,\n        save_data_path:\
      \ str, \n        service_type: str,\n        score_table: str,\n        score_date_dash:\
      \ str,\n        temp_table: str,\n        metrics: Output[Metrics],\n      \
      \  metricsc: Output[ClassificationMetrics],\n        model_uri: str, \n):\n\
      \    import time\n    import pandas as pd\n    import numpy as np\n    import\
      \ pickle\n    from datetime import date\n    from dateutil.relativedelta import\
      \ relativedelta\n    from google.cloud import bigquery\n    from google.cloud\
      \ import storage\n\n    MODEL_ID = '5090'\n\n    def if_tbl_exists(bq_client,\
      \ table_ref):\n        from google.cloud.exceptions import NotFound\n      \
      \  try:\n            bq_client.get_table(table_ref)\n            return True\n\
      \        except NotFound:\n            return False\n\n    def upsert_table(project_id,\
      \ dataset_id, table_id, sql, result):\n        new_values = ',\\n'.join(result.apply(lambda\
      \ row: row_format(row), axis=1))\n        new_sql = sql.format(proj_id=project_id,\
      \ dataset_id=dataset_id, table_id=table_id,\n                             new_values=new_values)\n\
      \        bq_client = bigquery.Client(project=project_id)\n        code = bq_client.query(new_sql)\n\
      \        time.sleep(5)\n\n    def row_format(row):\n        values = row.values\n\
      \        new_values = \"\"\n        v = str(values[0]) if not pd.isnull(values[0])\
      \ else 'NULL'\n        if 'str' in str(type(values[0])):\n            new_values\
      \ += f\"'{v}'\"\n        else:\n            new_values += f\"{v}\"\n\n     \
      \   for i in range(1, len(values)):\n            v = str(values[i]) if not pd.isnull(values[i])\
      \ else 'NULL'\n            if 'str' in str(type(values[i])):\n             \
      \   new_values += f\",'{v}'\"\n            else:\n                new_values\
      \ += f\",{v}\"\n        return '(' + new_values + ')'\n\n    def generate_sql_file(ll):\n\
      \        s = 'MERGE INTO `{proj_id}.{dataset_id}.{table_id}` a'\n        s +=\
      \ \" USING UNNEST(\"\n        s += \"[struct<\"\n        for i in range(len(ll)\
      \ - 1):\n            v = ll[i]\n            s += \"{} {},\".format(v[0], v[1])\n\
      \        s += \"{} {}\".format(ll[-1][0], ll[-1][1])\n        s += \">{new_values}]\"\
      \n        s += \") b\"\n        s += \" ON a.ban = b.ban and a.score_date =\
      \ b.score_date\"\n        s += \" WHEN MATCHED THEN\"\n        s += \" UPDATE\
      \ SET \"\n        s += \"a.{}=b.{},\".format(ll[0][0], ll[0][0])\n        for\
      \ i in range(1, len(ll) - 1):\n            v = ll[i]\n            s += \"a.{}=b.{},\"\
      .format(v[0], v[0])\n        s += \"a.{}=b.{}\".format(ll[-1][0], ll[-1][0])\n\
      \        s += \" WHEN NOT MATCHED THEN\"\n        s += \" INSERT(\"\n      \
      \  for i in range(len(ll) - 1):\n            v = ll[i]\n            s += \"\
      {},\".format(v[0])\n        s += \"{})\".format(ll[-1][0])\n        s += \"\
      \ VALUES(\"\n        for i in range(len(ll) - 1):\n            s += \"b.{},\"\
      .format(ll[i][0])\n        s += \"b.{}\".format(ll[-1][0])\n        s += \"\
      )\"\n\n        return s\n\n    def right(s, amount):\n        return s[-amount:]\n\
      \n    # MODEL_PATH = '{}_xgb_models/'.format(service_type)\n    MODEL_PATH =\
      \ right(model_uri, (len(model_uri) - 6 - len(file_bucket)))\n    df_score =\
      \ pd.read_csv(save_data_path, compression='gzip')\n    print(f'save_data_path:\
      \ {save_data_path}')\n    df_score.dropna(subset=['ban'], inplace=True)\n  \
      \  df_score.reset_index(drop=True, inplace=True)\n    print('......scoring data\
      \ loaded:{}'.format(df_score.shape))\n    time.sleep(10)\n\n    storage_client\
      \ = storage.Client()\n    bucket = storage_client.get_bucket(file_bucket)\n\
      \    # blobs = storage_client.list_blobs(file_bucket, prefix='{}{}_models_xgb_'.format(MODEL_PATH,\
      \ service_type))\n    blobs = storage_client.list_blobs(file_bucket, prefix=MODEL_PATH)\n\
      \n    model_lists = []\n    for blob in blobs:\n        model_lists.append(blob.name)\n\
      \n    blob = bucket.blob(model_lists[-1])\n    blob_in = blob.download_as_string()\n\
      \    model_dict = pickle.loads(blob_in)\n    model_xgb = model_dict['model']\n\
      \    features = model_dict['features']\n    print('...... model loaded')\n \
      \   time.sleep(10)\n\n    ll = [('ban', 'string'), ('score_date', 'string'),\
      \ ('model_id', 'string'), ('score', 'float64')]\n    sql = generate_sql_file(ll)\n\
      \n    df_score['ban'] = df_score['ban'].astype(int)\n    print('.... scoring\
      \ for {} FFH bans base'.format(len(df_score)))\n\n    # get full score to cave\
      \ into bucket\n    pred_prob = model_xgb.predict_proba(df_score[features], ntree_limit=model_xgb.best_iteration)[:,\
      \ 1]\n    result = pd.DataFrame(columns=['ban', 'score_date', 'model_id', 'score'])\n\
      \    result['score'] = list(pred_prob)\n    result['score'] = result['score'].fillna(0.0).astype('float64')\n\
      \    result['ban'] = list(df_score['ban'])\n    result['ban'] = result['ban'].astype('str')\n\
      \    result['score_date'] = score_date_dash\n    result['model_id'] = MODEL_ID\n\
      \n    result.to_csv('gs://{}/ucar/{}_prediction.csv.gz'.format(file_bucket,\
      \ service_type), compression='gzip',\n                  index=False)\n\n   \
      \ # define df_final\n    df_final = df_score[features]\n\n    # define dtype_bq_mapping\n\
      \    dtype_bq_mapping = {np.dtype('int64'): 'INTEGER', \n    np.dtype('float64'):\
      \  'FLOAT', \n    np.dtype('float32'):  'FLOAT', \n    np.dtype('object'): \
      \ 'STRING', \n    np.dtype('bool'):  'BOOLEAN', \n    np.dtype('datetime64[ns]'):\
      \  'DATE', \n    pd.Int64Dtype(): 'INTEGER'} \n\n    # export df_final to bigquery\
      \ \n    schema_list = [] \n    for column in df_final.columns: \n        schema_list.append(bigquery.SchemaField(column,\
      \ dtype_bq_mapping[df_final.dtypes[column]], mode='NULLABLE')) \n    print(schema_list)\
      \ \n\n    dest_table = f'{dataset_id}.{table_id}'\n\n    # Sending to bigquery\
      \ \n    client = bigquery.Client(project=project_id)\n    job_config = bigquery.LoadJobConfig(schema=schema_list,\
      \ write_disposition='WRITE_TRUNCATE') \n    job = client.load_table_from_dataframe(df_final,\
      \ dest_table, job_config=job_config) \n    job.result() \n    table = client.get_table(dest_table)\
      \ # Make an API request \n    print(\"Loaded {} rows and {} columns to {}\"\
      .format(table.num_rows, len(table.schema), table_id)) \n\n    time.sleep(60)\n\
      \n    table_ref = f'{project_id}.{dataset_id}.{score_table}'\n    client = bigquery.Client(project=project_id)\n\
      \    table = client.get_table(table_ref)\n    schema = table.schema\n\n    ll\
      \ = []\n    for item in schema:\n        col = item.name\n        d_type = item.field_type\n\
      \        if 'float' in str(d_type).lower():\n            d_type = 'FLOAT64'\n\
      \        ll.append((col, d_type))\n\n        if 'integer' in str(d_type).lower():\n\
      \            result[col] = result[col].fillna(0).astype(int)\n        if 'float'\
      \ in str(d_type).lower():\n            result[col] = result[col].fillna(0.0).astype(float)\n\
      \        if 'string' in str(d_type).lower():\n            result[col] = result[col].fillna('').astype(str)\n\
      \n    table_ref = '{}.{}.{}'.format(project_id, dataset_id, temp_table)\n  \
      \  client = bigquery.Client(project=project_id)\n    if if_tbl_exists(client,\
      \ table_ref):\n        client.delete_table(table_ref)\n\n    client.create_table(table_ref)\n\
      \    config = bigquery.LoadJobConfig(schema=schema)\n    config.write_disposition\
      \ = bigquery.WriteDisposition.WRITE_TRUNCATE\n    bq_table_instance = client.load_table_from_dataframe(result,\
      \ table_ref, job_config=config)\n    time.sleep(5)\n\n    drop_sql = f\"\"\"\
      delete from `{project_id}.{dataset_id}.{score_table}` where score_date = '{score_date_dash}'\"\
      \"\"  # .format(project_id, dataset_id, score_date_dash)\n    client.query(drop_sql)\n\
      \    #\n    load_sql = f\"\"\"insert into `{project_id}.{dataset_id}.{score_table}`\n\
      \                  select * from `{project_id}.{dataset_id}.{temp_table}`\"\"\
      \"\n    client.query(load_sql)\n\n"
    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - batch_prediction
