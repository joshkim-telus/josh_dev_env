{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c150da3-6e1e-4f02-a778-bb3b12af9054",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b559b7e6-fe51-49cc-8e9c-832380843832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, ClassificationMetrics,\n",
    "                        Metrics, component)\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import google\n",
    "from google.oauth2 import credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2.service_account import Credentials\n",
    "from google.cloud import storage\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components.v1.batch_predict_job import \\\n",
    "    ModelBatchPredictOp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558ff533-70c8-4421-813e-c567d6bc496b",
   "metadata": {},
   "source": [
    "### YAML Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f70d721-76e7-4c2e-8153-88b5bbe8ee47",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID =  ''\n",
    "DATASET_ID = ''\n",
    "RESOURCE_BUCKET = ''\n",
    "FILE_BUCKET = ''\n",
    "REGION = ''\n",
    "MODEL_ID = ''\n",
    "MODEL_NAME = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef1f0cb-c61e-479b-89e7-1053298ffc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Workbench only\n",
    "PROJECT_ID= \"divg-groovyhoon-pr-d2eab4\"\n",
    "DATASET_ID= \"nba_offer_targeting\"\n",
    "REGION= \"northamerica-northeast1\"\n",
    "FILE_BUCKET= \"divg-groovyhoon-pr-d2eab4-default\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eeed21-e12b-45ee-b1e4-d0c8f3843533",
   "metadata": {},
   "source": [
    "### Service Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65d1668-e1d8-4f58-958f-edc44d06a6b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SERVICE_TYPE = 'nba_offer_targeting_checker'\n",
    "SERVICE_TYPE_NAME = 'nba-offer-targeting-checker'\n",
    "REGION = \"northamerica-northeast1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2229e5d1-1362-40ed-aa7d-cb22e41e4960",
   "metadata": {},
   "source": [
    "### Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d628b-e759-4c9c-aab5-568c54721aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STACK_NAME = 'nba_offer_targeting_checker'\n",
    "SERVING_PIPELINE_NAME_PATH = 'nba_offer_targeting_checker_pipeline/serving_pipeline'\n",
    "SERVING_PIPELINE_NAME = 'nba-offer-targeting-checker-serving-pipeline' # Same name as pulumi.yaml\n",
    "SERVING_PIPELINE_DESCRIPTION = 'nba-offer-targeting-checker-serving-pipeline'\n",
    "PIPELINE_ROOT = f\"gs://{FILE_BUCKET}\"\n",
    "REGION = \"northamerica-northeast1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961cdfe6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import Text Files (.sql) (Workbench)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7124d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Specify the directory path\n",
    "directory = 'queries/'\n",
    "\n",
    "queries = [] \n",
    "\n",
    "# Iterate over files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    # Check if the file is a text file\n",
    "    if filename.endswith('.sql'):\n",
    "        # Construct the full file path\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        \n",
    "        # Open the file and read its contents\n",
    "        with open(filepath, 'r') as file:\n",
    "            # Read the contents of the file\n",
    "            content = file.read()\n",
    "            queries.append(content)\n",
    "            \n",
    "test_1 = queries[0]\n",
    "test_2 = queries[1] \n",
    "test_3 = queries[2] \n",
    "test_4 = queries[3] \n",
    "test_5 = queries[4] \n",
    "test_6 = queries[5] \n",
    "test_7 = queries[6] \n",
    "test_8 = queries[7] \n",
    "test_9 = queries[8] \n",
    "test_10 = queries[9]\n",
    "test_11 = queries[10] \n",
    "test_12 = queries[11] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a77f84f-11a6-4136-8de7-ec1351e6f455",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"nba_offer_targeting_np\"\n",
    "\n",
    "test_1.format(dataset_id=dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a727126d",
   "metadata": {},
   "source": [
    "### Queries Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1e8ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERIES_PATH = f'{STACK_NAME}/' + SERVING_PIPELINE_NAME_PATH + '/queries/'\n",
    "\n",
    "TEST_1 = QUERIES_PATH + 'test1.sql'\n",
    "TEST_2 = QUERIES_PATH + 'test2.sql'\n",
    "TEST_3 = QUERIES_PATH + 'test3.sql'\n",
    "TEST_4 = QUERIES_PATH + 'test4.sql'\n",
    "TEST_5 = QUERIES_PATH + 'test5.sql'\n",
    "TEST_6 = QUERIES_PATH + 'test6.sql'\n",
    "TEST_7 = QUERIES_PATH + 'test7.sql'\n",
    "TEST_8 = QUERIES_PATH + 'test8.sql'\n",
    "TEST_9 = QUERIES_PATH + 'test9.sql'\n",
    "TEST_10 = QUERIES_PATH + 'test10.sql'\n",
    "TEST_11 = QUERIES_PATH + 'test11.sql'\n",
    "TEST_12 = QUERIES_PATH + 'test12.sql'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209267cc",
   "metadata": {},
   "source": [
    "### Import Text Files (.sql) (BI Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1797eda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load query from .txt file\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(RESOURCE_BUCKET)\n",
    "\n",
    "queries = [] \n",
    "n = 0\n",
    "\n",
    "for i in range(n): \n",
    "    blob = bucket.get_blob(QUERIES_PATH + f'test_{i}.sql')\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "    queries.append(content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c486391b-c332-4591-bf52-12fffb79b1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = queries[0]\n",
    "test_2 = queries[1] \n",
    "test_3 = queries[2] \n",
    "test_4 = queries[3] \n",
    "test_5 = queries[4] \n",
    "test_6 = queries[5] \n",
    "test_7 = queries[6] \n",
    "test_8 = queries[7] \n",
    "test_9 = queries[8] \n",
    "test_10 = queries[9]\n",
    "test_11 = queries[10] \n",
    "test_12 = queries[11] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc0bd98-1d58-491e-bc5e-bbac378c7bbf",
   "metadata": {},
   "source": [
    "### Import Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442904b-a40a-4ba8-b237-e9894ffd4f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # download required component files to local\n",
    "# prefix = f'{STACK_NAME}/{SERVING_PIPELINE_NAME_PATH}/components/'\n",
    "# dl_dir = 'components/'\n",
    "\n",
    "# storage_client = storage.Client()\n",
    "# bucket = storage_client.bucket(RESOURCE_BUCKET)\n",
    "# blobs = bucket.list_blobs(prefix=prefix)  # Get list of files\n",
    "# for blob in blobs: # download each file that starts with \"prefix\" into \"dl_dir\"\n",
    "#     if blob.name.endswith(\"/\"):\n",
    "#         continue\n",
    "#     file_split = blob.name.split(prefix)\n",
    "#     file_path = f\"{dl_dir}{file_split[-1]}\"\n",
    "#     directory = \"/\".join(file_path.split(\"/\")[0:-1])\n",
    "#     Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "#     blob.download_to_filename(file_path) \n",
    "\n",
    "# import main pipeline components\n",
    "from components.output_validation import output_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdcc135-0580-4e4d-bebe-a72da778dc9f",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb70ef8-0fad-4590-ba5b-8090eeeb43c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "from kfp.v2 import compiler\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "@dsl.pipeline(\n",
    "    name=SERVING_PIPELINE_NAME, \n",
    "    description=SERVING_PIPELINE_DESCRIPTION\n",
    "    )\n",
    "def pipeline(\n",
    "        project_id: str = PROJECT_ID,\n",
    "        region: str = REGION,\n",
    "        resource_bucket: str = RESOURCE_BUCKET,\n",
    "        file_bucket: str = FILE_BUCKET\n",
    "    ):\n",
    "\n",
    "    #### this code block is only for a personal workbench \n",
    "    \n",
    "    import google.oauth2.credentials\n",
    "    token = !gcloud auth print-access-token\n",
    "    token_str = token[0]\n",
    "    \n",
    "    #### the end\n",
    "    \n",
    "    from datetime import datetime\n",
    "    update_ts = datetime.now()\n",
    "    update_ts_str = update_ts.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    #nba offer targeting checker function\n",
    "    output_validation_op_1 = output_validation(        \n",
    "        project_id=PROJECT_ID\n",
    "      , dataset_id=DATASET_ID\n",
    "      , query=test_1\n",
    "       , token=token_str) \n",
    "    \n",
    "    output_validation_op_1.set_memory_limit('16G')\n",
    "    output_validation_op_1.set_cpu_limit('4')\n",
    "    \n",
    "    #nba offer targeting checker function\n",
    "    output_validation_op_2 = output_validation(        \n",
    "        project_id=PROJECT_ID\n",
    "      , dataset_id=DATASET_ID\n",
    "      , query=test_2\n",
    "       , token=token_str) \n",
    "    \n",
    "    output_validation_op_2.set_memory_limit('16G')\n",
    "    output_validation_op_2.set_cpu_limit('4')\n",
    "    \n",
    "    #nba offer targeting checker function\n",
    "    output_validation_op_3 = output_validation(        \n",
    "        project_id=PROJECT_ID\n",
    "      , dataset_id=DATASET_ID\n",
    "      , query=test_3\n",
    "       , token=token_str) \n",
    "    \n",
    "    output_validation_op_3.set_memory_limit('16G')\n",
    "    output_validation_op_3.set_cpu_limit('4')\n",
    "    \n",
    "    output_validation_op_2.after(output_validation_op_1)\n",
    "    output_validation_op_3.after(output_validation_op_2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e28300-a8a8-4c99-a379-45d63095727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     nba offer targeting checker function\n",
    "#     output_validation_op_4 = output_validation(        \n",
    "#         project_id=PROJECT_ID\n",
    "#       , dataset_id=DATASET_ID\n",
    "#       , query=test_4\n",
    "#        , token=token[0]) \n",
    "    \n",
    "#     output_validation_op_4.set_memory_limit('16G')\n",
    "#     output_validation_op_4.set_cpu_limit('4')\n",
    "    \n",
    "#     #nba offer targeting checker function\n",
    "#     output_validation_op_5 = output_validation(        \n",
    "#         project_id=PROJECT_ID\n",
    "#       , dataset_id=DATASET_ID\n",
    "#       , query=test_5\n",
    "#        , token=token[0]) \n",
    "    \n",
    "#     output_validation_op_5.set_memory_limit('16G')\n",
    "#     output_validation_op_5.set_cpu_limit('4')\n",
    "    \n",
    "#     #nba offer targeting checker function\n",
    "#     output_validation_op_6 = output_validation(        \n",
    "#         project_id=PROJECT_ID\n",
    "#       , dataset_id=DATASET_ID\n",
    "#       , query=test_6\n",
    "#        , token=token[0]) \n",
    "    \n",
    "#     output_validation_op_6.set_memory_limit('16G')\n",
    "#     output_validation_op_6.set_cpu_limit('4')\n",
    "    \n",
    "#     #nba offer targeting checker function\n",
    "#     output_validation_op_7 = output_validation(        \n",
    "#         project_id=PROJECT_ID\n",
    "#       , dataset_id=DATASET_ID\n",
    "#       , query=test_7\n",
    "#        , token=token[0]) \n",
    "    \n",
    "#     output_validation_op_7.set_memory_limit('16G')\n",
    "#     output_validation_op_7.set_cpu_limit('4')\n",
    "    \n",
    "#     #nba offer targeting checker function\n",
    "#     output_validation_op_8 = output_validation(        \n",
    "#         project_id=PROJECT_ID\n",
    "#       , dataset_id=DATASET_ID\n",
    "#       , query=test_8\n",
    "#        , token=token[0]) \n",
    "    \n",
    "#     output_validation_op_8.set_memory_limit('16G')\n",
    "#     output_validation_op_8.set_cpu_limit('4')\n",
    "    \n",
    "#     #nba offer targeting checker function\n",
    "#     output_validation_op_9 = output_validation(        \n",
    "#         project_id=PROJECT_ID\n",
    "#       , dataset_id=DATASET_ID\n",
    "#       , query=test_9\n",
    "#        , token=token[0]) \n",
    "    \n",
    "#     output_validation_op_9.set_memory_limit('16G')\n",
    "#     output_validation_op_9.set_cpu_limit('4')\n",
    "    \n",
    "#     #nba offer targeting checker function\n",
    "#     output_validation_op_10 = output_validation(        \n",
    "#         project_id=PROJECT_ID\n",
    "#       , dataset_id=DATASET_ID\n",
    "#       , query=test_10\n",
    "#        , token=token[0]) \n",
    "    \n",
    "#     output_validation_op_10.set_memory_limit('16G')\n",
    "#     output_validation_op_10.set_cpu_limit('4')\n",
    "    \n",
    "#     #nba offer targeting checker function\n",
    "#     output_validation_op_11 = output_validation(        \n",
    "#         project_id=PROJECT_ID\n",
    "#       , dataset_id=DATASET_ID\n",
    "#       , query=test_11\n",
    "#        , token=token[0]) \n",
    "    \n",
    "#     output_validation_op_11.set_memory_limit('16G')\n",
    "#     output_validation_op_11.set_cpu_limit('4')\n",
    "    \n",
    "#     #nba offer targeting checker function\n",
    "#     output_validation_op_12 = output_validation(        \n",
    "#         project_id=PROJECT_ID\n",
    "#       , dataset_id=DATASET_ID\n",
    "#       , query=test_12\n",
    "#        , token=token[0]) \n",
    "    \n",
    "#     output_validation_op_12.set_memory_limit('16G')\n",
    "#     output_validation_op_12.set_cpu_limit('4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a51c8-4c37-4392-a21a-c4c3279d36d4",
   "metadata": {},
   "source": [
    "### Run the Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27465a63-5c3b-4e96-9c08-f9cc5dafde90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kfp.v2 import compiler\n",
    "# from google.cloud.aiplatform import pipeline_jobs\n",
    "# import json\n",
    "\n",
    "# compiler.Compiler().compile(\n",
    "#    pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    "# )\n",
    "\n",
    "# job = pipeline_jobs.PipelineJob(\n",
    "#                                    display_name=SERVING_PIPELINE_NAME,\n",
    "#                                    template_path=\"pipeline.json\",\n",
    "#                                    location=REGION,\n",
    "#                                    enable_caching=False,\n",
    "#                                    pipeline_root = PIPELINE_ROOT\n",
    "#                                 )\n",
    "# job.run(service_account = f\"bilayer-sa@{PROJECT_ID}.iam.gserviceaccount.com\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2fb80d-72f1-4cbc-a2f4-03601e1699a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.oauth2.credentials\n",
    "import json\n",
    "\n",
    "token = !gcloud auth print-access-token\n",
    "CREDENTIALS = google.oauth2.credentials.Credentials(token[0])\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "   pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    ")\n",
    "\n",
    "job = pipeline_jobs.PipelineJob(\n",
    "   display_name=SERVING_PIPELINE_NAME,\n",
    "   template_path=\"pipeline.json\",\n",
    "   credentials = CREDENTIALS,\n",
    "   pipeline_root = PIPELINE_ROOT,\n",
    "   location=REGION,\n",
    "   enable_caching=False # I encourage you to enable caching when testing as it will reduce resource use\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c98ed-3ebc-459d-b641-feef2e01b5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e2b5a5-9674-424b-b1bf-6e986239c082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2a6305-6afc-4a4e-9a4c-9ca2cf23095e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55a1299-0319-44a5-b84c-12116b36fdab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m119"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
